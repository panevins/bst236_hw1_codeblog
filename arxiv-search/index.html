<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 6/30/2025, 1:27:09 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Personnel-adjustment for home run park effects in Major League Baseball</h3>
                    <p><strong>Authors:</strong> Jason A. Osborne, Richard A. Levine</p>
                    <p>  In Major League Baseball, every ballpark is different, with different
dimensions and climates. These differences make some ballparks more conducive
to hitting home runs than others. Several factors conspire to make estimation
of these differences challenging. Home runs are relatively rare, occurring in
roughly 3\% of plate appearances. The quality of personnel and the frequency of
batter-pitcher handedness combinations that appear in the thirty ballparks vary
considerably. Because of asymmetries, effects due to ballpark can depend
strongly on hitter handedness. We consider generalized linear mixed effects
models based on the Poisson distribution for home runs. We use as our
observational unit the combination of game and handedness-matchup. Our model
allows for four theoretical mean home run frequency functions for each
ballpark. We control for variation in personnel across games by constructing
``elsewhere'' measures of batter ability to hit home runs and pitcher tendency
to give them up, using data from parks other than the one in which the response
is observed. We analyze 13 seasons of data and find that the estimated home run
frequencies adjusted to average personnel are substantially different from
observed home run frequencies, leading to considerably different ballpark
rankings than often appear in the media.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.22350v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Linking climate and dengue in the Philippines using a two-stage Bayesian
  spatio-temporal model</h3>
                    <p><strong>Authors:</strong> Stephen Jun Villejo, Sara Martino, Janine Illian</p>
                    <p>  Dengue is an infectious disease which poses significant socioeconomic and
disease burden in many tropical and subtropical regions of the world. This work
aims to provide additional insight into the association between dengue and
climate in the Philippines. We employ a two-stage modelling framework: the
first stage fits climate models, while the second stage fits a health model
that uses the climate predictions from the first stage as inputs. We postulate
a Bayesian spatio-temporal model and use the integrated nested Laplace
approximation (INLA) approach for inference. To account for the uncertainty in
the climate models, we perform posterior sampling and then perform Bayesian
model averaging to compute the final posterior estimates of second-stage model
parameters. The results indicate that temperature is positively associated with
dengue, although extremely hot conditions tend to have a negative effect.
Moreover, the relationship between rainfall and dengue varies in space. In
areas with uniform amounts of rainfall all year round, rainfall is negatively
associated with dengue. In contrast, in regions with pronounced dry and wet
season, rainfall shows a positive association with dengue. Finally, there
remains unexplained structured variation in space and time after accounting for
the impact of climate variables and other covariates.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.22334v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Uncovering smooth structures in single-cell data with PCS-guided
  neighbor embeddings</h3>
                    <p><strong>Authors:</strong> Rong Ma, Xi Li, Jingyuan Hu, Bin Yu</p>
                    <p>  Single-cell sequencing is revolutionizing biology by enabling detailed
investigations of cell-state transitions. Many biological processes unfold
along continuous trajectories, yet it remains challenging to extract smooth,
low-dimensional representations from inherently noisy, high-dimensional
single-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,
are widely used to embed high-dimensional single-cell data into low dimensions.
But they often introduce undesirable distortions, resulting in misleading
interpretations. Existing evaluation methods for NE algorithms primarily focus
on separating discrete cell types rather than capturing continuous cell-state
transitions, while dynamic modeling approaches rely on strong assumptions about
cellular processes and specialized data. To address these challenges, we build
on the Predictability-Computability-Stability (PCS) framework for reliable and
reproducible data-driven discoveries. First, we systematically evaluate popular
NE algorithms through empirical analysis, simulation, and theory, and reveal
their key shortcomings, such as artifacts and instability. We then introduce
NESS, a principled and interpretable machine learning approach to improve NE
representations by leveraging algorithmic stability and to enable robust
inference of smooth biological structures. NESS offers useful concepts,
quantitative stability metrics, and efficient computational workflows to
uncover developmental trajectories and cell-state transitions in single-cell
data. Finally, we apply NESS to six single-cell datasets, spanning pluripotent
stem cell differentiation, organoid development, and multiple tissue-specific
lineage trajectories. Across these diverse contexts, NESS consistently yields
useful biological insights, such as identification of transitional and stable
cell states and quantification of transcriptional dynamics during development.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.22228v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Simulated Intervention on Cross-Sectional Nested Data: Development of a
  Multilevel NIRA Approach</h3>
                    <p><strong>Authors:</strong> Yiming Wu, Fei Wang</p>
                    <p>  With the rise of the network perspective, researchers have made numerous
important discoveries over the past decade by constructing psychological
networks. Unfortunately, most of these networks are based on cross-sectional
data, which can only reveal associations between variables but not their
directional or causal relationships. Recently, the development of the
nodeIdentifyR algorithm (NIRA) technique has provided a promising method for
simulating causal processes based on cross-sectional network structures.
However, this algorithm is not capable of handling cross-sectional nested data,
which greatly limits its applicability. In response to this limitation, the
present study proposes a multilevel extension of the NIRA algorithm, referred
to as multilevel NIRA. We provide a detailed explanation of the algorithm's
core principles and modeling procedures. Finally, we discuss the potential
applications and practical implications of this approach, as well as its
limitations and directions for future research.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.21991v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Non-Parametric Time Between Events and Amplitude Methods for Monitoring
  Drought Characteristics</h3>
                    <p><strong>Authors:</strong> Michele Scagliarini</p>
                    <p>  Drought is a significant natural phenomenon with profound environmental,
economic, and societal impacts. Effective monitoring of drought characteristics
-- such as intensity, magnitude, and duration -- is crucial for resilience and
mitigation strategies. This study proposes the use of non-parametric Time
Between Events and Amplitude (TBEA) control charts for detecting changes in
drought characteristics, specifically applying them to the Standardized
Precipitation and Evapotranspiration Index. Aware of being non-exhaustive, we
considered two non-parametric change-point control charts based on the
Mann-Whitney and Kolmogorov-Smirnov statistics, respectively. We studied the
in-control statistical performances of the change-point control charts in the
time between events and amplitude framework through a simulation study.
Furthermore, we assessed the coherence of the results obtained with a
distribution-free upper sided Exponentially Weighted Moving Average control
chart specifically designed for monitoring TBEA data. The findings suggest that
the proposed methods may serve as valuable tools for climate resilience
planning and water resource management.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.21970v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Identifying High-Risk Areas for Traffic Collisions in Montgomery,
  Maryland Using KDE and Spatial Autocorrelation Analysis</h3>
                    <p><strong>Authors:</strong> Stanislav Liashkov</p>
                    <p>  Despite a global decline in motor vehicle crash fatalities due to improved
research and road safety policies, road traffic injuries remain a significant
public health concern. The World Health Organization 2023 report highlights
that road traffic injuries are the leading cause of death among individuals
aged 5-29, with over half of fatalities involving pedestrians, cyclists, and
motorcyclists. This study addresses this critical issue by identifying
high-risk areas in Montgomery County, Maryland, contributing to the global goal
of halving road traffic deaths and injuries by 2030. Using Kernel Density
Estimation (KDE) and spatial autocorrelation analysis, we estimate collision
densities and identify hotspots for targeted interventions. Our findings reveal
significant spatial clustering of traffic collisions, with distinct patterns in
densely populated urban areas and rural regions, offering valuable insights for
policymakers to enhance road safety.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.21930v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Explainable anomaly detection for sound spectrograms using pooling
  statistics with quantile differences</h3>
                    <p><strong>Authors:</strong> Nicolas Thewes, Philipp Steinhauer, Patrick Trampert, Markus Pauly, Georg Schneider</p>
                    <p>  Anomaly detection is the task of identifying rarely occurring (i.e. anormal
or anomalous) samples that differ from almost all other samples in a dataset.
As the patterns of anormal samples are usually not known a priori, this task is
highly challenging. Consequently, anomaly detection lies between semi- and
unsupervised learning. The detection of anomalies in sound data, often called
'ASD' (Anomalous Sound Detection), is a sub-field that deals with the
identification of new and yet unknown effects in acoustic recordings. It is of
great importance for various applications in Industry 4.0. Here, vibrational or
acoustic data are typically obtained from standard sensor signals used for
predictive maintenance. Examples cover machine condition monitoring or quality
assurance to track the state of components or products. However, the use of
intelligent algorithms remains a controversial topic. Management generally aims
for cost-reduction and automation, while quality and maintenance experts
emphasize the need for human expertise and comprehensible solutions. In this
work, we present an anomaly detection approach specifically designed for
spectrograms. The approach is based on statistical evaluations and is
theoretically motivated. In addition, it features intrinsic explainability,
making it particularly suitable for applications in industrial settings. Thus,
this algorithm is of relevance for applications in which black-box algorithms
are unwanted or unsuitable.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.21921v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Putting Skill as Nearly Indistinguishable from Noise: An Empirical Bayes
  Analysis of PGA Tour Performance</h3>
                    <p><strong>Authors:</strong> Ryan S. Brill, Abraham J. Wyner</p>
                    <p>  We revisit a foundational question in golf analytics: how important are the
core components of performance--driving, approach play, and putting--in
explaining success on the PGA Tour? Building on Mark Broadie's strokes gained
analyses, we use an empirical Bayes approach to estimate latent golfer skill
and assess statistical significance using a multiple testing procedure that
controls the false discovery rate. While tee-to-green skill shows clear and
substantial differences across players, putting skill is both less variable and
far less reliably estimable. Indeed, putting performance appears nearly
indistinguishable from noise.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.21822v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Federated Item Response Theory Models</h3>
                    <p><strong>Authors:</strong> Biying Zhou, Nanyu Luo, Feng Ji</p>
                    <p>  Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).
</p>
                    <p><a href="http://arxiv.org/pdf/2506.21744v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Dynamic Bayesian Item Response Model with Decomposition (D-BIRD):
  Modeling Cohort and Individual Learning Over Time</h3>
                    <p><strong>Authors:</strong> Hansol Lee, Jason B. Cho, David S. Matteson, Benjamin W. Domingue</p>
                    <p>  We present D-BIRD, a Bayesian dynamic item response model for estimating
student ability from sparse, longitudinal assessments. By decomposing ability
into a cohort trend and individual trajectory, D-BIRD supports interpretable
modeling of learning over time. We evaluate parameter recovery in simulation
and demonstrate the model using real-world personalized learning data.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.21723v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>