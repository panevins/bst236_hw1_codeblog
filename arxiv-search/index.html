<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 10/6/2025, 1:22:42 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Neural Posterior Estimation with Autoregressive Tiling for Detecting
  Objects in Astronomical Images</h3>
                    <p><strong>Authors:</strong> Jeffrey Regier</p>
                    <p>  Upcoming astronomical surveys will produce petabytes of high-resolution
images of the night sky, providing information about billions of stars and
galaxies. Detecting and characterizing the astronomical objects in these images
is a fundamental task in astronomy -- and a challenging one, as most of these
objects are faint and many visually overlap with other objects. We propose an
amortized variational inference procedure to solve this instance of
small-object detection. Our key innovation is a family of spatially
autoregressive variational distributions that partition and order the latent
space according to a $K$-color checkerboard pattern. By construction, the
conditional independencies of this variational family mirror those of the
posterior distribution. We fit the variational distribution, which is
parameterized by a convolutional neural network, using neural posterior
estimation (NPE) to minimize an expectation of the forward KL divergence. Using
images from the Sloan Digital Sky Survey, our method achieves state-of-the-art
performance. We further demonstrate that the proposed autoregressive structure
greatly improves posterior calibration.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.03074v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Data-Driven Bed Occupancy Planning in Intensive Care Units Using
  $M_t/G_t/\infty$ Queueing Models</h3>
                    <p><strong>Authors:</strong> Maryam Akbari-Moghaddam, Douglas G. Down, Na Li, Catherine Eastwood, Ayman Abou Mehrem, Alexandra Howlett</p>
                    <p>  Hospitals struggle to make effective long-term capacity planning decisions
for intensive care units (ICUs) under uncertainty in future demand. Admission
rates fluctuate over time due to temporal factors, and length of stay (LOS)
distributions vary with patient heterogeneity, hospital location, case mix, and
clinical practices. Common planning approaches rely on steady-state queueing
models or heuristic rules that assume fixed parameters, but these methods often
fall short in capturing real-world occupancy dynamics. One widely used example
is the 85\% occupancy rule, which recommends maintaining average utilization
below this level to ensure responsiveness; however, this rule is based on
stationary assumptions and may be unreliable when applied to time-varying
systems. Our analysis shows that even when long-run utilization targets are
met, day-to-day occupancy frequently exceeds 100\% capacity.
  We propose a data-driven framework for estimating ICU bed occupancy using an
$M_t/G_t/\infty$ queueing model, which incorporates time-varying arrival rates
and empirically estimated LOS distributions. The framework combines statistical
decomposition and parametric distribution fitting to capture temporal patterns
in ICU admissions and LOS. We apply it to multi-year data from neonatal ICUs
(NICUs) in Calgary as a case study. Several capacity planning scenarios are
evaluated, including average-based thresholds and surge estimates from Poisson
overflow approximations. Results demonstrate the inadequacy of static
heuristics in environments with fluctuating demand and highlight the importance
of modeling LOS variability when estimating bed needs. Although the case study
focuses on NICUs, the methodology generalizes to other ICU settings and
provides interpretable, data-informed support for healthcare systems facing
rising demand and limited capacity.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.02852v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Deciphering the influence of demographic factors on the treatment of
  pediatric patients in the emergency department</h3>
                    <p><strong>Authors:</strong> Helena Coggan, Anne Bischops, Pradip Chaudhari, Yuval Barak-Corren, Andrew M. Fine, Ben Y. Reis, Jaya Aysola, William G. La Cava</p>
                    <p>  Persistent demographic disparities have been identified in the treatment of
patients seeking care in the emergency department (ED). These may be driven in
part by subconscious biases, which providers themselves may struggle to
identify. To better understand the operation of these biases, we performed a
retrospective cross-sectional analysis using electronic health records
describing 339,400 visits to the ED of a single US pediatric medical center
between 2019-2024. Odds ratios were calculated using propensity-score matching.
Analyses were adjusted for confounding variables, including chief complaint,
insurance type, socio-economic deprivation, and patient comorbidities. We also
trained a machine learning [ML] model on this dataset to identify predictors of
admission. We found significant demographic disparities in admission
(Non-Hispanic Black [NHB] relative to Non-Hispanic White [NHW]: OR 0.77, 95\%
CI 0.73-0.81; Hispanic relative to NHW: OR 0.80, 95\% CI 0.76-0.83). We also
identified disparities in individual decisions taken during the ED stay. For
example, NHB patients were significantly less likely than NHW patients to be
assigned an `emergent' triage acuity score of (OR 0.70, 95\% CI 0.67-0.72), but
emergent NHB patients were also significantly less likely to be admitted than
NHW patients with the same triage acuity (OR 0.86, 95\% CI 0.80-0.93).
Demographic disparities were particularly acute wherever patients had normal
vital signs, public insurance, moderate socio-economic deprivation, or a home
address distant from the hospital. An ML model assigned higher importance to
triage score for NHB than NHW patients when predicting admission, reflecting
these disparities in assignment. We conclude that many visit characteristics,
clinical and otherwise, may influence the operation of subconscious biases and
affect ML-driven decision support tools.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.02841v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Mutual Information-Driven Visualization and Clustering for Core KPI
  Selection in O-RAN Testing</h3>
                    <p><strong>Authors:</strong> Anish Pradhan, Lingjia Liu, Harpreet S. Dhillon</p>
                    <p>  O-RAN testing is becoming increasingly difficult with the exponentially
growing number of performance measurements as the system grows more complex,
with additional units, interfaces, applications, and possible implementations
and configurations. To simplify the testing procedure and improve system design
for O-RAN systems, it is important to identify the dependencies among various
performance measurements, which are inherently time-series and can be modeled
as realizations of random processes. While information theory can be utilized
as a principled foundation for mapping these dependencies, the robust
estimation of such measures for random processes from real-world data remains
challenging. This paper introduces AMIF-MDS, which employs aggregate mutual
Information in frequency (AMIF), a practical proxy for directed information
(DI), to quantify similarity and visualize inter-series dependencies with
multidimensional scaling (MDS). The proposed quantile-based AMIF estimator is
applied to O-RAN time-series testing data to identify dependencies among
various performance measures so that we can focus on a set of ``core''
performance measures. Applying density-based spatial clustering of applications
with noise (DBSCAN) to the MDS embedding groups mutually informative metrics,
organically reveals the link-adaptation indicators among other clusters, and
yields a ``core'' performance measure set for future learning-driven O-RAN
testing.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.02696v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Model Falsification for Predicting Dynamical Responses of Uncertain
  Structural Systems</h3>
                    <p><strong>Authors:</strong> Subhayan De, Tianhao Yu, Patrick T. Brewick, Erik A. Johnson, Steven F. Wojtkiewicz</p>
                    <p>  Accurate prediction of dynamical response of structural system depends on the
correct modeling of that system. However, modeling becomes increasingly
challenging when there are many candidate models available to describe the
system behavior. Furthermore, uncertainties can be present even for the
parameters of these model classes. The plausibility of each input-output model
class of the structures with uncertain components can be determined by a
Bayesian approach from measured dynamic responses to one or more input records;
predictions of the structural system response to alternate input records can
then be made. However, this approach may require many model simulations, even
though most of those model classes are quite implausible. An approach is
proposed herein to use a bound, computed from the false discovery rate, on the
likelihood of measured data to falsify models considering uncertainties in the
passive control devices that do not reproduce the measured data to sufficient
accuracy. Response prediction is then performed using the unfalsified models in
an approximate Bayesian sense by assigning weights, computed from the
likelihoods, only to the unfalsified models approach incurring only a fraction
of the computational cost of the standard Bayesian approach. The proposed
approach for response prediction is illustrated using three structural
examples: an earthquake-excited four--degree-of-freedom building model with a
hysteretic isolation layer; a 1623--degree-of-freedom three-dimensional
building model, with tuned mass dampers attached to its roof, subjected to wind
loads; and a full-scale four-story base-isolated building tested on world's
largest shake table in Japan's E-Defense lab. The results exhibit accurate
response predictions and significant computational savings, thereby
illustrating the potential of the proposed method.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.02612v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Multivariate distributional modeling of low, moderate, and large
  intensities without threshold selection steps</h3>
                    <p><strong>Authors:</strong> Carlo Gaetan, Philippe Naveau</p>
                    <p>  In fields such as hydrology and climatology, modelling the entire
distribution of positive data is essential, as stakeholders require insights
into the full range of values, from low to extreme. Traditional approaches
often segment the distribution into separate regions, which introduces
subjectivity and limits coherence. This is especially true when dealing with
multivariate data.
  In line with multivariate extreme value theory, this paper presents a
unified, threshold-free framework for modelling marginal behaviours and
dependence structures based on an extended generalized Pareto distribution
(EGPD). We propose decomposing multivariate data into radial and angular
components. The radial component is modelled using a semi-parametric EGPD and
the angular distribution is permitted to vary conditionally. This approach
allows for sufficiently flexible dependence modelling.
  The hierarchical structure of the model facilitates the inference process.
First, we combine classical maximum likelihood estimation (MLE) methods with
semi-parametric approaches based on Bernstein polynomials to estimate the
distribution of the radial component. Then, we use multivariate regression
techniques to estimate the angular component's parameters.
  The model is evaluated through synthetic simulations and applied to
hydrological datasets to exemplify its capacity to capture heavy-tailed
marginals and complex multivariate dependencies without threshold
specification.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.02152v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>How to Find Fantastic Papers: Self-Rankings as a Powerful Predictor of
  Scientific Impact Beyond Peer Review</h3>
                    <p><strong>Authors:</strong> Buxin Su, Natalie Collina, Garrett Wen, Didong Li, Kyunghyun Cho, Jianqing Fan, Bingxin Zhao, Weijie Su</p>
                    <p>  Peer review in academic research aims not only to ensure factual correctness
but also to identify work of high scientific potential that can shape future
research directions. This task is especially critical in fast-moving fields
such as artificial intelligence (AI), yet it has become increasingly difficult
given the rapid growth of submissions. In this paper, we investigate an
underexplored measure for identifying high-impact research: authors' own
rankings of their multiple submissions to the same AI conference. Grounded in
game-theoretic reasoning, we hypothesize that self-rankings are informative
because authors possess unique understanding of their work's conceptual depth
and long-term promise. To test this hypothesis, we conducted a large-scale
experiment at a leading AI conference, where 1,342 researchers self-ranked
their 2,592 submissions by perceived quality. Tracking outcomes over more than
a year, we found that papers ranked highest by their authors received twice as
many citations as their lowest-ranked counterparts; self-rankings were
especially effective at identifying highly cited papers (those with over 150
citations). Moreover, we showed that self-rankings outperformed peer review
scores in predicting future citation counts. Our results remained robust after
accounting for confounders such as preprint posting time and self-citations.
Together, these findings demonstrate that authors' self-rankings provide a
reliable and valuable complement to peer review for identifying and elevating
high-impact research in AI.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.02143v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Multidata Causal Discovery for Statistical Hurricane Intensity
  Forecasting</h3>
                    <p><strong>Authors:</strong> Saranya Ganesh S., Frederick Iat-Hin Tam, Milton S. Gomez, Marie McGraw, Mark DeMaria, Kate Musgrave, Jakob Runge, Tom Beucler</p>
                    <p>  Improving statistical forecasts of Atlantic hurricane intensity is limited by
complex nonlinear interactions and difficulty in identifying relevant
predictors. Conventional methods prioritize correlation or fit, often
overlooking confounding variables and limiting generalizability to unseen
tropical storms. To address this, we leverage a multidata causal discovery
framework with a replicated dataset based on Statistical Hurricane Intensity
Prediction Scheme (SHIPS) using ERA5 meteorological reanalysis. We conduct
multiple experiments to identify and select predictors causally linked to
hurricane intensity changes. We train multiple linear regression models to
compare causal feature selection with no selection, correlation, and random
forest feature importance across five forecast lead times from 1 to 5 days (24
to 120 hours). Causal feature selection consistently outperforms on unseen test
cases, especially for lead times shorter than 3 days. The causal features
primarily include vertical shear, mid-tropospheric potential vorticity and
surface moisture conditions, which are physically significant yet often
underutilized in hurricane intensity predictions. Further, we build an extended
predictor set (SHIPS+) by adding selected features to the standard SHIPS
predictors. SHIPS+ yields increased short-term predictive skill at lead times
of 24, 48, and 72 hours. Adding nonlinearity using multilayer perceptron
further extends skill to longer lead times, despite our framework being purely
regional and not requiring global forecast data. Operational SHIPS tests
confirm that three of the six added causally discovered predictors improve
forecasts, with the largest gains at longer lead times. Our results demonstrate
that causal discovery improves hurricane intensity prediction and pave the way
toward more empirical forecasts.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.02050v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Z-scores-based methods and their application to biological monitoring:
  An extended analysis of professional soccer players and cyclists athletes</h3>
                    <p><strong>Authors:</strong> Geoffroy C. B. Berthelot, Brigitte Gelein, Eric Meinadier, Emmanuel Orhant, Jérôme Dedecker</p>
                    <p>  The increase in the collection of biological data allows for the individual
and longitudinal monitoring of hematological or urine biomarkers. However,
identifying abnormal behavior in these biological sequences is not trivial.
Moreover, the complexity of the biological data (correlation between
biomarkers, seasonal effects, etc.) is also an issue. Z-score methods can help
assess the abnormality in these longitudinal sequences while capturing some
features of the biological complexity. This work details a statistical
framework for handling biological sequences using three custom Z-score methods
in the intra-individual variability scope. These methods can detect abnormal
samples in the longitudinal sequences with respect to the seasonality,
chronological time or correlation between biomarkers. One of these methods is
an extension of one custom Z-score method to the Gaussian linear model, which
allows for including additional variables in the model design. We illustrate
the use of the framework on the longitudinal data of 3,936 professional soccer
players (5 biomarkers) and 1,683 amateur or professional cyclists (10
biomarkers). The results show that a particular Z-score method, designed to
detect a change in a series of consecutive observations, measured a high
proportion of abnormal values (more than three times the false positive rate)
in the ferritin and IGF1 biomarkers for both data sets. The proposed framework
and methods could be applied in other contexts, such as the clinical patient
follow-up in monitoring abnormal values of biological markers. The methods are
flexible enough to include more complicated biological features, which can be
directly incorporated into the model design.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.01810v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Dependent stochastic block models for age-indexed sequences of directed
  causes-of-death networks</h3>
                    <p><strong>Authors:</strong> Giovanni Romanò, Cristian Castiglione, Daniele Durante</p>
                    <p>  Death events commonly arise from complex interactions among interrelated
causes, formally classified in reporting practices as underlying and
contributing. Leveraging information from death certificates, these
interactions can be naturally represented through a sequence of directed
networks encoding co-occurrence strengths between pairs of underlying and
contributing causes across ages. Although this perspective opens the avenues to
learn informative age-specific block interactions among endogenous groups of
underlying and contributing causes displaying similar co-occurrence patterns,
there has been limited research along this direction in mortality modeling.
This is mainly due to the lack of suitable stochastic block models for
age-indexed sequences of directed networks. We cover this gap through a novel
Bayesian formulation which crucially learns two separate group structures for
underlying and contributing causes, while allowing both structures to change
smoothly across ages via dependent random partition priors. As illustrated in
simulations, this formulation outperforms state-of-the-art solutions that could
be adapted to our motivating application. Moreover, when applied to USA
mortality data, it unveils structures in the composition, evolution, and
modular interactions among causes-of-death groups that were hidden to previous
studies. Such findings could have relevant policy implications and contribute
to an improved understanding of the recent "death of despair" phenomena in USA.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.01806v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>