<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 3/3/2025, 12:21:44 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>A decision analysis model for colorectal cancer screening</h3>
                    <p><strong>Authors:</strong> Daniel Corrales, David Ríos Insua, Marino J. González</p>
                    <p>  Background and Objective. With minor differences, most national colorectal
cancer (CRC) screening programs in Europe consist of one-size-fits-all
aged-based strategies. This paper provides a decision analysis-based approach
to personalized CRC screening, supporting decisions concerning whether and
which screening method to consider and/or whether a colonoscopy should be
administered.
  Methods. We use an influence diagram which characterizes CRC risk with
respect to different variables of interest and includes comfort, costs,
complications, and information as decision criteria, the last one assessed
through information theory measures. The criteria are integrated with a
multi-attribute utility model. Optimal screening policies are then computed.
  Results. The proposed model is used to support personalized individual
screening based on relevant characteristics. It serves to assess existing
national screening programs and design new ones. In particular, it suggests
replacing current age-based strategies followed in many European countries by
more personalized strategies based on the type of model proposed. Additionally,
the model facilitates benchmarking of novel screening devices.
  Conclusions. This work creates a framework supporting personalized CRC
screening improving upon current age-based screening strategies.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.21210v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Negative correlations in Ising models of credit risk</h3>
                    <p><strong>Authors:</strong> Chiara Emonti, Roberto Fontana</p>
                    <p>  We analyze a subclass of Ising models in the context of credit risk, focusing
on Dandelion models when the correlations $\rho$ between the central node and
each non-central node are negative. We establish the possible range of values
for $\rho$ and derive an explicit formula linking the correlation between any
pair of non-central nodes to $\rho$. The paper concludes with a simulation
study.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.21199v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Forecasting Monthly Residential Natural Gas Demand Using
  Just-In-Time-Learning Modeling</h3>
                    <p><strong>Authors:</strong> Burak Alakent, Erkan Isikli, Cigdem Kadaifci, Tonguc S. Taspinar</p>
                    <p>  Natural gas (NG) is relatively a clean source of energy, particularly
compared to fossil fuels, and worldwide consumption of NG has been increasing
almost linearly in the last two decades. A similar trend can also be seen in
Turkey, while another similarity is the high dependence on imports for the
continuous NG supply. It is crucial to accurately forecast future NG demand
(NGD) in Turkey, especially, for import contracts; in this respect, forecasts
of monthly NGD for the following year are of utmost importance. In the current
study, the historical monthly NG consumption data between 2014 and 2024
provided by SOCAR, the local residential NG distribution company for two cities
in Turkey, Bursa and Kayseri, was used to determine out-of-sample monthly NGD
forecasts for a period of one year and nine months using various time series
models, including SARIMA and ETS models, and a novel proposed machine learning
method. The proposed method, named Just-in-Time-Learning-Gaussian Process
Regression (JITL-GPR), uses a novel feature representation for the past NG
demand values; instead of using past demand values as column-wise separate
features, they are placed on a two-dimensional (2-D) grid of year-month values.
For each test point, a kernel function, tailored for the NGD predictions, is
used in GPR to predict the query point. Since a model is constructed separately
for each test point, the proposed method is, indeed, an example of JITL. The
JITL-GPR method is easy to use and optimize, and offers a reduction in forecast
errors compared to traditional time series methods and a state-of-the-art
combination model; therefore, it is a promising tool for NGD forecasting in
similar settings.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.20989v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Nonparanormal Modeling Framework for Prognostic Biomarker Assessment
  with Application to Amyotrophic Lateral Sclerosis</h3>
                    <p><strong>Authors:</strong> Ainesh Sewak, Vanda Inacio, Joanne Wuu, Michael Benatar, Torsten Hothorn</p>
                    <p>  Identifying reliable biomarkers for predicting clinical events in
longitudinal studies is important for accurate disease prognosis and the
development of new treatments. However, prognostic studies are often not
randomized, making it difficult to account for patient heterogeneity. In
amyotrophic lateral sclerosis (ALS), factors such as age, site of disease onset
and genetics impact both survival duration and biomarker levels, yet their
impact on the prognostic accuracy of biomarkers over different time horizons
remains unclear. While existing methods for time-dependent receiver operating
characteristic (ROC) analysis have been adapted for censored time-to-event
outcomes, most do not adjust for patient covariates. To address this, we
propose the nonparanormal prognostic biomarker (NPB) framework, which models
the joint dependence between biomarker and event time distributions while
accounting for covariates. This provides covariate-specific ROC curves which
assess a potential biomarker's accuracy for a given time horizon. We apply this
framework to evaluate serum neurofilament light (NfL) as a biomarker in ALS and
demonstrate that its prognostic accuracy varies over time and across patient
subgroups. The NPB framework is broadly applicable to other conditions and has
the potential to improve clinical trial efficiency by refining patient
stratification and reducing sample size requirements.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.20892v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Adding smoothing splines to the SAM model improves stock assessment</h3>
                    <p><strong>Authors:</strong> Silius M. Vandeskog, Magne Aldrin, Daniel Howell, Edvin Fuglebakk</p>
                    <p>  The stock assessment model SAM contains a large number of age-dependent
parameters that must be manually grouped together to obtain robust inference.
This can make the model selection process slow, non-extensive and highly
subjective, while producing unrealistic looking parameter estimates with
discrete jumps. We propose to model age-dependent SAM parameters using
smoothing spline functions. This can lead to more smooth parameter estimates,
while speeding up and making the model selection process more automatic and
less subjective. We develop different spline models and compare them with
already existing SAM models for a selection of 17 different fish stocks, using
cross- and forward-validation methods. The results show that our automated
spline models overall outcompete the officially developed SAM models. We also
demonstrate how the developed spline models can be employed as a diagnostics
tool for improving and better understanding properties of the officially
developed SAM models.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.20788v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Collective Reasoning Among LLMs A Framework for Answer Validation
  Without Ground Truth</h3>
                    <p><strong>Authors:</strong> Seyed Pouyan Mousavi Davoudi, Alireza Shafiee Fard, Alireza Amiri-Margavi</p>
                    <p>  We present a collaborative framework where multiple large language models,
namely GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and
Gemini-1.5-Flash, work together to generate and respond to complex PhD-level
probability questions in the absence of definitive ground truth. This study
explores how inter-model consensus enhances response reliability and serves as
a proxy for assessing the quality of generated questions. To quantify agreement
and consistency, we employ statistical methods including chi-square tests,
Fleiss' Kappa, and confidence interval analysis, measuring both response
precision and question clarity. Our findings highlight that Claude and Gemini
generate well-structured and less ambiguous questions, leading to higher
inter-model agreement. This is reflected in their narrower confidence intervals
and stronger alignment with answering models. Conversely, LLaMA demonstrates
increased variability and lower reliability in question formulation, as
indicated by broader confidence intervals and reduced consensus rates. These
results suggest that multi-model collaboration not only enhances the
reliability of responses but also provides a valuable framework for assessing
and improving question quality in the absence of explicit ground truth. This
research offers meaningful insights into optimizing AI-driven reasoning through
collaborative large-language model interactions.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.20758v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Modelling the Spatially Varying Non-Linear Effects of Heat Exposure</h3>
                    <p><strong>Authors:</strong> Xinyi Chen, Marta Blangiardo, Connor Gascoigne, Garyfallos Konstantinoudis</p>
                    <p>  Exposure to high ambient temperatures is a significant driver of preventable
mortality, with non-linear health effects and elevated risks in specific
regions. To capture this complexity and account for spatial dependencies across
small areas, we propose a Bayesian framework that integrates non-linear
functions with the Besag, York, and Mollie (BYM2) model. Applying this
framework to all-cause mortality data in Switzerland, we quantified spatial
inequalities in heat-related mortality. We retrieved daily all-cause mortality
at small areas (2,145 municipalities) for people older than 65 years from the
Swiss Federal Office of Public Health and daily mean temperature at
1km$\times$1km grid from the Swiss Federal Office of Meteorology. By fully
propagating uncertainties, we derived key epidemiological metrics, including
heat-related excess mortality and minimum mortality temperature (MMT).
Heat-related excess mortality rates were higher in northern Switzerland, while
lower MMTs were observed in mountainous regions. Further, we explored the role
of the proportion of individuals older than 85 years, green space, average
temperature, deprivation, urbanicity, and language regions in explaining these
discrepancies. We found that spatial disparities in heat-related excess
mortality were primarily driven by population age distribution, green space,
and vulnerabilities associated with elevated temperature exposure.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.20745v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Modeling times to multiple events under informative censoring with
  C-vine copula</h3>
                    <p><strong>Authors:</strong> Xinyuan Chen, Yiwei Li, Qian M. Zhou</p>
                    <p>  The study of times to nonterminal events of different types and their
interrelation is a compelling area of interest. The primary challenge in
analyzing such multivariate event times is the presence of informative
censoring by the terminal event. While numerous statistical methods have been
proposed for a single nonterminal event, i.e., semi-competing risks data, there
remains a dearth of tools for analyzing times to multiple nonterminal events.
These events involve more complex dependence structures between nonterminal and
terminal events and between nonterminal events themselves. This paper
introduces a novel modeling framework leveraging the vine copula to directly
estimate the joint distribution of the multivariate times to nonterminal and
terminal events. Unlike the few existing methods based on multivariate or
nested copulas, our model excels in capturing the heterogeneous dependence
between each pair of event times in terms of strength and structure.
Furthermore, our model allows regression modeling for all the marginal
distributions of times to nonterminal and terminal events, a feature lacking in
existing methods. We propose a likelihood-based estimation and inference
procedure, which can be implemented efficiently in sequential stages. Through
simulation studies, we demonstrate the satisfactory finite-sample performance
of our proposed stage-wise estimators and analytical variance estimators, as
well as their superiority over existing methods. We apply our approach to data
from a crowdfunding platform to investigate the relationship between
creator-backer interactions of various types and a creator's lifetime on the
platform.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.20608v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A review of Bayesian sensor-based estimation and uncertainty
  quantification of aerodynamic flows</h3>
                    <p><strong>Authors:</strong> Jeff D. Eldredge, Hanieh Mousavi</p>
                    <p>  Many applications in aerodynamics depend on the use of sensors to estimate
the evolving state of the flow. In particular, a wide variety of traditional
and learning-based strategies for closed-loop control rely on some knowledge of
the aerodynamic state in order to decide on actions. This estimation task is
inherently accompanied by uncertainty due to the noisy measurements of sensors
or the non-uniqueness of the underlying mapping, and knowledge of this
uncertainty can be as important for decision-making as that of the state
itself. The tracking of uncertainty is challenged by the often-nonlinear
relationship between the sensor measurements and the flow state. For example, a
collection of passing vortices leaves a footprint in wall pressure sensors that
depends nonlinearly on the strengths and positions of the vortices. In this
paper, we will review the recent body of work on flow estimation. We will
discuss the basic tools of probability, including sampling and estimation, in
the powerful setting of Bayesian inference and demonstrate these tools in
static flow estimation examples. We will then proceed to unsteady examples and
illustrate the application of sequential estimation, and particularly, the
ensemble Kalman filter. Finally, we will discuss uncertainty quantification in
neural network approximations of the mappings between sensor measurements and
flow states. Recent aerodynamic applications of neural networks have shown that
the flow state can be encoded into a very low-dimensional latent space, and we
will discuss the implications of this encoding on uncertainty.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.20280v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A New Method for High-Resolution Dating of Radiocarbon Data: The Example
  of the First Three Centuries B.C</h3>
                    <p><strong>Authors:</strong> Sebastian Fürst</p>
                    <p>  Radiocarbon dating poses a challenge in many archaeological contexts due to
the limited precision of conventional calibration methods. In this study, we
introduce a novel approach to fine-dating that is based on the repeated
application of OxCal's R_Simulate function. By constructing extensive reference
tables and aggregating measures of central tendency (means and medians),
uncalibrated 14C measurements are directly mapped to calendar dates. The method
is validated through comprehensive simulations and comparisons with
dendrochronologically dated tree rings. Despite challenges in segments of the
calibration curve with low gradients, the approach demonstrates that a
significant improvement in dating precision is achievable. Limitations and
potential avenues for further methodological optimisation are discussed.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.20211v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>