<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 7/14/2025, 1:30:07 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Propensity score with factor loadings: the effect of the Paris Agreement</h3>
                    <p><strong>Authors:</strong> Angelo Forino, Andrea Mercatanti, Giacomo Morelli</p>
                    <p>  Factor models for longitudinal data, where policy adoption is unconfounded
with respect to a low-dimensional set of latent factor loadings, have become
increasingly popular for causal inference. Most existing approaches, however,
rely on a causal finite-sample approach or computationally intensive methods,
limiting their applicability and external validity. In this paper, we propose a
novel causal inference method for panel data based on inverse propensity score
weighting where the propensity score is a function of latent factor loadings
within a framework of causal inference from super-population. The approach
relaxes the traditional restrictive assumptions of causal panel methods, while
offering advantages in terms of causal interpretability, policy relevance, and
computational efficiency. Under standard assumptions, we outline a three-step
estimation procedure for the ATT and derive its large-sample properties using
Mestimation theory. We apply the method to assess the causal effect of the
Paris Agreement, a policy aimed at fostering the transition to a low-carbon
economy, on European stock returns. Our empirical results suggest a
statistically significant and negative short-run effect on the stock returns of
firms that issued green bonds.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.08764v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Influence of river incision on landslides triggered in Nepal by the
  Gorkha earthquake: Results from a pixel-based susceptibility model using
  inlabru</h3>
                    <p><strong>Authors:</strong> Man Ho Suen, Mark Naylor, Simon Mudd, Finn Lindgren</p>
                    <p>  This study presents a comprehensive framework for modelling
earthquake-induced landslides (EQILs) through a channel-based analysis of
landslide centroid distributions. A key innovation is the incorporation of the
normalised channel steepness index ($k_{sn}$) as a physically meaningful and
novel covariate, inferring hillslope erosion and fluvial incision processes.
Used within spatial point process models, $k_{sn}$ supports the generation of
landslide susceptibility maps with quantified uncertainty. To address spatial
data misalignment between covariates and landslide observations, we leverage
the inlabru framework, which enables coherent integration through mesh-based
disaggregation, thereby overcoming challenges associated with spatially
misaligned data integration. Our modelling strategy explicitly prioritises
prospective transferability to unseen geographical regions, provided that
explanatory variable data are available. By modelling both landslide locations
and sizes, we find that elevated $k_{sn}$ is strongly associated with increased
landslide susceptibility but not with landslide magnitude. The best-fitting
Bayesian model, validated through cross-validation, offers a scalable and
interpretable solution for predicting earthquake-induced landslides in complex
terrain.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.08742v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Sensitivity measures for engineering and environmental decision support</h3>
                    <p><strong>Authors:</strong> Daniel Straub, Wolfgang Betz, Mara Ruf, Amelie Hoffmann, Angela Landgraf, Lea Friedli, Iason Papaioannou</p>
                    <p>  Information value, a measure for decision sensitivity, can provide essential
information in engineering and environmental assessments. It quantifies the
potential for improved decision-making when reducing uncertainty in specific
inputs. By contrast to other sensitivity measures, it admits not only a
relative ranking of input factors but also an absolute interpretation through
statements like ''Eliminating the uncertainty in factor $A$ has an expected
value of $5000$ Euro''. In this paper, we present a comprehensive overview of
the information value by presenting the theory and methods in view of their
application to engineering and environmental assessments. We show how one
should differentiate between aleatory and epistemic uncertainty in the
analysis. Furthermore, we introduce the evaluation of the information value in
applications where the decision is described by a continuous parameter. The
paper concludes with two real-life applications of the information value to
highlight its power in supporting decision-making in engineering and
environmental applications.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.08488v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Modeling Wallet-Level Behavioral Shifts Post-FTX Collapse: An XAI-Driven
  GLM Study on Ethereum Transactions</h3>
                    <p><strong>Authors:</strong> Benjamin Gillen, Rashmi Ranjan Bhuyan, Gourab Mukherjee, Austin Pollok</p>
                    <p>  The Ethereum blockchain plays a central role in the broader cryptocurrency
ecosystem, enabling a wide range of financial activity through the use of smart
contracts. This paper investigates how individual Ethereum wallets responded to
the collapse of FTX, one of the largest centralized cryptocurrency exchanges.
Moving beyond price-based event studies, we adopt a bottom-up approach using
granular wallet-level data. We construct a representative sample of Ethereum
addresses and analyze their transaction behavior before and after the collapse
using an explainable artificial intelligence (XAI) framework. Our proposed
framework addresses data scarcity in high-resolution wallet-level daily
transactions by employing a calibrated zero-inflated generalized linear fixed
effects model. Our analysis quantifies distinct shifts in transaction intensity
and stablecoin usage, highlighting a flight to safety within the ecosystem.
These findings underscore the value of a bottom-up methodology for quantifying
the user-level impact of blockchain-based shocks, offering insights beyond
traditional price-level analysis through wallet-level data.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.08455v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Optimal Experimental Design for Microplastics Sampling Experiments</h3>
                    <p><strong>Authors:</strong> Marco A. Aquino-López, Ana Carolina Ruiz-Fernández, Joan-Albert Sanchez-Cabeza, J. Andrés Christen</p>
                    <p>  Microplastics contamination is one of the most rapidly growing research
topics. However, monitoring microplastics contamination in the environment
presents both logistical and statistical challenges, particularly when
constrained resources limit the scale of sampling and laboratory analysis. In
this paper, we propose a Bayesian framework for the optimal experimental design
of microplastic sampling campaigns. Our approach integrates prior knowledge and
uncertainty quantification to guide decisions on how many spatial Centrosamples
to collect and how many particles to analyze for polymer composition. By
modeling particle counts as a Poisson distribution and polymer types as a
Multinomial distribution, we developed a conjugate Bayesian model that enables
efficient posterior inference. We introduce variance-based loss functions to
evaluate expected information gain for both abundance and composition, and we
formulate a constrained optimization problem that incorporates realistic cost
structures. Our results provide principled and interpretable recommendations
for allocating limited resources across the sampling and analysis phases.
Through simulated scenarios and real-world-inspired examples, we demonstrate
how the proposed methodology adapts to prior assumptions and cost variations,
ensuring robustness and flexibility. This work contributes to the broader field
of Bayesian experimental design by offering a concrete, application-driven case
study that underscores the value of formal design strategies in environmental
monitoring contexts.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.08170v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Uncertainty quantification of a multi-component Hall thruster model at
  varying facility pressures</h3>
                    <p><strong>Authors:</strong> Thomas A. Marks, Joshua D. Eckels, Gabriel E. Mora, Alex A. Gorodetsky</p>
                    <p>  Bayesian inference is applied to calibrate and quantify prediction
uncertainty in a coupled multi-component Hall thruster model at varying
facility background pressures. The model, consisting of a cathode model,
discharge model, and plume model, is used to simulate two thrusters across a
range of background pressures in multiple vacuum test facilities. The model
outputs include thruster performance metrics, one-dimensional plasma
properties, and the angular distribution of the current density in the plume.
The simulated thrusters include a magnetically shielded thruster, the H9, and
an unshielded thruster, the SPT-100. After calibration, the model captures
several key performance trends with background pressure, including changes in
thrust and upstream shifts in the ion acceleration region. Furthermore, the
model exhibits predictive accuracy to within 10\% when evaluated on flow rates
and pressures not included in the training data, and the model can predict some
performance characteristics across test facilities to within the same range.
Evaluated on the same data as prior work [Eckels et al. 2024], the model
reduced predictive errors in thrust and discharge current by greater than 50%.
An extrapolation to on-orbit performance is performed with an error of 9\%,
capturing trends in discharge current but not thrust. Possible extensions and
improvements are discussed in the context of using data for predictive Hall
thruster modeling across vacuum facilities.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.08113v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A new coefficient to measure agreement between continuous variables</h3>
                    <p><strong>Authors:</strong> Ronny Vallejos, Felipe Osorio, Clemente Ferrer</p>
                    <p>  Assessing agreement between two instruments is crucial in clinical studies to
evaluate the similarity between two methods measuring the same subjects. This
paper introduces a novel coefficient, termed rho1, to measure agreement between
continuous variables, focusing on scenarios where two instruments measure
experimental units in a study. Unlike existing coefficients, rho1 is based on
L1 distances, making it robust to outliers and not relying on nuisance
parameters. The coefficient is derived for bivariate normal and elliptically
contoured distributions, showcasing its versatility. In the case of normal
distributions, rho1 is linked to Lin's coefficient, providing a useful
alternative. The paper includes theoretical properties, an inference framework,
and numerical experiments to validate the performance of rho1. This novel
coefficient presents a valuable tool for researchers assessing agreement
between continuous variables in various fields, including clinical studies and
spatial analysis.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.07913v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Efficient Causal Discovery for Autoregressive Time Series</h3>
                    <p><strong>Authors:</strong> Mohammad Fesanghary, Achintya Gopal</p>
                    <p>  In this study, we present a novel constraint-based algorithm for causal
structure learning specifically designed for nonlinear autoregressive time
series. Our algorithm significantly reduces computational complexity compared
to existing methods, making it more efficient and scalable to larger problems.
We rigorously evaluate its performance on synthetic datasets, demonstrating
that our algorithm not only outperforms current techniques, but also excels in
scenarios with limited data availability. These results highlight its potential
for practical applications in fields requiring efficient and accurate causal
inference from nonlinear time series data.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.07898v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Novel Hybrid Approach for Time Series Forecasting: Period Estimation
  and Climate Data Analysis Using Unsupervised Learning and Spline
  Interpolation</h3>
                    <p><strong>Authors:</strong> Tanmay Kayal, Abhishek Das, U Saranya</p>
                    <p>  This article explores a novel approach to time series forecasting applied to
the context of Chennai's climate data. Our methodology comprises two distinct
established time series models, leveraging their strengths in handling
seasonality and periods. Notably, a new algorithm is developed to compute the
period of the time series using unsupervised machine learning and spline
interpolation techniques. Through a meticulous ensembling process that combines
these two models, we achieve optimized forecasts. This research contributes to
advancing forecasting techniques and offers valuable insights into climate data
analysis.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.07652v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Functional Time Series Forecasting of Distributions: A
  Koopman-Wasserstein Approach</h3>
                    <p><strong>Authors:</strong> Ziyue Wang, Yuko Araki</p>
                    <p>  We propose a novel method for forecasting the temporal evolution of
probability distributions observed at discrete time points. Extending the
Dynamic Probability Density Decomposition (DPDD), we embed distributional
dynamics into Wasserstein geometry via a Koopman operator framework. Our
approach introduces an importance-weighted variant of Extended Dynamic Mode
Decomposition (EDMD), enabling accurate, closed-form forecasts in 2-Wasserstein
space. Theoretical guarantees are established: our estimator achieves spectral
convergence and optimal finite-sample Wasserstein error. Simulation studies and
a real-world application to U.S. housing price distributions show substantial
improvements over existing methods such as Wasserstein Autoregression. By
integrating optimal transport, functional time series modeling, and spectral
operator theory, DPDD offers a scalable and interpretable solution for
distributional forecasting. This work has broad implications for behavioral
science, public health, finance, and neuroimaging--domains where evolving
distributions arise naturally. Our framework contributes to functional data
analysis on non-Euclidean spaces and provides a general tool for modeling and
forecasting distributional time series.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.07570v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>