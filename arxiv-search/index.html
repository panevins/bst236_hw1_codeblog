<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 8/4/2025, 1:39:34 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Toward using explainable data-driven surrogate models for treating
  performance-based seismic design as an inverse engineering problem</h3>
                    <p><strong>Authors:</strong> Mohsen Zaker Esteghamati</p>
                    <p>  This study presents a methodology to treat performance-based seismic design
as an inverse engineering problem, where design parameters are directly derived
to achieve specific performance objectives. By implementing explainable machine
learning models, this methodology directly maps design variables and
performance metrics, tackling computational inefficiencies of performance-based
design. The resultant machine learning model is integrated as an evaluation
function into a genetic optimization algorithm to solve the inverse problem.
The developed methodology is then applied to two different inventories of steel
and concrete moment frames in Los Angeles and Charleston to obtain sectional
properties of frame members that minimize expected annualized seismic loss in
terms of repair costs. The results show high accuracy of the surrogate models
(e.g., R2> 90%) across a diverse set of building types, geometries, seismic
design, and site hazard, where the optimization algorithm could identify the
optimum values of members' properties for a fixed set of geometric variables,
consistent with engineering principles.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.00286v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Predicting Formula 1 Race Outcomes: Decomposing the Roles of Drivers and
  Constructors through Linear Modeling</h3>
                    <p><strong>Authors:</strong> Saurabh Rane</p>
                    <p>  Formula 1 performance is a combination of the car's ability and the driver's
ability. While a given race or season can tell you how well a car and driver
performed jointly, isolating the individual impact of the driver and
constructor remains challenging. This paper extends a Regularized Adjusted Plus
Minus (RAPM) methodology (Sill 2010), commonly used in basketball and hockey,
to parse out individual driver and constructor impact. It employs a
time-decayed ridge regression with LOESS (Jacoby 2000) smoothing to predict
race results for the Hybrid Engine Era (2014 - 2024). By measuring the
constructor and driver coefficients over time, we measure the relative
individual impact of driver and constructor throughout the period. Results show
that constructors explain 64.0% of the variance in race outcomes in the Hybrid
Engine Era. Additionally, constructors have increased importance in benchmarked
rank-agnostic cohorts (e.g., Top 10 points finishers) and decreased importance
in qualifying. By decomposing performance into individual driver and
constructor metrics, we create a robust framework for inter-constructor driver
comparisons that the Formula 1 points system obfuscates. Our work enhances the
understanding of driver and constructor contributions to race success, offering
valuable insights for strategic decision-making in Formula 1.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.00200v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>New Pilot-Study Design in Functional Data Analysis</h3>
                    <p><strong>Authors:</strong> Ping-Han Huang, Ming-Hung Kao</p>
                    <p>  Efficient data collection is essential in applied studies where frequent
measurements are costly, time-consuming, or burdensome. This challenge is
especially pronounced in functional data settings, where each subject is
observed at only a few time points due to practical constraints. Most existing
design approaches focus on selecting optimal time points for individual
subjects, typically relying on model parameters estimated from a pilot study.
However, the design of the pilot study itself has received limited attention.
We propose a framework for constructing pilot-study designs that support both
accurate trajectory recovery and effective planning of future designs. A search
algorithm is developed to generate such high-quality pilot-study designs.
Simulation studies and a real data application demonstrate that our approach
outperforms commonly used alternatives, highlighting its value in
resource-limited settings.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.00176v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>The effect of a new power interconnector on energy prices volatility:
  the case of Sicily</h3>
                    <p><strong>Authors:</strong> Francesco Lisi, Pierdomenico Duttilo, Marina Bertolini</p>
                    <p>  The energy transition process requires the transformation of energy markets
and substantial infrastructure investments. In electricity markets, in
particular, infrastructure developments can significantly influence market
performance. This study examines the impact of the introduction of the
"Sorgente-Rizziconi" interconnector, which was integrated into the Italian
electricity grid in May 2016 and enhanced power interconnection between Sicily
and the mainland. The analysis focusses on the interconnector's effects on
zonal price volatility within the Italian day-ahead electricity market. Prices
and volatility are modelled using two distinct heteroscedastic approaches: a
semi-parametric additive model and a fully non-parametric additive model. The
results obtained from both modelling strategies indicate that: (i) the
interconnector had a direct effect only on Sicily; (ii) it did not lead to a
reduction in the price level; and (iii) it contributed to an increase in the
baseline level of price volatility.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.23505v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Kilo-scale point-source inference using Parametric Cataloging</h3>
                    <p><strong>Authors:</strong> Gabriel H. Collin</p>
                    <p>  The estimation of the number of point-sources in the sky is one the oldest
problems in astronomy, yet an easy and efficient method for estimating the
uncertainty on these counts is still an open problem. Probabilistic cataloging
solves the general point-source inference problem, but the trans-dimensional
nature of the inference method requires a bespoke approach that is difficult to
scale. Here it is shown that probabilistic cataloging can be performed in a
fixed-dimensional framework called Parametric Cataloging under mild assumptions
on some of the priors. The method requires only a simple reparameterization of
the flux coordinates, yielding an accessible method that can be implemented in
most probabilistic programming environments. As the parameter space is
fixed-dimensional, off the shelf gradient based samplers can be employed which
allows the method to scale to tens of thousands of sources.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.23472v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Regime-Aware Conditional Neural Processes with Multi-Criteria Decision
  Support for Operational Electricity Price Forecasting</h3>
                    <p><strong>Authors:</strong> Abhinav Das, Stephan Schlüter</p>
                    <p>  This work integrates Bayesian regime detection with conditional neural
processes for 24-hour electricity price prediction in the German market. Our
methodology integrates regime detection using a disentangled sticky
hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to
daily electricity prices. Each identified regime is subsequently modeled by an
independent conditional neural process (CNP), trained to learn localized
mappings from input contexts to 24-dimensional hourly price trajectories, with
final predictions computed as regime-weighted mixtures of these CNP outputs. We
rigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated
auto-regressive (LEAR) models by integrating their forecasts into diverse
battery storage optimization frameworks, including price arbitrage, risk
management, grid services, and cost minimization. This operational utility
assessment revealed complex performance trade-offs: LEAR often yielded superior
absolute profits or lower costs, while DNN showed exceptional optimality in
specific cost-minimization contexts. Recognizing that raw prediction accuracy
doesn't always translate to optimal operational outcomes, we employed TOPSIS as
a comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified
LEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model
emerged as the most balanced and preferred solution for 2021, 2022 and 2023.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.00040v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Bayesian reliability acceptance sampling plan sampling plans under
  adaptive accelerated type-II censored competing risk data</h3>
                    <p><strong>Authors:</strong> Rathin Das, Soumya Roy, Biswabrata Pradhan</p>
                    <p>  In recent times, products have become increasingly complex and highly
reliable, so failures typically occur after long periods of operation under
normal conditions and may arise from multiple causes. This paper employs simple
step-stress partial accelerated life testing (SSSPALT) within the competing
risks framework to determine the Bayesian reliability acceptance sampling plan
(BRASP) under type-II censoring. Elevating the stress during the life test
incurs an additional cost that increases the cost of the life test. In this
context, an adaptive scenario is also considered in that sampling plan. The
adaptive scenario is as follows: the stress is increased after a certain time
if the number of failures up to that point is less than a pre-specified number
of failures. The Bayes decision function and Bayes risk are derived for the
general loss function. An optimal BRASP under that adaptive SSSPALT is obtained
for the quadratic loss function by minimizing Bayes risk. An algorithm is
provided to determine the optimal proposed BRASP. Further, comparative studies
are conducted between the proposed BRASP, the conventional non-accelerated
BRASP, and the conventional accelerated BRASP under type-II censoring to
evaluate the effectiveness of the proposed approach. Finally, the methodology
is illustrated using real data.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.23293v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Countering the Forgetting of Novel Health Information with 'Social
  Boosting'</h3>
                    <p><strong>Authors:</strong> Vaibhav Krishna, Nicholas A. Christakis</p>
                    <p>  To mitigate the adverse effects of low-quality or false information, studies
have shown the effectiveness of various intervention techniques through
debunking or so-called pre-bunking. However, the effectiveness of such
interventions can decay. Here, we investigate the role of the detailed social
structure of the local villages within which the intervened individuals live,
which provides opportunities for the targeted individuals to discuss and
internalize new knowledge. We evaluated this with respect to a critically
important topic, information about maternal and child health care, delivered
via a 22-month in-home intervention. Specifically, we examined the effect of
having friendship ties on the retention of knowledge interventions among
targeted individuals in 110 isolated Honduran villages. We hypothesize that
individuals who receive specific knowledge can internalize and consolidate this
information by engaging in social interactions where, for instance, they have
an opportunity to discuss it with others in the process. The opportunity to
explain information to others (knowledge sharing) promotes deeper cognitive
processing and elaborative encoding, which ultimately enhances memory
retention. We found that well-connected individuals within a social network
experience an enhanced effectiveness of knowledge interventions. These
individuals may be more likely to internalize and retain the information and
reinforce it in others, due to increased opportunities for social interaction
where they teach others or learn from them, a mechanism we refer to as "social
boosting". These findings underscore the role of social interactions in
reinforcing health knowledge interventions over the long term. We believe these
findings would be of interest to the health policy, the global health
workforce, and healthcare professionals focusing on disadvantaged populations
and UN missions on infodemics.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.23148v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Learning Smooth Populations of Parameters with Trial Heterogeneity</h3>
                    <p><strong>Authors:</strong> JungHo Lee, Valerio Baćak, Edward H. Kennedy</p>
                    <p>  We consider the classical problem of estimating the mixing distribution of
binomial mixtures, but under trial heterogeneity and smoothness. This problem
has been studied extensively when the trial parameter is homogeneous, but not
under the more general scenario of heterogeneous trials, and only within a low
smoothness regime, where the resulting rates are slow. Under the assumption
that the density is s-smooth, we derive fast error rates for the kernel density
estimator under trial heterogeneity that depend on the harmonic mean of the
trials. Importantly, even when reduced to the homogeneous case, our result
improves on the state-of-the-art rate of Ye and Bickel (2021). We also study
nonparametric estimation of the difference between two densities, which can be
smoother than the individual densities, in both i.i.d. and binomial-mixture
settings. Our work is motivated by an application in criminal justice:
comparing conviction rates of indigent representation in Pennsylvania. We find
that the estimated conviction rates for appointed counsel (court-appointed
private attorneys) are generally higher than those for public defenders,
potentially due to a confounding factor: appointed counsel are more likely to
take on severe cases.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.23140v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Watermark in the Classroom: A Conformal Framework for Adaptive AI Usage
  Detection</h3>
                    <p><strong>Authors:</strong> Yangxinyu Xie, Xuyang Chen, Zhimei Ren, Weijie J. Su</p>
                    <p>  As artificial intelligence tools become ubiquitous in education, maintaining
academic integrity while accommodating pedagogically beneficial AI assistance
presents unprecedented challenges. Current AI detection systems fail to control
false positive rates (FPR) and suffer from bias against minority student
groups, prompting institutional suspensions of these technologies. Watermarking
techniques offer statistical rigor through precise $p$-values but remain
untested in educational contexts where students may use varying levels of
permitted AI edits. We present the first adaptation of watermarking-based
detection methods for classroom settings, introducing conformal methods that
effectively control FPR across diverse classroom settings. Using essays from
native and non-native English speakers, we simulate seven levels of AI editing
interventions--from grammar correction to content expansion--across multiple
language models and watermarking schemes, and evaluate our proposal under these
different setups. Our findings provide educators with quantitative frameworks
to enforce academic integrity standards while embracing AI integration in the
classroom.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.23113v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>