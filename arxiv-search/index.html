<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 12/15/2025, 12:30:48 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Forest Kernel Balancing Weights: Outcome-Guided Features for Causal Inference</h3>
                    <p><strong>Authors:</strong> Andy A. Shen, Eli Ben-Michael, Avi Feller, Luke Keele, Jared Murray</p>
                    <p>While balancing covariates between groups is central for observational causal inference, selecting which features to balance remains a challenging problem. Kernel balancing is a promising approach that first estimates a kernel that captures similarity across units and then balances a (possibly low-dimensional) summary of that kernel, indirectly learning important features to balance. In this paper, we propose forest kernel balancing, which leverages the underappreciated fact that tree-based machine learning models, namely random forests and Bayesian additive regression trees (BART), implicitly estimate a kernel based on the co-occurrence of observations in the same terminal leaf node. Thus, even though the resulting kernel is solely a function of baseline features, the selected nonlinearities and other interactions are important for predicting the outcome -- and therefore are important for addressing confounding. Through simulations and applied illustrations, we show that forest kernel balancing leads to meaningful computational and statistical improvement relative to standard kernel methods, which do not incorporate outcome information when learning features.</p>
                    <p><a href="https://arxiv.org/pdf/2512.11751v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Maritime Vessel Tracking</h3>
                    <p><strong>Authors:</strong> John Mahlon Scott, Hsin-Hsiung Huang</p>
                    <p>The Automatic Identification System (AIS) provides time stamped vessel positions and kinematic reports that enable maritime authorities to monitor traffic. We consider the problem of relabeling AIS trajectories when vessel identifiers are missing, focusing on a challenging nationwide setting in which tracks are heavily downsampled and span diverse operating environments across continental U.S. waters. We propose a hybrid pipeline that first applies a physics-based screening step to project active track endpoints forward in time and select a small set of plausible ancestors for each new observation. A supervised neural classifier then chooses among these candidates, or initiates a new track, using engineered space time and kinematic consistency features. On held out data, this approach improves posit accuracy relative to unsupervised baselines, demonstrating that combining simple motion models with learned disambiguation can scale vessel relabeling to heterogeneous, high volume AIS streams.</p>
                    <p><a href="https://arxiv.org/pdf/2512.11707v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Relational Model of Neighborhood Mobility: The Role of Amenities and Cultural Alignment</h3>
                    <p><strong>Authors:</strong> Thiago H Silva, Daniel Silver, Gustavo Santos, Myriam Delgado</p>
                    <p>Why are some neighborhoods strongly connected while others remain isolated? Although standard explanations focus on demographics, economics, and geography, movement across the city may also depend on cultural styles and amenity mix. This study proposes a relational, cross-national model in which local culture and amenity mix alignment creates a "soft infrastructure" of urban mobility, i.e., symbolic cues and functional features that shape expectations about the character of places. Using ~650 million Google Places reviews to measure co-visitation between U.S. ZIP codes and ~30 million Canadian change-of-address to track residential mobility, results show that neighborhoods with similar cultural styles and amenities are significantly more connected. These effects persist even after controlling for race, income, education, politics, housing costs, and distance. Urban cohesion and segregation depend not only on who lives where or how far apart neighborhoods are, but on the shared cultural and material ecologies that structure movement across the city.</p>
                    <p><a href="https://arxiv.org/pdf/2512.11662v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Unified Approach to Portfolio Optimization using the `Gain Probability Density Function' and Applications</h3>
                    <p><strong>Authors:</strong> Jean-Patrick Mascomère, Jérémie Messud, Yagnik Chatterjee, Isabel Barros Garcia</p>
                    <p>This article proposes a unified framework for portfolio optimization (PO), recognizing an object called the `gain probability density function (PDF)' as the fundamental object of the problem from which any objective function could be derived. The gain PDF has the advantage of being 1-dimensional for any given portfolio and thus is easy to visualize and interpret. The framework allows us to naturally incorporate all existing approaches (Markowitz, CVaR-deviation, higher moments...) and represents an interesting basis to develop new approaches. It leads us to propose a method to directly match a target PDF defined by the portfolio manager, giving them maximal control on the PO problem and moving beyond approaches that focus only on expected return and risk. As an example, we develop an application involving a new objective function to control high profits, to be applied after a conventional PO (including expected return and risk criteria) and thus leading to sub-optimality w.r.t. the conventional objective function. We then propose a methodology to quantify a cost associated with this optimality deviation in a common budget unit, providing a meaningful information to portfolio managers. Numerical experiments considering portfolios with energy-producing assets illustrate our approach. The framework is flexible and can be applied to other sectors (financial assets, etc).</p>
                    <p><a href="https://arxiv.org/pdf/2512.11649v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Dynamic Conditional SKEPTIC</h3>
                    <p><strong>Authors:</strong> Gabriele Di Luzio, Giacomo Morelli</p>
                    <p>We introduce the Dynamic Conditional SKEPTIC (DCS), a semiparametric approach for efficiently and robustly estimating time-varying correlations in multivariate models. We exploit nonparametric rank-based statistics, namely Spearman's rho and Kendall's tau, to estimate the unknown correlation matrix and discuss the stationarity, beta- and rho- mixing conditions of the model. We illustrate the methodology by estimating the time-varying conditional correlation matrix of the stocks included in the S&P100 and S&P500 during the period from 02/01/2013 to 23/01/2025. The results show that DCS improves diagnostic checks compared to the classical Dynamic Conditional Correlation (DCC) models, providing uncorrelated and normally distributed residuals. A risk management application shows that global minimum variance portfolios estimated using the DCS model exhibit lower turnover than those based on the DCC and DCC-NL models, while also achieving higher Sharpe ratios for portfolios constructed from S&P 100 constituents.</p>
                    <p><a href="https://arxiv.org/pdf/2512.11648v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Euclidean Ideal Point Estimation From Roll-Call Data via Distance-Based Bipartite Network Models</h3>
                    <p><strong>Authors:</strong> Seungju Lee, In Kyun Kim, Jong Hee Park, Ick Hoon Jin</p>
                    <p>Conventional ideal point models rely on Gaussian or quadratic utility functions that violate the triangle inequality, producing non-metric distances that complicate geometric interpretation and undermine clustering and dispersion-based analyses. We introduce a distance-based alternative that adapts the Latent Space Item Response Model (LSIRM) to roll-call data, treating legislators and bills as nodes in a bipartite network jointly embedded in a Euclidean metric space. Through controlled simulations, Euclidean LSIRM consistently recovers latent coalition structure with superior cluster separation relative to existing methods. Applied to the 118th U.S. House, the model improves vote prediction and yields bill embeddings that clarify cross-cutting issue alignments. The results show that restoring metric structure to ideal point estimation provides a clearer and more coherent inference about party cohesion, factional divisions, and multidimensional legislative behavior.</p>
                    <p><a href="https://arxiv.org/pdf/2512.11610v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis</h3>
                    <p><strong>Authors:</strong> Hyungrok Do, Yuyan Wang, Mengling Liu, Myeonggyun Lee</p>
                    <p>Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\texttt{https://github.com/hyungrok-do/NeuralPLSI}).</p>
                    <p><a href="https://arxiv.org/pdf/2512.11593v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Unified Micro-Model for Loss Reserves, IBNR and Unearned Premium Risk with Dependence, Inflation, and Discounting</h3>
                    <p><strong>Authors:</strong> Emmanuel Hamel, Anas Abdallah, Ghislain Léveillé</p>
                    <p>This paper introduces a unified micro-level stochastic framework for the joint modeling of loss reserves (RBNS), incurred but not reported (IBNR) reserves, and unearned premium risk under dependence, inflation, and discounting. The proposed framework accommodates interactions between indemnities, expenses, reporting delays, and settlement delays, while allowing for flexible parametric dependence structures and dynamic financial adjustments. An Aggregate Trend Renewal Process (ATRP) is used as one possible implementation of the joint model for payments, expenses, and delays; however, the methodological contribution of the paper lies in the unified micro-level reserving architecture rather than in the ATRP itself. The framework produces forward-looking reserve and premium risk measures with direct applications to pricing, reserving, and capital management.
  We implement the framework using an aggregate trend renewal process at the individual claim level, which can be applied to the usual run-off triangle to obtain predictions for each accident-development year. Closed-form expressions for the first two raw and joint conditional moments of predicted payments are derived, together with approximations of their distribution functions. A detailed case study on medical malpractice insurance illustrates the practical relevance of the approach and its calibration on real-world data. We also investigate data heterogeneity, parameter uncertainty, distributional approximations, premium risk, UPR sensitivity to operational delays and inflation, and risk capital implications under alternative assumptions. The results highlight the advantages of unified micro-level modeling for dynamic liability and premium risk assessment in long-tailed lines of business.</p>
                    <p><a href="https://arxiv.org/pdf/2512.11197v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Estimation of Contextual Exposure to HIV from GPS Data</h3>
                    <p><strong>Authors:</strong> Haoyang Wu, Zhaoxing Wu, Thulile Mathenjwa, Elphas Okango, Khai Hoan Tram, Margot Otto, Maxime Inghels, Paul Mee, Diego Cuadros, Hae-Young Kim, Till Barnighausen, Frank Tanser, Adrian Dobra</p>
                    <p>We present a comprehensive statistical methodological framework for estimating contextual exposure to HIV that includes local (grid-cell level) estimation of HIV prevalence and human activity space estimation based on GPS data. The development of our framework was necessary to analyze HIV surveillance and sociodemographic survey data in conjunction with GPS data collected in rural KwaZulu-Natal, South Africa, to study the mobility patterns of young people. Based on mobility and contextual exposure measures, we examine whether the sex and age of study participants systematically influence the extent and structure of their mobility patterns. We discuss techniques for investigating how the study participants' contextual exposure to HIV changes as their activity spaces expand beyond residential locations, as well as methods for identifying study participants who may be at increased risk of acquiring HIV. KEYWORDS: Contextual HIV exposure; GPS-based mobility analysis; Activity space; HIV prevalence mapping</p>
                    <p><a href="https://arxiv.org/pdf/2512.11159v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems</h3>
                    <p><strong>Authors:</strong> Eddie Landesberg</p>
                    <p>LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover.</p>
                    <p><a href="https://arxiv.org/pdf/2512.11150v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>