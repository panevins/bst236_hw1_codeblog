<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 9/29/2025, 1:23:55 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Estimating average treatment effects when treatment data are absent in a
  target study</h3>
                    <p><strong>Authors:</strong> Lan Wen, Aaron L Sarvet</p>
                    <p>  Researchers are frequently interested in understanding the causal effect of
treatment interventions. However, in some cases, the treatment of
interest--readily available in a randomized controlled trial (RCT)--is either
not directly measured or entirely unavailable in observational datasets. This
challenge has motivated the development of stochastic incremental propensity
score interventions which operate on post-treatment exposures affected by the
treatment of interest with the aim of approximating the causal effects of the
treatment intervention. Yet, a key challenge lies in the fact that the precise
distributional shift of these post-treatment exposures induced by the treatment
is typically unknown, making it uncertain whether the approximation truly
reflects the causal effect of interest. The primary objective of this paper is
to explore data integration methodologies to characterize a distribution of
post-treatment exposures resulting from the treatment in an external dataset,
and to use this information to estimate counterfactual mean outcomes under
treatment interventions, in settings where the observational data lack
treatment information and the external data may not contain measurements of the
outcome of interest. We will discuss the underlying assumptions required for
this approach and provide methodological guidance on estimation strategies to
address these challenges.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.22543v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Mental Health Impacts of AI Companions: Triangulating Social Media
  Quasi-Experiments, User Perspectives, and Relational Theory</h3>
                    <p><strong>Authors:</strong> Yunhao Yuan, Jiaxun Zhang, Talayeh Aledavood, Renwen Zhang, Koustuv Saha</p>
                    <p>  AI-powered companion chatbots (AICCs) such as Replika are increasingly
popular, offering empathetic interactions, yet their psychosocial impacts
remain unclear. We examined how engaging with AICCs shaped wellbeing and how
users perceived these experiences. First, we conducted a large-scale
quasi-experimental study of longitudinal Reddit data, applying stratified
propensity score matching and Difference-in-Differences regression. Findings
revealed mixed effects -- greater affective and grief expression, readability,
and interpersonal focus, alongside increases in language about loneliness and
suicidal ideation. Second, we complemented these results with 15
semi-structured interviews, which we thematically analyzed and contextualized
using Knapp's relationship development model. We identified trajectories of
initiation, escalation, and bonding, wherein AICCs provided emotional
validation and social rehearsal but also carried risks of over-reliance and
withdrawal. Triangulating across methods, we offer design implications for AI
companions that scaffold healthy boundaries, support mindful engagement,
support disclosure without dependency, and surface relationship stages --
maximizing psychosocial benefits while mitigating risks.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.22505v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Modelling non-stationary extremal dependence through a geometric
  approach</h3>
                    <p><strong>Authors:</strong> C. J. R. Murphy-Barltrop, J. L. Wadsworth, M. de Carvalho, B. D. Youngman</p>
                    <p>  Non-stationary extremal dependence, whereby the relationship between the
extremes of multiple variables evolves over time, is commonly observed in many
environmental and financial data sets. However, most multivariate extreme value
models are only suited to stationary data. A recent approach to multivariate
extreme value modelling uses a geometric framework, whereby extremal dependence
features are inferred through the limiting shapes of scaled sample clouds. This
framework can capture a wide range of dependence structures, and a variety of
inference procedures have been proposed in the stationary setting. In this
work, we first extend the geometric framework to the non-stationary setting and
outline assumptions to ensure the necessary convergence conditions hold. We
then introduce a flexible, semi-parametric modelling framework for obtaining
estimates of limit sets in the non-stationary setting. Through rigorous
simulation studies, we demonstrate that our proposed framework can capture a
wide range of dependence forms and is robust to different model formulations.
We illustrate the proposed methods on financial returns data and present
several practical uses.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.22501v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Model Training, Data Assimilation, and Forecast Experiments with a
  Hybrid Atmospheric Model that Incorporates Machine Learning</h3>
                    <p><strong>Authors:</strong> Dylan Elliott, Troy Arcomano, Istvan Szunyogh, Brian R. Hunt</p>
                    <p>  The hybrid model combines the physics-based primitive-equations model SPEEDY
with a machine learning-based (ML-based) model component, while ERA5 reanalyses
provide the presumed true states of the atmosphere. Six-hourly simulated noisy
observations are generated for a 30-year ML training period and a one-year
testing period. These observations are assimilated with a Local Ensemble
Transform Kalman Filter (LETKF), and a 10-day deterministic forecast is also
started from each ensemble mean analysis of the testing period. In the first
experiment, the physics-based model provides the background ensemble members
and the 10-day deterministic forecasts. In the other three experiments, the
hybrid model plays the same role as the physics-based model in the first
experiment, but it is trained on a different data set in each experiment. These
training data sets are analyses obtained by using the physics-based model
(second experiment), the hybrid model of the previous experiment (third
experiment), and for comparison, ERA5 reanalyses (fourth experiment). The
results of the experiments show that hybridizing the model can substantially
improve the accuracy of the analyses and forecasts. When the model is trained
on ERA5 reanalyses, the biases of the analyses are negligible and the magnitude
of the flow-dependent part of the analysis errors is greatly reduced. While the
gains in analysis accuracy are distinctly more modest in the other two hybrid
model experiments, the gains in forecast accuracy tend to be larger in those
experiments after 1-3 forecast days. However, these extra gains of forecast
accuracy are achieved, in part, by a modest gradual reduction of the spatial
variability of the forecasts.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.22465v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Chronic Stress, Immune Suppression, and Cancer Occurrence: Unveiling the
  Connection using Survey Data and Predictive Models</h3>
                    <p><strong>Authors:</strong> Teddy Lazebnik, Vered Aharonson</p>
                    <p>  Chronic stress was implicated in cancer occurrence, but a direct causal
connection has not been consistently established. Machine learning and causal
modeling offer opportunities to explore complex causal interactions between
psychological chronic stress and cancer occurrences. We developed predictive
models employing variables from stress indicators, cancer history, and
demographic data from self-reported surveys, unveiling the direct and immune
suppression mitigated connection between chronic stress and cancer occurrence.
The models were corroborated by traditional statistical methods. Our findings
indicated significant causal correlations between stress frequency, stress
level and perceived health impact, and cancer incidence. Although stress alone
showed limited predictive power, integrating socio-demographic and familial
cancer history data significantly enhanced model accuracy. These results
highlight the multidimensional nature of cancer risk, with stress emerging as a
notable factor alongside genetic predisposition. These findings strengthen the
case for addressing chronic stress as a modifiable cancer risk factor,
supporting its integration into personalized prevention strategies and public
health interventions to reduce cancer incidence.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.22275v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>COMPASS: Robust Feature Conformal Prediction for Medical Segmentation
  Metrics</h3>
                    <p><strong>Authors:</strong> Matt Y. Cheung, Ashok Veeraraghavan, Guha Balakrishnan</p>
                    <p>  In clinical applications, the utility of segmentation models is often based
on the accuracy of derived downstream metrics such as organ size, rather than
by the pixel-level accuracy of the segmentation masks themselves. Thus,
uncertainty quantification for such metrics is crucial for decision-making.
Conformal prediction (CP) is a popular framework to derive such principled
uncertainty guarantees, but applying CP naively to the final scalar metric is
inefficient because it treats the complex, non-linear segmentation-to-metric
pipeline as a black box. We introduce COMPASS, a practical framework that
generates efficient, metric-based CP intervals for image segmentation models by
leveraging the inductive biases of their underlying deep neural networks.
COMPASS performs calibration directly in the model's representation space by
perturbing intermediate features along low-dimensional subspaces maximally
sensitive to the target metric. We prove that COMPASS achieves valid marginal
coverage under exchangeability and nestedness assumptions. Empirically, we
demonstrate that COMPASS produces significantly tighter intervals than
traditional CP baselines on four medical image segmentation tasks for area
estimation of skin lesions and anatomical structures. Furthermore, we show that
leveraging learned internal features to estimate importance weights allows
COMPASS to also recover target coverage under covariate shifts. COMPASS paves
the way for practical, metric-based uncertainty quantification for medical
image segmentation.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.22240v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Modeling Psychological Profiles in Volleyball via Mixed-Type Bayesian
  Networks</h3>
                    <p><strong>Authors:</strong> Maria Iannario, Dae-Jin Lee, Manuele Leonelli</p>
                    <p>  Psychological attributes rarely operate in isolation: coaches reason about
networks of related traits. We analyze a new dataset of 164 female volleyball
players from Italy's C and D leagues that combines standardized psychological
profiling with background information. To learn directed relationships among
mixed-type variables (ordinal questionnaire scores, categorical demographics,
continuous indicators), we introduce latent MMHC, a hybrid structure learner
that couples a latent Gaussian copula and a constraint-based skeleton with a
constrained score-based refinement to return a single DAG. We also study a
bootstrap-aggregated variant for stability. In simulations spanning sample
size, sparsity, and dimension, latent Max-Min Hill-Climbing (MMHC) attains
lower structural Hamming distance and higher edge recall than recent
copula-based learners while maintaining high specificity. Applied to
volleyball, the learned network organizes mental skills around goal setting and
self-confidence, with emotional arousal linking motivation and anxiety, and
locates Big-Five traits (notably neuroticism and extraversion) upstream of
skill clusters. Scenario analyses quantify how improvements in specific skills
propagate through the network to shift preparation, confidence, and
self-esteem. The approach provides an interpretable, data-driven framework for
profiling psychological traits in sport and for decision support in athlete
development.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.22111v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Personalized Oncology: Feasibility of Evaluating Treatment Effects for
  Individual Patients</h3>
                    <p><strong>Authors:</strong> Lydia Jang, Stefan Konigorski</p>
                    <p>  The effectiveness of personalized oncology treatments ultimately depends on
whether outcomes can be causally attributed to the treatment. Advances in
precision oncology have improved molecular profiling of individuals, and
tailored therapies have led to more effective treatments for select patient
groups. However, treatment responses still vary among individuals. As cancer is
a heterogeneous and dynamic disease with varying treatment outcomes across
different molecular types and resistance mechanisms, it requires customized
approaches to identify cause-and-effect relationships. N-of-1 trials, or
single-subject clinical trials, are designed to evaluate individual treatment
effects. Several works have described different causal frameworks to identify
treatment effects in N-of-1 trials, yet whether these approaches can be
extended to single-cancer patient settings remains unclear. To explore this
possibility, a longitudinal dataset from a single metastatic cancer patient
with adaptively chosen treatments was considered. The dataset consisted of a
detailed treatment plan as well as biomarker and lesion measurements recorded
over time. After data processing, a treatment period with sufficient data
points to conduct causal inference was selected. Under this setting, a causal
framework was applied to define an estimand, identify causal relationships and
assumptions, and calculate an individual-specific treatment effect using a
time-varying g-formula. Through this application, we illustrate explicitly when
and how causal treatment effects can be estimated in single-patient oncology
settings. Our findings not only demonstrate the feasibility of applying causal
methods in a single-cancer patient setting but also offer a blueprint for using
causal methods across a broader spectrum of cancer types in individualized
settings.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.22089v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Quantifying Fire Risk Index in Chemical Industry Using Statistical
  Modeling Procedure</h3>
                    <p><strong>Authors:</strong> Hyewon Jung, Seungil Ahn, Seungho Choi, Yeseul Jeon</p>
                    <p>  Fire incident reports contain detailed textual narratives that capture causal
factors often overlooked in structured records, while financial damage amounts
provide measurable outcomes of these events. Integrating these two sources of
information is essential for uncovering interpretable links between descriptive
causes and their economic consequences. To this end, we develop a data-driven
framework that constructs a composite Risk Index, enabling systematic
quantification of how specific keywords relate to property damage amounts. This
index facilitates both the identification of high-impact terms and the
aggregation of risks across semantically related clusters, thereby offering a
principled measure of fire-related financial risk. Using more than a decade of
Korean fire investigation reports on the chemical industry classified as
Special Buildings (2013 through 2024), we employ topic modeling and
network-based embedding to estimate semantic similarities from interactions
among words and subsequently apply Lasso regression to quantify their
associations with property damage amounts, thereby estimate fire risk index.
This approach enables us to assess fire risk not only at the level of
individual terms but also within their broader textual context, where highly
interactive related words provide insights into collective patterns of hazard
representation and their potential impact on expected losses. The analysis
highlights several domains of risk, including hazardous chemical leakage,
unsafe storage practices, equipment and facility malfunctions, and
environmentally induced ignition. The results demonstrate that text-derived
indices provide interpretable and practically relevant insights, bridging
unstructured narratives with structured loss information and offering a basis
for evidence-based fire risk assessment and management.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.21736v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Not All Accuracy Is Equal: Prioritizing Diversity in Infectious Disease
  Forecasting</h3>
                    <p><strong>Authors:</strong> Carson Dudley, Marisa Eisenberg</p>
                    <p>  Ensemble forecasts have become a cornerstone of large-scale disease response,
underpinning decision making at agencies such as the US Centers for Disease
Control and Prevention (CDC). Their growing use reflects the goal of combining
multiple models to improve accuracy and stability versus using a single model.
However, recent experience shows these benefits are not guaranteed. During the
COVID-19 pandemic, the CDC's multi-model forecasting ensemble outperformed the
best single model by only 1%, and CDC flu forecasting ensembles have often
ranked below multiple individual models.
  This raises a key question: why are ensembles underperforming? We posit that
a central reason is that both model developers and ensemble builders typically
focus on stand-alone accuracy. Models are fit to minimize their own forecasting
error, and ensembles are often weighted according to those same scores.
However, most epidemic forecasts are built from a small set of approaches and
trained on the same surveillance data, leading to highly correlated errors.
This redundancy limits the benefit of ensembling and may explain why large
ensembles sometimes deliver only marginal gains.
  To realize the potential of ensembles, both modelers and ensemblers should
prioritize models that contribute complementary information rather than
replicating existing approaches. Ensembles built with this principle in mind
move beyond size for its own sake toward true diversity, producing forecasts
that are more robust and more valuable for epidemic preparedness and response.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.21191v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>