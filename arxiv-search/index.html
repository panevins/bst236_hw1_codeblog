<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 2/25/2025, 8:53:52 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Sustainable Greenhouse Management: A Comparative Analysis of Recurrent
  and Graph Neural Networks</h3>
                    <p><strong>Authors:</strong> Emiliano Seri, Marcello Petitta, Cristina Cornaro</p>
                    <p>  The integration of photovoltaic (PV) systems into greenhouses not only
optimizes land use but also enhances sustainable agricultural practices by
enabling dual benefits of food production and renewable energy generation.
However, accurate prediction of internal environmental conditions is crucial to
ensure optimal crop growth while maximizing energy production. This study
introduces a novel application of Spatio-Temporal Graph Neural Networks
(STGNNs) to greenhouse microclimate modeling, comparing their performance with
traditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal
pattern recognition, they cannot explicitly model the directional relationships
between environmental variables. Our STGNN approach addresses this limitation
by representing these relationships as directed graphs, enabling the model to
capture both spatial dependencies and their directionality. Using
high-frequency data collected at 15-minute intervals from a greenhouse in
Volos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter
conditions (R^2 = 0.985) but show limitations during summer cooling system
operation. Though STGNNs currently show lower performance (winter R^2 = 0.947),
their architecture offers greater potential for integrating additional
variables such as PV generation and crop growth indicators.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.17371v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A tutorial on optimal dynamic treatment regimes</h3>
                    <p><strong>Authors:</strong> Chunyu Wang, Brian DM Tom</p>
                    <p>  A dynamic treatment regime is a sequence of treatment decision rules tailored
to an individual's evolving status over time. In precision medicine, much focus
has been placed on finding an optimal dynamic treatment regime which, if
followed by everyone in the population, would yield the best outcome on
average; and extensive investigation has been conducted from both
methodological and applications standpoints. The aim of this tutorial is to
provide readers who are interested in optimal dynamic treatment regimes with a
systematic, detailed but accessible introduction, including the formal
definition and formulation of this topic within the framework of causal
inference, identification assumptions required to link the causal quantity of
interest to the observed data, existing statistical models and estimation
methods to learn the optimal regime from data, and application of these methods
to both simulated and real data.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.16988v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Adjustment for Inconsistency in Adaptive Phase 2/3 Designs with Dose
  Optimization</h3>
                    <p><strong>Authors:</strong> Cong Chen, Mo Huang</p>
                    <p>  Adaptive Phase 2/3 designs hold great promise in contemporary oncology drug
development, especially when limited data from Phase 1 dose-finding is
insufficient for identifying an optimal dose. However, there is a general
concern about inconsistent results before and after the adaptation. The
imperfection in dose selection further complicates the issue. In this paper, we
explicitly incorporate the concerns about inconsistency into the statistical
analysis under three hypothesis testing strategies. This investigation paves
the way for further research in a less explored area.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.16591v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Beyond Words: How Large Language Models Perform in Quantitative
  Management Problem-Solving</h3>
                    <p><strong>Authors:</strong> Jonathan Kuzmanko</p>
                    <p>  This study examines how Large Language Models (LLMs) perform when tackling
quantitative management decision problems in a zero-shot setting. Drawing on
900 responses generated by five leading models across 20 diverse managerial
scenarios, our analysis explores whether these base models can deliver accurate
numerical decisions under varying presentation formats, scenario complexities,
and repeated attempts. Contrary to prior findings, we observed no significant
effects of text presentation format (direct, narrative, or tabular) or text
length on accuracy. However, scenario complexity -- particularly in terms of
constraints and irrelevant parameters -- strongly influenced performance, often
degrading accuracy. Surprisingly, the models handled tasks requiring multiple
solution steps more effectively than expected. Notably, only 28.8\% of
responses were exactly correct, highlighting limitations in precision. We
further found no significant ``learning effect'' across iterations: performance
remained stable across repeated queries. Nonetheless, significant variations
emerged among the five tested LLMs, with some showing superior binary accuracy.
Overall, these findings underscore both the promise and the pitfalls of
harnessing LLMs for complex quantitative decision-making, informing managers
and researchers about optimal deployment strategies.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.16556v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Subspace Recovery in Winsorized PCA: Insights into Accuracy and
  Robustness</h3>
                    <p><strong>Authors:</strong> Sangil Han, Kyoowon Kim, Sungkyu Jung</p>
                    <p>  In this paper, we explore the theoretical properties of subspace recovery
using Winsorized Principal Component Analysis (WPCA), utilizing a common data
transformation technique that caps extreme values to mitigate the impact of
outliers. Despite the widespread use of winsorization in various tasks of
multivariate analysis, its theoretical properties, particularly for subspace
recovery, have received limited attention. We provide a detailed analysis of
the accuracy of WPCA, showing that increasing the number of samples while
decreasing the proportion of outliers guarantees the consistency of the sample
subspaces from WPCA with respect to the true population subspace. Furthermore,
we establish perturbation bounds that ensure the WPCA subspace obtained from
contaminated data remains close to the subspace recovered from pure data.
Additionally, we extend the classical notion of breakdown points to
subspace-valued statistics and derive lower bounds for the breakdown points of
WPCA. Our analysis demonstrates that WPCA exhibits strong robustness to
outliers while maintaining consistency under mild assumptions. A toy example is
provided to numerically illustrate the behavior of the upper bounds for
perturbation bounds and breakdown points, emphasizing winsorization's utility
in subspace recovery.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.16391v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Including an infrequently measured time-dependent error-prone covariate
  in survival analyses: a simulation-based comparison of methods</h3>
                    <p><strong>Authors:</strong> Viviane Philipps, Laurence Freedman, Veronika Deffner, Catherine Helmer, Hendriek Boshuizen, Anne C. M. Thiébaut, Cécile Proust-Lima</p>
                    <p>  Epidemiologic studies often evaluate the association between an exposure and
an event risk. When time-varying, exposure updates usually occur at discrete
visits although changes are in continuous time and survival models require
values to be constantly known. Moreover, exposures are likely measured with
error, and their observation truncated at the event time. We aimed to quantify
in a Cox regression the bias in the association resulting from intermittent
measurements of an error-prone exposure. Using simulations under various
scenarios, we compared five methods: last observation carried-forward (LOCF),
classical two-stage regression-calibration using measurements up to the event
(RC) or also after (PE-RC), multiple imputation (MI) and joint modeling of the
exposure and the event (JM). The LOCF, and to a lesser extent the classical RC,
showed substantial bias in almost all 43 scenarios. The RC bias was avoided
when considering post-event information. The MI performed relatively well, as
did the JM. Illustrations exploring the association of Body Mass Index and
Executive Functioning with dementia risk showed consistent conclusions.
Accounting for measurement error and discrete updates is critical when studying
time-varying exposures. MI and JM techniques may be applied in this context,
while classical RC should be avoided due to the informative truncation.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.16362v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A multilevel model with heterogeneous variances for snap timing in the
  National Football League</h3>
                    <p><strong>Authors:</strong> Quang Nguyen, Ronald Yurko</p>
                    <p>  Player tracking data have provided great opportunities to generate novel
insights into understudied areas of American football, such as pre-snap motion.
Using a Bayesian multilevel model with heterogeneous variances, we provide an
assessment of NFL quarterbacks and their ability to synchronize the timing of
the ball snap with pre-snap movement from their teammates. We focus on passing
plays with receivers in motion at the snap and running a route, and define the
snap timing as the time between the moment a receiver begins motioning and the
ball snap event. We assume a Gamma distribution for the play-level snap timing
and model the mean parameter with player and team random effects, along with
relevant fixed effects such as the motion type identified via a Gaussian
mixture model. Most importantly, we model the shape parameter with quarterback
random effects, which enables us to estimate the differences in snap timing
variability among NFL quarterbacks. We demonstrate that higher variability in
snap timing is beneficial for the passing game, as it relates to facing less
havoc created by the opposing defense. We also obtain a quarterback leaderboard
based on our snap timing variability measure, and Patrick Mahomes stands out as
the top player.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.16313v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Bayesian mixed-effects model to evaluate the determinants of COVID-19
  vaccine uptake in the US</h3>
                    <p><strong>Authors:</strong> Asim K. Dey</p>
                    <p>  The COVID-19 pandemic has adversely affected US public health, resulting in
over a hundred million cases and more than one million deaths. Vaccination is
the key intervention against the COVID-19 pandemic. Multiple COVID-19 vaccines
are now available for human use. However, a number of factors, including
socio-demographic variables, impact the uptake of COVID-19 vaccines. In this
study, we apply a Bayesian mixed-effects model to assess different
socio-demographic and spatial factors that influence the acceptance of COVID-19
vaccines in the US. The fitted mixed-effects model provides the probabilistic
inference about the vaccine acceptance determinants with uncertainty
quantification.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.16130v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Local False Sign Rate and the Role of Prior Covariance Rank in
  Multivariate Empirical Bayes Multiple Testing</h3>
                    <p><strong>Authors:</strong> Dongyue Xie</p>
                    <p>  We study the relationship between the rank of the prior covariance matrix and
the local false sign rate in a multivariate empirical Bayes normal mean model.
It has been observed that the false sign rate is inflated when the prior
assigns weight to low-rank covariance matrices. We show that this issue arises
due to the rank deficiency of prior covariance matrices and propose an
adjustment to mitigate it.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.16118v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Breaking the Balance: Asymmetric Negative Voting in the 2020
  Presidential Election</h3>
                    <p><strong>Authors:</strong> Bang Quan Zheng</p>
                    <p>  While voters from opposing parties have traditionally exhibited symmetric
levels of hostility toward out-party candidates, our analysis of the 2016 and
2020 Nationscape data reveals a notable departure from this pattern. In 2016,
negative voting was relatively balanced, with similar levels of hostility
directed at Hillary Clinton and Donald Trump. However, by 2020, asymmetric
negative voting had emerged. As an incumbent seeking re-election amid a rapidly
declining economy, the COVID-19 pandemic, and widespread uncertainty, Trump
faced heightened negative perceptions fueled by dissatisfaction with his
handling of the economy, race relations, the pandemic, and his leadership
style. These factors galvanized younger, educated Democrats and Independents to
vote against him in unprecedented numbers. In contrast, Republicans expressed
less animosity toward Biden in 2020 than they had toward Clinton in 2016. This
shift disrupted the balance in the typical pattern of symmetric negative
voting.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.16081v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>