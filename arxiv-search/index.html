<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 12/8/2025, 12:28:10 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Developing synthetic microdata through machine learning for firm-level business surveys</h3>
                    <p><strong>Authors:</strong> Jorge Cisneros Paz, Timothy Wojan, Matthew Williams, Jennifer Ozawa, Robert Chew, Kimberly Janda, Timothy Navarro, Michael Floyd, Christine Task, Damon Streat</p>
                    <p>Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.</p>
                    <p><a href="https://arxiv.org/pdf/2512.05948v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Multi-state Modeling of Delay Evolution in Suburban Rail Transports</h3>
                    <p><strong>Authors:</strong> Stefania Colombo, Alfredo Gimenez Zapiola, Francesca Ieva, Simone Vantini</p>
                    <p>Train delays are a persistent issue in railway systems, particularly in suburban networks where operational complexity is heightened by frequent services and high passenger volumes. Traditional delay models often overlook the temporal and structural dynamics of real delay propagation.
  This work applies continuous-time multi-state models to analyze the temporal evolution of delay on the S5 suburban line in Lombardy, Italy. Using detailed operational, meteorological, and contextual data, the study models delay transitions while accounting for observable heterogeneity.
  The findings reveal how delay dynamics vary by travel direction, time slot, and route segment. Covariates such as station saturation and passenger load are shown to significantly affect the risk of delay escalation or recovery. The study offers both methodological advancements and practical results for improving the reliability of rail services.</p>
                    <p><a href="https://arxiv.org/pdf/2512.05521v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Consistency of Familial DNA Search Results in Southeast Asian Populations</h3>
                    <p><strong>Authors:</strong> Monchai Kooakachai, Tiwakorn Chapalee, Chairat Thitiyan, Patsaya Jumnongwut</p>
                    <p>DNA databases are widely used in forensic science to identify unknown offenders. When no exact match is found, familial DNA searches can help by identifying first-degree relatives using likelihood ratios. If multiple subpopulations are relevant, likelihood ratios can be computed separately based on allele frequency estimates. Various strategies exist to combine these ratios, such as averaging allele frequencies or taking the average, maximum, or minimum likelihood ratio. While some comparisons have been made in populations like those in the U.S., their effectiveness in other regions remains unclear. This study evaluates likelihood ratio-based strategies in Southeast Asian populations, specifically Thailand, Malaysia, and Singapore. Our findings align with previous research, showing that statistical power varies across strategies. Among Thai subpopulations, the minimum likelihood ratio strategy is preferred, as it maintains high power while minimizing differences between subpopulations.</p>
                    <p><a href="https://arxiv.org/pdf/2512.05490v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Functional Approach to Testing Overall Effect of Interaction Between DNA Methylation and SNPs</h3>
                    <p><strong>Authors:</strong> Yvelin Gansou, Karim Oualkacha, Marzia Angela Cremona, Lajmi Lakhal-Chaieb</p>
                    <p>We introduce a test for the overall effect of interaction between DNA methylation and a set of single nucleotide polymorphisms (SNPs) on a quantitative phenotype. The developed inference procedure is based on a functional approach that extends existing regression models in functional data analysis. Through extensive simulations, we show that the proposed test effectively controls type I error rates and highlights increased empirical power over existing methods, particularly when multiple interactions are present. The use of the proposed test is illustrated with an application to data from obesity patients and controls.</p>
                    <p><a href="https://arxiv.org/pdf/2512.05276v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Exchangeable Gaussian Processes with application to epidemics</h3>
                    <p><strong>Authors:</strong> Lampros Bouranis, Petros Barmpounakis, Nikolaos Demiris, Konstantinos Kalogeropoulos</p>
                    <p>We develop a Bayesian non-parametric framework based on multi-task Gaussian processes, appropriate for temporal shrinkage. We focus on a particular class of dynamic hierarchical models to obtain evidence-based knowledge of infectious disease burden. These models induce a parsimonious way to capture cross-dependence between groups while retaining a natural interpretation based on an underlying mean process, itself expressed as a Gaussian process. We analyse distinct types of outbreak data from recent epidemics and find that the proposed models result in improved predictive ability against competing alternatives.</p>
                    <p><a href="https://arxiv.org/pdf/2512.05227v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Clustering country-level all-cause mortality data: a review</h3>
                    <p><strong>Authors:</strong> Pedro Menezes de Araujo, Isobel Claire Gormley, Thomas Brendan Murphy</p>
                    <p>Mortality data are relevant to demography, public health, and actuarial science. Whilst clustering is increasingly used to explore patterns in such data, no study has reviewed its application to country-level all-cause mortality. This review therefore summarises recent work and addresses key questions: why clustering is used, which mortality data are analysed, which methods are most common, and what main findings emerge. To address these questions, we examine studies applying clustering to country-level all-cause mortality, focusing on mortality indices, data sources, and methodological choices, and we replicate some approaches using Human Mortality Database (HMD) data. Our analysis reveals that clustering is mainly motivated by forecasting and by studying convergence and inequality. Most studies use HMD data from developed countries and rely on k-means, hierarchical, or functional clustering. Main findings include a persistent East-West European division across applications, with clustering generally improving forecast accuracy over single-country models. Overall, this review highlights the methodological range in the literature, summarises clustering results, and identifies gaps, such as the limited evaluation of clustering quality and the underuse of data from countries outside the high-income world.</p>
                    <p><a href="https://arxiv.org/pdf/2512.04831v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Bayesian Graphical High-Dimensional Time Series Models for Detecting Structural Changes</h3>
                    <p><strong>Authors:</strong> Shuvrarghya Ghosh, Arkaprava Roy, Anindya Roy, Subhashis Ghosal</p>
                    <p>We study the structural changes in multivariate time-series by estimating and comparing stationary graphs for macroeconomic time series before and after an economic crisis such as the Great Recession. Building on a latent time series framework called Orthogonally-rotated Univariate Time-series (OUT), we propose a shared-parameter framework-the spOUT autoregressive model (spOUTAR)-that jointly models two related multivariate time series and enables coherent Bayesian estimation of their corresponding stationary precision matrices. This framework provides a principled mechanism to detect and quantify which conditional relationships among the variables changed, or formed following the crisis. Specifically, we study the impact of the Great Recession (December 2007-June 2009) that substantially disrupted global and national economies, prompting long-lasting shifts in macroeconomic indicators and their interrelationships. While many studies document its economic consequences, far less is known about how the underlying conditional dependency structure among economic variables changed as economies moved from pre-crisis stability through the shock and back to normalcy. Using the proposed approach to analyze U.S. and OECD macroeconomic data, we demonstrate that spOUTAR effectively captures recession-induced changes in stationary graphical structure, offering a flexible and interpretable tool for studying structural shifts in economic systems.</p>
                    <p><a href="https://arxiv.org/pdf/2512.04444v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Sequential Randomization Tests Using E-values: A Betting Approach for Clinical Trials</h3>
                    <p><strong>Authors:</strong> Fernando G Zampieri</p>
                    <p>Sequential monitoring of randomized trials traditionally relies on parametric assumptions or asymptotic approximations. We present a nonparametric sequential test, the randomization e-process (e-RT), that derives validity solely from the randomization mechanism. Using a betting framework, e-RT constructs a test martingale by sequentially wagering on treatment assignments given observed outcomes. Under the null hypothesis of no treatment effect, the expected wealth cannot grow, guaranteeing anytime-valid Type I error control regardless of stopping rule. We prove validity and present simulation studies demonstrating calibration and power. The e-RT provides a conservative, assumption-free complement to model-based sequential analyses.</p>
                    <p><a href="https://arxiv.org/pdf/2512.04366v2" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A comparison between initialization strategies for the infinite hidden Markov model</h3>
                    <p><strong>Authors:</strong> Federico P. Cortese, Luca Rossini</p>
                    <p>Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature.</p>
                    <p><a href="https://arxiv.org/pdf/2512.03777v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Learning from crises: A new class of time-varying parameter VARs with observable adaptation</h3>
                    <p><strong>Authors:</strong> Nicolas Hardy, Dimitris Korobilis</p>
                    <p>We revisit macroeconomic time-varying parameter vector autoregressions (TVP-VARs), whose persistent coefficients may adapt too slowly to large, abrupt shifts such as those during major crises. We explore the performance of an adaptively-varying parameter (AVP) VAR that incorporates deterministic adjustments driven by observable exogenous variables, replacing latent state innovations with linear combinations of macroeconomic and financial indicators. This reformulation collapses the state equation into the measurement equation, enabling simple linear estimation of the model. Simulations show that adaptive parameters are substantially more parsimonious than conventional TVPs, effectively disciplining parameter dynamics without sacrificing flexibility. Using macroeconomic datasets for both the U.S. and the euro area, we demonstrate that AVP-VAR consistently improves out-of-sample forecasts, especially during periods of heightened volatility.</p>
                    <p><a href="https://arxiv.org/pdf/2512.03763v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>