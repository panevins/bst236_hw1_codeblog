<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 8/18/2025, 1:28:26 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Contrastive CUR: Interpretable Joint Feature and Sample Selection for
  Case-Control Studies</h3>
                    <p><strong>Authors:</strong> Eric Zhang, Michael Love, Didong Li</p>
                    <p>  Dimension reduction is an essential tool for analyzing high dimensional data.
Most existing methods, including principal component analysis (PCA), as well as
their extensions, provide principal components that are often linear
combinations of features, which are often challenging to interpret. CUR
decomposition, another matrix decomposition technique, is a more interpretable
and efficient alternative, offers simultaneous feature and sample selection.
Despite this, many biomedical studies involve two groups: a foreground
(treatment or case) group and a background (control) group, where the objective
is to identify features unique to or enriched in the foreground. This need for
contrastive dimension reduction is not well addressed by existing CUR methods,
nor by contrastive approaches rooted in PCAs. Furthermore, they fail to address
a key challenge in biomedical studies: the need for selecting samples unique to
the foreground. In this paper, we address this gap by proposing a Contrastive
CUR (CCUR), a novel method specifically designed for case-control studies.
Through extensive experiments, we demonstrate that CCUR outperforms existing
techniques in isolating biologically relevant features as well as identifying
sample-specific responses unique to the foreground, offering deeper insights
into case-control biomedical data.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.11557v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Estimating Covariate Effects on Functional Connectivity using
  Voxel-Level fMRI Data</h3>
                    <p><strong>Authors:</strong> Wei Zhao, Brian J. Reich, Emily C. Hector</p>
                    <p>  Functional connectivity (FC) analysis of resting-state fMRI data provides a
framework for characterizing brain networks and their association with
participant-level covariates. Due to the high dimensionality of neuroimaging
data, standard approaches often average signals within regions of interest
(ROIs), which ignores the underlying spatiotemporal dependence among voxels and
can lead to biased or inefficient inference. We propose to use a summary
statistic -- the empirical voxel-wise correlations between ROIs -- and,
crucially, model the complex covariance structure among these correlations
through a new positive definite covariance function. Building on this
foundation, we develop a computationally efficient two-step estimation
procedure that enables statistical inference on covariate effects on
region-level connectivity. Simulation studies show calibrated uncertainty
quantification, and substantial gains in validity of the statistical inference
over the standard averaging method. With data from the Autism Brain Imaging
Data Exchange, we show that autism spectrum disorder is associated with altered
FC between attention-related ROIs after adjusting for age and gender. The
proposed framework offers an interpretable and statistically rigorous approach
to estimation of covariate effects on FC suitable for large-scale neuroimaging
studies.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.11213v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Functional Analysis of Variance for Association Studies</h3>
                    <p><strong>Authors:</strong> Olga A. Vsevolozhskaya, Dmitri V. Zaykin, Mark C. Greenwood, Changshuai Wei, Qing Lu</p>
                    <p>  While progress has been made in identifying common genetic variants
associated with human diseases, for most of common complex diseases, the
identified genetic variants only account for a small proportion of
heritability. Challenges remain in finding additional unknown genetic variants
predisposing to complex diseases. With the advance in next-generation
sequencing technologies, sequencing studies have become commonplace in genetic
research. The ongoing exome-sequencing and whole-genome-sequencing studies
generate a massive amount of sequencing variants and allow researchers to
comprehensively investigate their role in human diseases. The discovery of new
disease-associated variants can be enhanced by utilizing powerful and
computationally efficient statistical methods. In this paper, we propose a
functional analysis of variance (FANOVA) method for testing an association of
sequence variants in a genomic region with a qualitative trait. The FANOVA has
a number of advantages: (1) it tests for a joint effect of gene variants,
including both common and rare; (2) it fully utilizes linkage disequilibrium
and genetic position information; and (3) allows for either protective or
risk-increasing causal variants. Through simulations, we show that FANOVA
outperform two popularly used methods - SKAT and a previously proposed method
based on functional linear models (FLM), - especially if a sample size of a
study is small and/or sequence variants have low to moderate effects. We
conduct an empirical study by applying three methods (FANOVA, SKAT and FLM) to
sequencing data from Dallas Heart Study. While SKAT and FLM respectively
detected ANGPTL 4 and ANGPTL 3 associated with obesity, FANOVA was able to
identify both genes associated with obesity.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.11069v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Sample efficient likelihood-free inference for virus dynamics with
  different types of experiments</h3>
                    <p><strong>Authors:</strong> Yingying Xu, Ulpu Remes, Enrico Rinaldi, Henri Pesonen</p>
                    <p>  This study applied Bayesian optimization likelihood-free inference(BOLFI) to
virus dynamics experimental data and efficiently inferred the model parameters
with uncertainty measure. The computational benefit is remarkable compared to
existing methodology on the same problem. No likelihood knowledge is needed in
the inference. Improvement of the BOLFI algorithm with Gaussian process based
classifier for treatment of extreme values are provided. Discrepancy design for
combining different forms of data from completely different experiment
processes are suggested and tested with synthetic data, then applied to real
data. Reasonable parameter values are estimated for influenza A virus data.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.11042v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Dissecting Microbial Community Structure and Heterogeneity via
  Multivariate Covariate-Adjusted Clustering</h3>
                    <p><strong>Authors:</strong> Zhongmao Liu, Xiaohui Yin, Yanjiao Zhou, Gen Li, Kun Chen</p>
                    <p>  In microbiome studies, it is often of great interest to identify clusters or
partitions of microbiome profiles within a study population and to characterize
the distinctive attributes of each resulting microbial community. While raw
counts or relative compositions are commonly used for such analysis, variations
between clusters may be driven or distorted by subject-level covariates,
reflecting underlying biological and clinical heterogeneity across individuals.
Simultaneously detecting latent communities and identifying covariates that
differentiate them can enhance our understanding of the microbiome and its
association with health outcomes. To this end, we propose a
Dirichlet-multinomial mixture regression (DMMR) model that enables joint
clustering of microbiome profiles while accounting for covariates with either
homogeneous or heterogeneous effects across clusters. A novel symmetric link
function is introduced to facilitate covariate modeling through the
compositional parameters. We develop efficient algorithms with convergence
guarantees for parameter estimation and establish theoretical properties of the
proposed estimators. Extensive simulation studies demonstrate the effectiveness
of the method in clustering, feature selection, and heterogeneity detection. We
illustrate the utility of DMMR through a comprehensive application to
upper-airway microbiota data from a pediatric asthma study, uncovering distinct
microbial subtypes and their associations with clinical characteristics.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.11036v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Risk-Based Prognostics and Health Management</h3>
                    <p><strong>Authors:</strong> John W. Sheppard</p>
                    <p>  It is often the case that risk assessment and prognostics are viewed as
related but separate tasks. This chapter describes a risk-based approach to
prognostics that seeks to provide a tighter coupling between risk assessment
and fault prediction. We show how this can be achieved using the
continuous-time Bayesian network as the underlying modeling framework.
Furthermore, we provide an overview of the techniques that are available to
derive these models from data and show how they might be used in practice to
achieve tasks like decision support and performance-based logistics. This work
is intended to provide an overview of the recent developments related to
risk-based prognostics, and we hope that it will serve as a tutorial of sorts
that will assist others in adopting these techniques.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.11031v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Does fertility affect woman's labor force participation in low- and
  middle-income settings? Findings from a Bayesian nonparametric analysis</h3>
                    <p><strong>Authors:</strong> Lucas Godoy Garraza, Leontine Alkema</p>
                    <p>  Estimating the causal effect of fertility on women's employment is
challenging because fertility and labor decisions are jointly determined. The
difficulty is amplified in low- and middle-income countries, where longitudinal
data are scarce. In this study, we propose a novel approach to estimating the
causal effect of fertility on employment using widely available Demographic and
Health Survey (DHS) observational data. Using infecundity as an instrument for
family size, our approach combines principal stratification with Bayesian
Additive Regression Trees to flexibly account for covariate-dependent
instrument validity, work with count-valued intermediate variables, and produce
estimates of causal effects and effect heterogeneity, i.e., how effects vary
with covariates in the survey population. We apply the approach to DHS data
from Nigeria, Senegal, and Kenya. We find in the survey sample and general
population that an additional child significantly reduces employment among
women in Nigeria but has no clear average effect in Senegal or Kenya. Across
all three countries, however, there is strong evidence of effect heterogeneity:
younger, less-educated women experience large employment penalties, while older
or more advantaged women are largely unaffected. Robustness checks confirm that
these findings are not sensitive to key modeling assumptions. While limitations
remain due to the cross-sectional nature of the DHS data, our results
illustrate how flexible non-parametric models can uncover important effect
variation.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.10787v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Dynamic Skewness in Stochastic Volatility Models: A Penalized Prior
  Approach</h3>
                    <p><strong>Authors:</strong> Bruno E. Holtz, Ricardo S. Ehlers, Adriano K. Suzuki, Francisco Louzada</p>
                    <p>  Financial time series often exhibit skewness and heavy tails, making it
essential to use models that incorporate these characteristics to ensure
greater reliability in the results. Furthermore, allowing temporal variation in
the skewness parameter can bring significant gains in the analysis of this type
of series. However, for more robustness, it is crucial to develop models that
balance flexibility and parsimony. In this paper, we propose dynamic skewness
stochastic volatility models in the SMSN family (DynSSV-SMSN), using priors
that penalize model complexity. Parameter estimation was carried out using the
Hamiltonian Monte Carlo (HMC) method via the \texttt{RStan} package. Simulation
results demonstrated that penalizing priors present superior performance in
several scenarios compared to the classical choices. In the empirical
application to returns of cryptocurrencies, models with heavy tails and dynamic
skewness provided a better fit to the data according to the DIC, WAIC, and
LOO-CV information criteria.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.10778v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Matters Arising: Spatial correlation in economic analysis of climate
  change</h3>
                    <p><strong>Authors:</strong> Christof Sch√∂tz</p>
                    <p>  Climate change poses substantial risks to the global economy. Kotz, Levermann
and Wenz (Nature, 2024) statistically analyzed economic and climate data,
finding significant projected damages until mid-century and a divergence in
outcomes between high- and low-emission scenarios thereafter. We find that
their analysis underestimates uncertainty owing to large, unaccounted-for
spatial correlations on the subnational level, rendering their results
statistically insignificant when properly corrected. Thus, their study does not
provide the robust empirical evidence needed to inform climate policy.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.10575v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Maximum Entropy Models for Unimodal Time Series: Case Studies of
  Universe 25 and St. Matthew Island</h3>
                    <p><strong>Authors:</strong> Sabin Roman</p>
                    <p>  We present a maximum entropy modeling framework for unimodal time series:
signals that begin at a reference level, rise to a single peak, and return.
Such patterns are commonly observed in ecological collapse, population
dynamics, and resource depletion. Traditional dynamical models are often
inapplicable in these settings due to limited or sparse data, frequently
consisting of only a single historical trajectory. In addition, standard
fitting approaches can introduce structural bias, particularly near the mode,
where most interpretive focus lies. Using the maximum entropy principle, we
derive a least-biased functional form constrained only by minimal prior
knowledge, such as the starting point and estimated end. This leads to
analytically tractable and interpretable models.
  We apply this method to the collapse of the Universe 25 mouse population and
the reindeer crash on St. Matthew Island. These case studies demonstrate the
robustness and flexibility of the approach in fitting diverse unimodal time
series with minimal assumptions. We also conduct a cross-comparison against
established models, including the Richards, Skewnormal, and Generalized Gamma
functions. While models typically fit their own generated data best, the
maximum entropy models consistently achieve the lowest off-diagonal
root-mean-square losses, indicating superior generalization. These results
suggest that maximum entropy methods provide a unifying and efficient
alternative to mechanistic models when data is limited and generalization is
essential.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.10518v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>