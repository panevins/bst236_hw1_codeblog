<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 10/27/2025, 1:25:43 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Representing caregiver burden in observational studies: Development of
  the Caregiver Burden Index (CareBI) using NSOC</h3>
                    <p><strong>Authors:</strong> Forough Mahpouya, Sabrina Casucci, Suzanne Sullivan, Christopher Barrick</p>
                    <p>  Informal caregiving often carries a significant emotional, physical, and
financial toll, yet caregiver burden is often underrepresented in healthcare
research and methods. Existing caregiver burden instruments, while valuable in
clinical research, often lack compatibility with observational datasets
regularly used in health services research and planning. This study introduces
the Caregiver Burden Index (CareBI) developed for the National Study of
Caregiving (NSOC), that can be used to represent caregiver burden in
quantitative models and observational research studies. CareBI was developed
and validated using a multistep process that included the identification and
preparation of individual NSOC survey items, exploratory and confirmatory
factor analysis, score estimation, interpretation, and external validation. The
study used data from round 12 of the NSOC. CareBI represents three domains of
burden: objective, subjective, and interpersonal, providing a comprehensive
view of both the positive and negative aspects of caregiving. It also aligns
with the Zarit Burden Interview, a widely used tool for prospectively assessing
caregiver burden. Construct validity was assessed by comparing CareBI's
relationship with caregiver and care recipient outcomes, as well as sensitivity
to known burden-related risk and mitigation factors. Early findings affirm the
scale's utility in categorizing low-, moderate-, and high-burden caregivers and
guiding resource-oriented strategies. CareBI represents a reproducible tool for
embedding caregiver metrics into health operations, predictive modeling, and
public policy frameworks, and provides a template for applying operations
research and industrial engineering methods to psychosocial measurement
challenges in aging and long-term care.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.21630v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Forecast reconciliation with non-linear constraints</h3>
                    <p><strong>Authors:</strong> Daniele Girolimetto, Anastasios Panagiotelis, Tommaso Di Fonzo, Han Li</p>
                    <p>  Methods for forecasting time series adhering to linear constraints have seen
notable development in recent years, especially with the advent of forecast
reconciliation. This paper extends forecast reconciliation to the open question
of non-linearly constrained time series. Non-linear constraints can emerge with
variables that are formed as ratios such as mortality rates and unemployment
rates. On the methodological side, Non-linearly Constrained Reconciliation
(NLCR) is proposed. This algorithm adjusts forecasts that fail to meet
non-linear constraints, in a way that ensures the new forecasts meet the
constraints. The NLCR method is a projection onto a non-linear surface,
formulated as a constrained optimisation problem. On the theoretical side,
optimisation methods are again used, this time to derive sufficient conditions
for when the NLCR methodology is guaranteed to improve forecast accuracy.
Finally on the empirical side, NLCR is applied to two datasets from demography
and economics and shown to significantly improve forecast accuracy relative to
relevant benchmarks.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.21249v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Sensitivity Analysis when Generalizing Causal Effects from Multiple
  Studies to a Target Population: Motivation from the ECHO Program</h3>
                    <p><strong>Authors:</strong> Bolun Liu, Trang Quynh Nguyen, Elizabeth A. Stuart, Bryan Lau, Amii M. Kress, Michael R. Elliott, Kyle R. Busse, Ellen C. Caniglia, Yajnaseni Chakraborti, Amy J. Elliott, James E. Gern, Alison E. Hipwell, Catherine J. Karr, Kaja Z. LeWinn, Li Luo, Hans-Georg MÃ¼ller, Sunni L. Mumford, Ruby H. N. Nguyen, Emily Oken, Janet L. Peacock, Enrique F. Schisterman, Arjun Sondhi, Rosalind J. Wright, Yidong Zhou, Elizabeth L. Ogburn</p>
                    <p>  Unobserved effect modifiers can induce bias when generalizing causal effect
estimates to target populations. In this work, we extend a sensitivity analysis
framework assessing the robustness of study results to unobserved effect
modification that adapts to various generalizability scenarios, including
multiple (conditionally) randomized trials, observational studies, or
combinations thereof. This framework is interpretable and does not rely on
distributional or functional assumptions about unknown parameters. We
demonstrate how to leverage the multi-study setting to detect violation of the
generalizability assumption through hypothesis testing, showing with
simulations that the proposed test achieves high power under real-world sample
sizes. Finally, we apply our sensitivity analysis framework to analyze the
generalized effect estimate of secondhand smoke exposure on birth weight using
cohort sites from the Environmental influences on Child Health Outcomes (ECHO)
study.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.21116v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A comparison of methods for designing hybrid type 2 cluster-randomized
  trials with continuous effectiveness and implementation endpoints</h3>
                    <p><strong>Authors:</strong> Melody Owen, Fan Li, Ruyi Liu, Donna Spiegelman</p>
                    <p>  Hybrid type 2 studies are gaining popularity for their ability to assess both
implementation and health outcomes as co-primary endpoints. Often conducted as
cluster-randomized trials (CRTs), five design methods can validly power these
studies: p-value adjustment methods, combined outcomes approach, single
weighted 1-DF test, disjunctive 2-DF test, and conjunctive test. We compared
all of the methods theoretically and numerically. Theoretical comparisons of
the power equations allowed us to identify if any method globally had more or
less power than other methods. It was shown that the p-value adjustment methods
are always less powerful than the combined outcomes approach and the single
1-DF test. We also identified the conditions under which the disjunctive 2-DF
test is less powerful than the single 1-DF test. Because our theoretical
comparison showed that some methods could be more powerful than others under
certain conditions, and less powerful under others, we conducted a numerical
study to understand these differences. The crt2power R package was created to
calculate the power or sample size for CRTs with two continuous co-primary
endpoints. Using this package, we conducted a numerical evaluation across
30,000 input scenarios to compare statistical power. Specific patterns were
identified where a certain method consistently achieved the highest power. When
the treatment effects are unequal, the disjunctive 2-DF test tends to have
higher power. When the treatment effect sizes are the same, the single 1-DF
test tends to have higher power. Together, these comparisons provide clearer
insights to guide method selection for powering hybrid type 2 studies.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.20741v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Systematic Evaluation of Uncertainty Estimation Methods in Large
  Language Models</h3>
                    <p><strong>Authors:</strong> Christian Hobelsberger, Theresa Winner, Andreas Nawroth, Oliver Mitevski, Anna-Carolina Haensch</p>
                    <p>  Large language models (LLMs) produce outputs with varying levels of
uncertainty, and, just as often, varying levels of correctness; making their
practical reliability far from guaranteed. To quantify this uncertainty, we
systematically evaluate four approaches for confidence estimation in LLM
outputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For
the evaluation of the approaches, we conduct experiments on four
question-answering tasks using a state-of-the-art open-source LLM. Our results
show that each uncertainty metric captures a different facet of model
confidence and that the hybrid CoCoA approach yields the best reliability
overall, improving both calibration and discrimination of correct answers. We
discuss the trade-offs of each method and provide recommendations for selecting
uncertainty measures in LLM applications.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.20460v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Clustering of multivariate tail dependence using conditional methods</h3>
                    <p><strong>Authors:</strong> Patrick O'Toole, Christian Rohrbeck, Jordan Richards</p>
                    <p>  The conditional extremes (CE) framework has proven useful for analysing the
joint tail behaviour of random vectors. However, when applied across many
locations or variables, it can be difficult to interpret or compare the
resulting extremal dependence structures, particularly for high dimensional
vectors. To address this, we propose a novel clustering method for multivariate
extremes using the CE framework. Our approach introduces a closed-form,
computationally efficient dissimilarity measure for multivariate tails, based
on the skew-geometric Jensen-Shannon divergence, and is applicable in arbitrary
dimensions. Applying standard clustering algorithms to a matrix of pairwise
distances, we obtain interpretable groups of random vectors with homogeneous
tail dependence. Simulation studies demonstrate that our method outperforms
existing approaches for clustering bivariate extremes, and uniquely extends to
the multivariate setting. In our application to Irish meteorological data, our
clustering identifies spatially coherent regions with similar extremal
dependence between precipitation and wind speeds.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.20424v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Reorienting Age-Friendly Frameworks for Rural Contexts: A Spatial
  Competence-Press Framework for Aging in Chinese Villages</h3>
                    <p><strong>Authors:</strong> Ziyuan Gao</p>
                    <p>  While frameworks such as the WHO Age-Friendly Cities have advanced urban
aging policy, rural contexts demand fundamentally different analytical
approaches. The spatial dispersion, terrain variability, and agricultural labor
dependencies that characterize rural aging experiences require moving beyond
service-domain frameworks toward spatial stress assessment models. Current
research on rural aging in China exhibits methodological gaps, systematically
underrepresenting the spatial stressors that older adults face daily, including
terrain barriers, infrastructure limitations, climate exposure, and
agricultural labor burdens. Existing rural revitalization policies emphasize
standardized interventions while inadequately addressing spatial heterogeneity
and the spatially-differentiated needs of aging populations. This study
developed a GIS-based spatial stress analysis framework that applies Lawton and
Nahemow's competence-press model to quantify aging-related stressors and
classify rural villages by intervention needs. Using data from 27 villages in
Mamuchi Township, Shandong Province, we established four spatial stress
indicators: slope gradient index (SGI), solar radiation exposure index (SREI),
walkability index (WI), and agricultural intensity index (AII). Analysis of
variance and hierarchical clustering revealed significant variation in spatial
pressures across villages and identified distinct typologies that require
targeted intervention strategies. The framework produces both quantitative
stress measurements for individual villages and a classification system that
groups villages with similar stress patterns, providing planners and
policymakers with practical tools for designing spatially-targeted age-friendly
interventions in rural China and similar contexts.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.20343v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Multi-Task Deep Learning for Surface Metrology</h3>
                    <p><strong>Authors:</strong> D. Kucharski, A. Gaska, T. Kowaluk, K. Stepien, M. Repalska, B. Gapinski, M. Wieczorowski, M. Nawotka, P. Sobecki, P. Sosinowski, J. Tomasik, A. Wojtowicz</p>
                    <p>  A reproducible deep learning framework is presented for surface metrology to
predict surface texture parameters together with their reported standard
uncertainties. Using a multi-instrument dataset spanning tactile and optical
systems, measurement system type classification is addressed alongside
coordinated regression of Ra, Rz, RONt and their uncertainty targets
(Ra_uncert, Rz_uncert, RONt_uncert). Uncertainty is modelled via quantile and
heteroscedastic heads with post-hoc conformal calibration to yield calibrated
intervals. On a held-out set, high fidelity was achieved by single-target
regressors (R2: Ra 0.9824, Rz 0.9847, RONt 0.9918), with two uncertainty
targets also well modelled (Ra_uncert 0.9899, Rz_uncert 0.9955); RONt_uncert
remained difficult (R2 0.4934). The classifier reached 92.85% accuracy and
probability calibration was essentially unchanged after temperature scaling
(ECE 0.00504 -> 0.00503 on the test split). Negative transfer was observed for
naive multi-output trunks, with single-target models performing better. These
results provide calibrated predictions suitable to inform instrument selection
and acceptance decisions in metrological workflows.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.20339v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Treatment Effect Learning Under Sequential Randomization</h3>
                    <p><strong>Authors:</strong> Rina Friedberg, Richard Mudd, Patrick Johnstone, Melissa Pothen, Vishal Vaingankar, Vishwanath Sangale, Abbas Zaidi</p>
                    <p>  Sequential treatment assignments in online experiments lead to complex
dependency structures, often rendering identification, estimation and inference
over treatments a challenge. Treatments in one session (e.g., a user logging
on) can have an effect that persists into subsequent sessions, leading to
cumulative effects on outcomes measured at a later stage. This can render
standard methods for identification and inference trivially misspecified. We
propose T-Learners layered into the G-Formula for this setting, building on
literature from causal machine learning and identification in sequential
settings. In a simple simulation, this approach prevents decaying accuracy in
the presence of carry-over effects, highlighting the importance of
identification and inference strategies tailored to the nature of systems often
seen in the tech domain.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.20078v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>AI Pose Analysis and Kinematic Profiling of Range-of-Motion Variations
  in Resistance Training</h3>
                    <p><strong>Authors:</strong> Adam Diamant</p>
                    <p>  This study develops an AI-based pose estimation pipeline to enable precise
quantification of movement kinematics in resistance training. Using video data
from Wolf et al. (2025), which compared lengthened partial (pROM) and full
range-of-motion (fROM) training across eight upper-body exercises in 26
participants, 280 recordings were processed to extract frame-level joint-angle
trajectories. After filtering and smoothing, per-set metrics were derived,
including range of motion (ROM), tempo, and concentric/eccentric phase
durations. A random-effects meta-analytic model was applied to account for
within-participant and between-exercise variability. Results show that pROM
repetitions were performed with a smaller ROM and shorter overall durations,
particularly during the eccentric phase of movement. Variance analyses revealed
that participant-level differences, rather than exercise-specific factors, were
the primary driver of variation, although there is substantial evidence of
heterogeneous treatment effects. We then introduce a novel metric, \%ROM, which
is the proportion of full ROM achieved during pROM, and demonstrate that this
definition of lengthened partials remains relatively consistent across
exercises. Overall, these findings suggest that lengthened partials differ from
full ROM training not only in ROM, but also in execution dynamics and
consistency, highlighting the potential of AI-based methods for advancing
research and improving resistance training prescription.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.20012v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>