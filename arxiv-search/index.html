<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 5/26/2025, 1:25:06 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Bayesian Deep Learning for Discrete Choice</h3>
                    <p><strong>Authors:</strong> Daniel F. Villarraga, Ricardo A. Daziano</p>
                    <p>  Discrete choice models (DCMs) are used to analyze individual decision-making
in contexts such as transportation choices, political elections, and consumer
preferences. DCMs play a central role in applied econometrics by enabling
inference on key economic variables, such as marginal rates of substitution,
rather than focusing solely on predicting choices on new unlabeled data.
However, while traditional DCMs offer high interpretability and support for
point and interval estimation of economic quantities, these models often
underperform in predictive tasks compared to deep learning (DL) models. Despite
their predictive advantages, DL models remain largely underutilized in discrete
choice due to concerns about their lack of interpretability, unstable parameter
estimates, and the absence of established methods for uncertainty
quantification. Here, we introduce a deep learning model architecture
specifically designed to integrate with approximate Bayesian inference methods,
such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model
collapses to behaviorally informed hypotheses when data is limited, mitigating
overfitting and instability in underspecified settings while retaining the
flexibility to capture complex nonlinear relationships when sufficient data is
available. We demonstrate our approach using SGLD through a Monte Carlo
simulation study, evaluating both predictive metrics--such as out-of-sample
balanced accuracy--and inferential metrics--such as empirical coverage for
marginal rates of substitution interval estimates. Additionally, we present
results from two empirical case studies: one using revealed mode choice data in
NYC, and the other based on the widely used Swiss train choice stated
preference data.
</p>
                    <p><a href="http://arxiv.org/pdf/2505.18077v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Rethinking Climate Econometrics: Data Cleaning, Flexible Trend Controls,
  and Predictive Validation</h3>
                    <p><strong>Authors:</strong> Christof Schötz, Jan Hassel, Christian Otto</p>
                    <p>  We assess empirical models in climate econometrics using modern statistical
learning techniques. Existing approaches are prone to outliers, ignore sample
dependencies, and lack principled model selection. To address these issues, we
implement robust preprocessing, nonparametric time-trend controls, and
out-of-sample validation across 700+ climate variables. Our analysis reveals
that widely used models and predictors-such as mean temperature-have little
predictive power. A previously overlooked humidity-related variable emerges as
the most consistent predictor, though even its performance remains limited.
These findings challenge the empirical foundations of climate econometrics and
point toward a more robust, data-driven path forward.
</p>
                    <p><a href="http://arxiv.org/pdf/2505.18033v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Federated Causal Inference from Multi-Site Observational Data via
  Propensity Score Aggregation</h3>
                    <p><strong>Authors:</strong> Khellaf Rémi, Bellet Aurélien, Josse Julie</p>
                    <p>  Causal inference typically assumes centralized access to individual-level
data. Yet, in practice, data are often decentralized across multiple sites,
making centralization infeasible due to privacy, logistical, or legal
constraints. We address this by estimating the Average Treatment Effect (ATE)
from decentralized observational data using federated learning, which enables
inference through the exchange of aggregate statistics rather than
individual-level data. We propose a novel method to estimate propensity scores
in a (non-)parametric manner by computing a federated weighted average of local
scores, using two theoretically grounded weighting schemes -- Membership
Weights (MW) and Density Ratio Weights (DW) -- that balance communication
efficiency and model flexibility. These federated scores are then used to
construct two ATE estimators: the Federated Inverse Propensity Weighting
estimator (Fed-IPW) and its augmented variant (Fed-AIPW). Unlike meta-analysis
methods, which fail when any site violates positivity, our approach leverages
heterogeneity in treatment assignment across sites to improve overlap. We show
that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample
sizes, treatment mechanisms, and covariate distributions, with theoretical
analysis and experiments on simulated and real-world data highlighting their
strengths and limitations relative to meta-analysis and related methods.
</p>
                    <p><a href="http://arxiv.org/pdf/2505.17961v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Robust Distributed Estimation: Extending Gossip Algorithms to Ranking
  and Trimmed Means</h3>
                    <p><strong>Authors:</strong> Anna Van Elst, Igor Colin, Stephan Clémençon</p>
                    <p>  This paper addresses the problem of robust estimation in gossip algorithms
over arbitrary communication graphs. Gossip algorithms are fully decentralized,
relying only on local neighbor-to-neighbor communication, making them
well-suited for situations where communication is constrained. A fundamental
challenge in existing mean-based gossip algorithms is their vulnerability to
malicious or corrupted nodes. In this paper, we show that an outlier-robust
mean can be computed by globally estimating a robust statistic. More
specifically, we propose a novel gossip algorithm for rank estimation, referred
to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated
to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed
description of the proposed methods, a key contribution of our work is a
precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank
estimation and an $\mathcal{O}(\log(t)/t)$ rate for trimmed mean estimation,
where by $t$ is meant the number of iterations. Moreover, we provide a
breakdown point analysis of \textsc{GoTrim}. We empirically validate our
theoretical results through experiments on diverse network topologies, data
distributions and contamination schemes.
</p>
                    <p><a href="http://arxiv.org/pdf/2505.17836v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Predicting hazards of climate extremes: a statistical perspective</h3>
                    <p><strong>Authors:</strong> Carlotta Pacifici, Simone A. Padoan, Jaroslav Mysiak</p>
                    <p>  Climate extremes such as floods, storms, and heatwaves have caused severe
economic and human losses across Europe in recent decades. To support the
European Union's climate resilience efforts, we propose a statistical framework
for short-to-medium-term prediction of tail risks related to extreme economic
losses and fatalities. Our approach builds on Extreme Value Theory and employs
the predictive distribution of future tail events to quantify both estimation
and aleatoric uncertainty. Using data on EU-wide losses and fatalities from
1980 to 2023, we model extreme events through Peaks Over Threshold methodology
and fit Generalised Pareto (GP) and discrete-GP models using an empirical Bayes
procedure. Our predictive approach enables a 'What-if' analysis to evaluate
hypothetical scenarios beyond observed levels, including potential worst-case
outcomes for a precautionary risk assessment of future extreme episodes. To
account for a time-varying behavior of extreme losses and fatalities we extend
our predictive method using a proportional tail model that allows to handle
heteroscedastic extremes over time. Results of our analysis under stationarity
and non-stationary settings raise concerns, reinforcing the urgency of
integrating predictive tail risk assessment into EU adaptation strategies.
</p>
                    <p><a href="http://arxiv.org/pdf/2505.17622v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Local improvement of NO2 concentration maps derived from physicochemical
  models, using low-cost sensors</h3>
                    <p><strong>Authors:</strong> Camille Coron, Emma Thulliez</p>
                    <p>  Urban air quality is a major issue today. Pollutant concentrations, such as
NO2's, must be monitored to ensure that they do not exceed dangerous
thresholds. Two recent techniques help to map pollutant concentrations on a
small scale. First, deterministic physicochemical models take into account the
street network and calculate concentration estimates on a grid, providing a
map. On the other hand, the advent of new low-cost technologies allows
monitoring organizations to densify measurement networks. However, these
devices are less reliable than reference devices and need to be corrected. We
propose a new approach to improve maps generated using deterministic models by
combining measurements from multiple sensor networks. More precisely, we model
the bias of deterministic models and estimate it using an MCMC method. Our
approach also enables to analyze the behavior of the sensors. The method is
applied to the city of Rouen, France, with measurements provided by 4
monitoring stations and 10 low-cost sensors during December 2022. Results show
that the method indeed allows to correct the map, reducing estimation errors by
about 9.7%.
</p>
                    <p><a href="http://arxiv.org/pdf/2505.17564v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Novel Bayesian Extrapolation Design for Assessing Equivalence in
  Exposure-Response Curves between Pediatric and Adult Populations</h3>
                    <p><strong>Authors:</strong> Zhongheng Cai, Lian Ma, Jingjing Ye, Haitao Pan</p>
                    <p>  Development of effective treatments in pediatric population poses unique
scientific and ethical challenges in addition to the small population. In this
regard, both the U.S. and E.U. regulations suggest a complementary strategy,
pediatric extrapolation, based on assessing the relevance of existing
information in the adult population to the pediatric population. The pediatric
extrapolation approach often relies on data extrapolation from adults,
contingent upon evidence of similar disease progression, pharmacology and
clinical response to treatment between adult and children. Similarity
evaluation in pharmacology is usually characterized through the
exposure-response relationship. Current methodologies for comparing
exposure-response (E-R) curves between these groups are inadequate, typically
focusing on isolated data points rather than the entire curve spectrum (Zhang
et al., 2021). To overcome this limitation, we introduce an innovative Bayesian
approach for a comprehensive evaluation of E-R curve similarities between adult
and pediatric populations. This method encompasses the entire curve, employing
logistic regression for binary endpoints. We have developed an algorithm to
determine sample size and key design parameters, such as the Bayesian posterior
probability threshold, and utilize the maximum curve distance as a measure of
similarity. Integrating Bayesian and frequentist principles, our approach
involves developing a method to simulate datasets under both null and
alternative hypotheses, allowing for type I error and type II error control.
Simulation studies and sensitivity analyses demonstrate that our method
maintains a stable performance with type I error and type II error control.
</p>
                    <p><a href="http://arxiv.org/pdf/2505.17397v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Deconfounded Warm-Start Thompson Sampling with Applications to Precision
  Medicine</h3>
                    <p><strong>Authors:</strong> Prateek Jaiswal, Esmaeil Keyvanshokooh, Junyu Cao</p>
                    <p>  Randomized clinical trials often require large patient cohorts before drawing
definitive conclusions, yet abundant observational data from parallel studies
remains underutilized due to confounding and hidden biases. To bridge this gap,
we propose Deconfounded Warm-Start Thompson Sampling (DWTS), a practical
approach that leverages a Doubly Debiased LASSO (DDL) procedure to identify a
sparse set of reliable measured covariates and combines them with key hidden
covariates to form a reduced context. By initializing Thompson Sampling (LinTS)
priors with DDL-estimated means and variances on these measured features --
while keeping uninformative priors on hidden features -- DWTS effectively
harnesses confounded observational data to kick-start adaptive clinical trials.
Evaluated on both a purely synthetic environment and a virtual environment
created using real cardiovascular risk dataset, DWTS consistently achieves
lower cumulative regret than standard LinTS, showing how offline causal
insights from observational data can improve trial efficiency and support more
personalized treatment decisions.
</p>
                    <p><a href="http://arxiv.org/pdf/2505.17283v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Transfer Faster, Price Smarter: Minimax Dynamic Pricing under
  Cross-Market Preference Shift</h3>
                    <p><strong>Authors:</strong> Yi Zhang, Elynn Chen, Yujun Yan</p>
                    <p>  We study contextual dynamic pricing when a target market can leverage K
auxiliary markets -- offline logs or concurrent streams -- whose mean utilities
differ by a structured preference shift. We propose Cross-Market Transfer
Dynamic Pricing (CM-TDP), the first algorithm that provably handles such
model-shift transfer and delivers minimax-optimal regret for both linear and
non-parametric utility models.
  For linear utilities of dimension d, where the difference between source- and
target-task coefficients is $s_{0}$-sparse, CM-TDP attains regret
$\tilde{O}((d*K^{-1}+s_{0})\log T)$. For nonlinear demand residing in a
reproducing kernel Hilbert space with effective dimension $\alpha$, complexity
$\beta$ and task-similarity parameter $H$, the regret becomes
$\tilde{O}\!(K^{-2\alpha\beta/(2\alpha\beta+1)}T^{1/(2\alpha\beta+1)} +
H^{2/(2\alpha+1)}T^{1/(2\alpha+1)})$, matching information-theoretic lower
bounds up to logarithmic factors. The RKHS bound is the first of its kind for
transfer pricing and is of independent interest.
  Extensive simulations show up to 50% lower cumulative regret and 5 times
faster learning relative to single-market pricing baselines. By bridging
transfer learning, robust aggregation, and revenue optimization, CM-TDP moves
toward pricing systems that transfer faster, price smarter.
</p>
                    <p><a href="http://arxiv.org/pdf/2505.17203v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Interpretable contour level selection for heat maps for gridded data</h3>
                    <p><strong>Authors:</strong> Tarn Duong</p>
                    <p>  Gridded data formats, where the observed multivariate data are aggregated
into grid cells, ensure confidentiality and reduce storage requirements, with
the trade-off that access to the underlying point data is lost. Heat maps are a
highly pertinent visualisation for gridded data, and heat maps with a small
number of well-selected contour levels offer improved interpretability over
continuous contour levels. There are many possible contour level choices.
Amongst them, density contour levels are highly suitable in many cases, and
their probabilistic interpretation form a rigorous statistical basis for
further quantitative data analyses. Current methods for computing density
contour levels requires access to the observed point data, so they are not
applicable to gridded data. To remedy this, we introduce an approximation of
density contour levels for gridded data. We then compare our proposed method to
existing contour level selection methods, and conclude that our proposal
provides improved interpretability for synthetic and experimental gridded data.
</p>
                    <p><a href="http://arxiv.org/pdf/2505.16788v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>