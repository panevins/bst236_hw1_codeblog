<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions to update, hopefully each day at midnight.</p>
    <p id="last-updated">Last updated: 2/12/2025, 12:08:57 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>A Nonparametric and Functional Wombling Methodology</h3>
                    <p><strong>Authors:</strong> Luke A. Barratt, John A. D. Aston</p>
                    <p>  Wombling methods, first introduced in 1951, have been widely applied to
detect boundaries and variations across spatial domains, particularly in
biological, public health and meteorological studies. Traditional applications
focus on finite-dimensional observations, where significant changes in
measurable traits indicate structural boundaries. In this work, wombling
methodologies are extended to functional data, enabling the identification of
spatial variation in infinite-dimensional settings. Proposed is a nonparametric
approach that accommodates functional observations without imposing strict
distributional assumptions. This methodology successfully captures complex
spatial structures and discontinuities, demonstrating superior sensitivity and
robustness compared to existing finite-dimensional techniques. This methodology
is then applied to analyse regional epidemiological disparities between London
and the rest of the UK, identifying key spatial boundaries in the shape of the
first trajectory of Covid-19 incidence in 2020. Through extensive simulations
and empirical validation, demonstrated is the method's effectiveness in
uncovering meaningful spatial variations, with potential applications in a wide
variety of fields.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07740v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Evaluating the impact of racial disproportionality in Stop & Search on
  expressive crimes in London</h3>
                    <p><strong>Authors:</strong> Yu Luo, Yijing Li</p>
                    <p>  Racial disproportionality in Stop & Search practices elicits substantial
concerns about its societal and behavioral impacts. This paper aims to
investigate the effect of disproportionality, particularly on the black
community, on expressive crimes in London using data from January 2019 to
December 2023. We focus on a semi-parametric partially linear structural
regression method and introduce a Bayesian empirical likelihood procedure
combined with double machine learning techniques to control for
high-dimensional confounding and to accommodate the strong prior assumption. In
addition, we show that the proposed procedure generates a valid posterior in
terms of coverage. Applying this approach to the Stop & Search dataset, we find
that racial disproportionality aimed at the Black community may be alleviated
by taking into account the proportion of the Black population when focusing on
expressive crimes.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07695v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Response rate estimation in single-stage basket trials: A comparison of
  estimators that allow for borrowing across cohorts</h3>
                    <p><strong>Authors:</strong> Antonios Daletzakis, Rutger van den Bor, Vincent van der Noort, Kit CB Roes</p>
                    <p>  Therapeutic advancements in oncology have shifted towards targeted therapy
based on genomic aberrations. This necessitates innovative statistical
approaches in clinical trials, particularly in master protocol studies. Basket
trials, a type of master protocol, evaluate a single treatment across cohorts
sharing a genomic aberration but differing in tumor histology. While offering
operational advantages, basket trial analysis presents statistical inference
challenges. These trials help determine for which tumor histology the treatment
is promising enough to advance to confirmatory evaluation and often use
Bayesian designs to support decisions. Beyond decision-making, estimating
cohort-specific response rates is crucial for designing subsequent trials. This
study compares seven Bayesian estimation methods for basket trials with binary
outcomes against the (frequentist) sample proportion estimate through
simulations. The goal is to estimate cohort-specific response rates, focusing
on bias, mean squared error, and information borrowing. Various scenarios are
examined, including homogeneous, heterogeneous, and clustered response rates
across cohorts. Results show trade-offs in bias and precision, highlighting the
importance of method selection. Berry's method performs best with limited
heterogeneity. No clear winner emerges in general cases, with performance
affected by shrinkage, bias, and the choice of priors and tuning parameters.
Challenges include computational complexity, parameter tuning, and the lack of
clear guidance on selection. Researchers should consider these factors when
designing and analyzing basket trials.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07639v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Forecasting the future development in quality and value of professional
  football players for applications in team management</h3>
                    <p><strong>Authors:</strong> Koen W. van Arem, Floris Goes-Smit, Jakob Söhl</p>
                    <p>  Transfers in professional football (soccer) are risky investments because of
the large transfer fees and high risks involved. Although data-driven models
can be used to improve transfer decisions, existing models focus on describing
players' historical progress, leaving their future performance unknown.
Moreover, recent developments have called for the use of explainable models
combined with uncertainty quantification of predictions. This paper assesses
explainable machine learning models based on predictive accuracy and
uncertainty quantification methods for the prediction of the future development
in quality and transfer value of professional football players. Using a
historical data set of data-driven indicators describing player quality and the
transfer value of a football player, the models are trained to forecast player
quality and player value one year ahead. These two prediction problems
demonstrate the efficacy of tree-based models, particularly random forest and
XGBoost, in making accurate predictions. In general, the random forest model is
found to be the most suitable model because it provides accurate predictions as
well as an uncertainty quantification method that naturally arises from the
bagging procedure of the random forest model. Additionally, our research shows
that the development of player performance contains nonlinear patterns and
interactions between variables, and that time series information can provide
useful information for the modeling of player performance metrics. Our research
provides models to help football clubs make more informed, data-driven transfer
decisions by forecasting player quality and transfer value.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07528v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Quantitative evaluation of unsupervised clustering algorithms for
  dynamic total-body PET image analysis</h3>
                    <p><strong>Authors:</strong> Oona Rainio, Maria K. Jaakkola, Riku Klén</p>
                    <p>  Background. Recently, dynamic total-body positron emission tomography (PET)
imaging has become possible due to new scanner devices. While clustering
algorithms have been proposed for PET analysis already earlier, there is still
little research systematically evaluating these algorithms for processing of
dynamic total-body PET images. Materials and methods. Here, we compare the
performance of 15 unsupervised clustering methods, including K-means either by
itself or after principal component analysis (PCA) or independent component
analysis (ICA), Gaussian mixture model (GMM), fuzzy c-means (FCM),
agglomerative clustering, spectral clustering, and several newer clustering
algorithms, for classifying time activity curves (TACs) in dynamic PET images.
We use dynamic total-body $^{15}$O-water PET images collected from 30 patients
with suspected or confirmed coronary artery disease. To evaluate the clustering
algorithms in a quantitative way, we use them to classify 5000 TACs from each
image based on whether the curve is taken from brain, right heart ventricle,
right kidney, lower right lung lobe, or urinary bladder. Results. According to
our results, the best methods are GMM, FCM, and ICA combined with mini batch
K-means, which classified the TACs with a median accuracies of 89\%, 83\%, and
81\%, respectively, in a processing time of half a second or less on average
for each image. Conclusion. GMM, FCM, and ICA with mini batch K-means show
promise for dynamic total-body PET analysis.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07511v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Applications of robust statistics for cyclostationarity detection in
  non-Gaussian signals for local damage detection in bearings</h3>
                    <p><strong>Authors:</strong> Wojciech Żuławiński, Jérôme Antoni, Radosław Zimroz, Agnieszka Wyłomańska</p>
                    <p>  Signals with periodic characteristics are ubiquitous in real-world
applications. One of these areas is condition monitoring, where the vibration
signals from rotating machines naturally display periodic behavior. Thus, the
cyclostationary analysis has evolved into the investigation of such signals.
For the traditional cyclostationary approaches, the autocovariance function
(ACVF) and its bi-frequency representation, spectral coherence (SC), are
regarded as the base. However, recent research has revealed that real vibration
signals increasingly exhibit impulsive behavior in addition to periodicity. As
a result, there was a need for new methods to identify periodic behavior that
take into account the impulsiveness of the data. In this article, we provide a
way to improve the SC method by using its robust variants in place of the
classical ACVF estimator (sample ACVF). The suggested concept is intuitive and
relatively simple. We create robust versions of the SC algorithm that more
accurately detect periodic behavior in signals with significant disruptions in
contrast to the classical techniques. The efficiency of the proposed approach
is demonstrated for simulated signals with three different types of
non-Gaussian noise distribution and different levels of periodic impulses
imitating a local damage. The introduced approach is also validated on real
vibration signal from the rolling element bearings operating in a crushing
machine.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07478v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>The Variable Multiple Bandpass Periodic Block Bootstrap for Time Series
  with Multiple Periodic Correlations</h3>
                    <p><strong>Authors:</strong> Edward Valachovic</p>
                    <p>  This work introduces a novel block bootstrap method for time series with
multiple periodically correlated (MPC) components called the Variable Multiple
Bandpass Periodic Block Bootstrap (VMBPBB). While past methodological
advancements permitted bootstrapping time series to preserve certain
correlations, and then periodically correlated (PC) structures, there does not
appear to be adequate or efficient methods to bootstrap estimate the sampling
distribution of estimators for MPC time series. Current methods that preserve
the PC correlation structure resample the original time series, selecting block
size to preserve one PC component frequency while simultaneously and
unnecessarily resampling all frequencies. This destroys PC components at other
frequencies. VMBPBB uses bandpass filters to separate each PC component,
creating a set of PC component time series each composed principally of one
component. VMBPBB then resamples each PC component time series, not the
original MPC time series, with the respective block size preserving the
correlation structure of each PC component. Finally, VMBPBB aggregates the PC
component bootstraps to form a bootstrap of the MPC time series, successfully
preserving all correlations. A simulation study across a wide range of
different MPC component frequencies and signal-to-noise ratios is presented and
reveals that VMBPBB almost universally outperforms existing methods that fail
to bandpass filter the MPC time series.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07462v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Valeriepieris Circles Reveal City and Regional Boundaries in England and
  Wales</h3>
                    <p><strong>Authors:</strong> Rudy Arthur, Federico Botta</p>
                    <p>  We propose a new method of determining regional and city boundaries based on
the Valeriepieris circle, the smallest circle containing a given fraction of
the data. By varying the fraction in the circle we can map complex spatial data
to a simple model of concentric rings which we then fit to determine natural
density cutoffs. We apply this method to population, occupation, economic and
transport data from England and Wales, finding that the regions determined by
this method affirm well known social facts such as the disproportionate wealth
of London or the relative isolation of the North East and South West of
England. We then show how different data sets give us different views of the
same cities, providing insight into their development and dynamics.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07451v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Likelihood-Free Estimation for Spatiotemporal Hawkes processes with
  missing data and application to predictive policing</h3>
                    <p><strong>Authors:</strong> Pramit Das, Moulinath Banerjee, Yuekai Sun</p>
                    <p>  With the growing use of AI technology, many police departments use
forecasting software to predict probable crime hotspots and allocate patrolling
resources effectively for crime prevention. The clustered nature of crime data
makes self-exciting Hawkes processes a popular modeling choice. However, one
significant challenge in fitting such models is the inherent missingness in
crime data due to non-reporting, which can bias the estimated parameters of the
predictive model, leading to inaccurate downstream hotspot forecasts, often
resulting in over or under-policing in various communities, especially the
vulnerable ones. Our work introduces a Wasserstein Generative Adversarial
Networks (WGAN) driven likelihood-free approach to account for unreported
crimes in Spatiotemporal Hawkes models. We demonstrate through empirical
analysis how this methodology improves the accuracy of parametric estimation in
the presence of data missingness, leading to more reliable and efficient
policing strategies.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07111v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Framework for Supervised and Unsupervised Segmentation and
  Classification of Materials Microstructure Images</h3>
                    <p><strong>Authors:</strong> Kungang Zhang, Daniel W. Apley, Wei Chen, Wing K. Liu, L. Catherine Brinson</p>
                    <p>  Microstructure of materials is often characterized through image analysis to
understand processing-structure-properties linkages. We propose a largely
automated framework that integrates unsupervised and supervised learning
methods to classify micrographs according to microstructure phase/class and,
for multiphase microstructures, segments them into different homogeneous
regions. With the advance of manufacturing and imaging techniques, the
ultra-high resolution of imaging that reveals the complexity of microstructures
and the rapidly increasing quantity of images (i.e., micrographs) enables and
necessitates a more powerful and automated framework to extract materials
characteristics and knowledge. The framework we propose can be used to
gradually build a database of microstructure classes relevant to a particular
process or group of materials, which can help in analyzing and
discovering/identifying new materials. The framework has three steps: (1)
segmentation of multiphase micrographs through a recently developed score-based
method so that different microstructure homogeneous regions can be identified
in an unsupervised manner; (2) {identification and classification of}
homogeneous regions of micrographs through an uncertainty-aware supervised
classification network trained using the segmented micrographs from Step $1$
with their identified labels verified via the built-in uncertainty
quantification and minimal human inspection; (3) supervised segmentation (more
powerful than the segmentation in Step $1$) of multiphase microstructures
through a segmentation network trained with micrographs and the results from
Steps $1$-$2$ using a form of data augmentation. This framework can iteratively
characterize/segment new homogeneous or multiphase materials while expanding
the database to enhance performance. The framework is demonstrated on various
sets of materials and texture images.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07107v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>

<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins">@panevins</a> on GitHub</p>
</footer>

</html>