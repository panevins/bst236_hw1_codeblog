<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 11/10/2025, 12:25:22 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Function on Scalar Regression with Complex Survey Designs</h3>
                    <p><strong>Authors:</strong> Lily Koffman, Sunan Gao, Xinkai Zhou, Andrew Leroux, Ciprian Crainiceanu, John Muschelli III</p>
                    <p>  Large health surveys increasingly collect high-dimensional functional data
from wearable devices, and function on scalar regression (FoSR) is often used
to quantify the relationship between these functional outcomes and scalar
covariates such as age and sex. However, existing methods for FoSR fail to
account for complex survey design. We introduce inferential methods for FoSR
for studies with complex survey designs. The method combines fast univariate
inference (FUI) developed for functional data outcomes and survey sampling
inferential methods developed for scalar outcomes. Our approach consists of
three steps: (1) fit survey weighted GLMs at each point along the functional
domain, (2) smooth coefficients along the functional domain, and (3) use
balanced repeated replication (BRR) or the Rao-Wu-Yue-Beaumont (RWYB) bootstrap
to obtain pointwise and joint confidence bands for the functional coefficients.
The method is motivated by association studies between continuous physical
activity data and covariates collected in the National Health and Nutrition
Examination Survey (NHANES). A first-of-its-kind analytical simulation study
and empirical simulation using the NHANES data demonstrates that our method
performs better than existing methods that do not account for the survey
structure. Finally, application of the method in NHANES shows the practical
implications of accounting for survey structure. The method is implemented in
the R package svyfosr.
</p>
                    <p><a href="http://arxiv.org/pdf/2511.05487v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Exponential Spatiotemporal GARCH Model with Asymmetric Volatility
  Spillovers</h3>
                    <p><strong>Authors:</strong> Ariane Nidelle Meli Chrisko, Philipp Otto, Wolfgang Schmid</p>
                    <p>  This paper introduces a spatiotemporal exponential generalised autoregressive
conditional heteroscedasticity (spatiotemporal E-GARCH) model, extending
traditional spatiotemporal GARCH models by incorporating asymmetric volatility
spillovers, while also generalising the time-series E-GARCH model to a
spatiotemporal setting with instantaneous, potentially asymmetric volatility
spillovers across space. The model allows for both temporal and spatial
dependencies in volatility dynamics, capturing how financial shocks propagate
across time, space, and network structures. We establish the theoretical
properties of the model, deriving stationarity conditions and moment existence
results. For estimation, we propose a quasi-maximum likelihood (QML) estimator
and assess its finite-sample performance through Monte Carlo simulations.
Empirically, we apply the model to financial networks, specifically analysing
volatility spillovers in stock markets. We compare different network structures
and analyse asymmetric effects in instantaneous volatility interactions.
</p>
                    <p><a href="http://arxiv.org/pdf/2511.05126v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>On the Estimation of Climate Normals and Anomalies</h3>
                    <p><strong>Authors:</strong> Tommaso Proietti, Alessandro Giovannelli</p>
                    <p>  The quantification of the interannual component of variability in
climatological time series is essential for the assessment and prediction of
the El Ni\~{n}o - Southern Oscillation phenomenon. This is achieved by
estimating the deviation of a climate variable (e.g., temperature, pressure,
precipitation, or wind strength) from its normal conditions, defined by its
baseline level and seasonal patterns. Climate normals are currently estimated
by simple arithmetic averages calculated over the most recent 30-year period
ending in a year divisible by 10. The suitability of the standard methodology
has been questioned in the context of a changing climate, characterized by
nonstationary conditions. The literature has focused on the choice of the
bandwidth and the ability to account for trends induced by climate change. The
paper contributes to the literature by proposing a regularized real time filter
based on local trigonometric regression, optimizing the estimation
bias-variance trade-off in the presence of climate change, and by introducing a
class of seasonal kernels enhancing the localization of the estimates of
climate normals. Application to sea surface temperature series in the \nino 3.4
region and zonal and trade winds strength in the equatorial and tropical
Pacific region, illustrates the relevance of our proposal.
</p>
                    <p><a href="http://arxiv.org/pdf/2511.05071v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Uncertainty quantification and parameter optimization of plasma etching
  process using heteroscedastic Gaussian process</h3>
                    <p><strong>Authors:</strong> Yongsu Jung, Minji Kang, Muyoung Kim, Min Sup Choi, Hyeong-U Kim, Jaekwang Kim</p>
                    <p>  This study presents a comprehensive framework for uncertainty quantification
(UQ) and design optimization of plasma etching in semiconductor manufacturing.
The framework is demonstrated using experimental measurements of etched depth
collected at nine wafer locations under various plasma conditions. A
heteroscedastic Gaussian process (hetGP) surrogate model is employed to capture
the complex uncertainty structure in the data, enabling distinct quantification
of (a) spatial variability across the wafer and (b) process-related uncertainty
arising from variations in chamber pressure, gas flow rate, and RF power.
Epistemic uncertainty due to sparse data is further quantified and incorporated
into a reliability-based design optimization (RBDO) scheme. The proposed method
identifies optimal process parameters that minimize spatial variability of etch
depth while maintaining reliability under both aleatory and epistemic
uncertainties. The results demonstrate that this framework effectively
integrates data-driven surrogate modeling with robust optimization, enhancing
predictive accuracy and process reliability. Moreover, the proposed approach is
generalizable to other semiconductor processes, such as photolithography, where
performance is highly sensitive to multifaceted uncertainties.
</p>
                    <p><a href="http://arxiv.org/pdf/2511.04990v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Estimating Inhomogeneous Spatio-Temporal Background Intensity Functions
  using Graphical Dirichlet Processes</h3>
                    <p><strong>Authors:</strong> Isaías Bañales, Tomoaki Nishikawa, Yoshihiro Ito, Manuel J. Aguilar-Velázquez</p>
                    <p>  An enhancement in seismic measuring instrumentation has been proven to have
implications in the quantity of observed earthquakes, since denser networks
usually allow recording more events. However, phenomena such as strong
earthquakes or even aseismic transients, as slow slip earthquakes, may alter
the occurrence of earthquakes. In the field of seismology, it is a standard
practice to model background seismicity as a Poisson process. Based on this
idea, this work proposes a model that can incorporate the evolving spatial
intensity of Poisson processes over time (i.e., we include temporal changes in
the background seismicity when modeling). In recent years, novel methodologies
have been developed for quantifying the uncertainty in the estimation of the
background seismicity in homogeneous cases using Bayesian non-parametric
techniques. This work proposes a novel methodology based on graphical Dirichlet
processes for incorporating spatial and temporal inhomogeneities in background
seismicity. The proposed model in this work is applied to study the seismicity
in the southern Mexico, using recorded data from 2000 to 2015.
</p>
                    <p><a href="http://arxiv.org/pdf/2511.04974v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Clinical-ComBAT: a diffusion-weighted MRI harmonization method for
  clinical applications</h3>
                    <p><strong>Authors:</strong> Gabriel Girard, Manon Edde, Félix Dumais, Yoan David, Matthieu Dumont, Guillaume Theaud, Jean-Christophe Houde, Arnaud Boré, Maxime Descoteaux, Pierre-Marc Jodoin</p>
                    <p>  Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps
are effective for assessing neurodegenerative diseases and microstructural
properties of white matter in large number of brain conditions. However, DW-MRI
inherently limits the combination of data from multiple acquisition sites
without harmonization to mitigate scanner-specific biases. While the widely
used ComBAT method reduces site effects in research, its reliance on linear
covariate relationships, homogeneous populations, fixed site numbers, and well
populated sites constrains its clinical use. To overcome these limitations, we
propose Clinical-ComBAT, a method designed for real-world clinical scenarios.
Clinical-ComBAT harmonizes each site independently, enabling flexibility as new
data and clinics are introduced. It incorporates a non-linear polynomial data
model, site-specific harmonization referenced to a normative site, and variance
priors adaptable to small cohorts. It further includes hyperparameter tuning
and a goodness-of-fit metric for harmonization assessment. We demonstrate its
effectiveness on simulated and real data, showing improved alignment of
diffusion metrics and enhanced applicability for normative modeling.
</p>
                    <p><a href="http://arxiv.org/pdf/2511.04871v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Inference for the Extended Functional Cox Model: A UK Biobank Case Study</h3>
                    <p><strong>Authors:</strong> Erjia Cui, Angela Zhao, Ciprian M. Crainiceanu</p>
                    <p>  Multiple studies have shown that scalar summaries of objectively measured
physical activity (PA) using accelerometers are the strongest predictors of
mortality, outperforming all traditional risk factors, including age, sex, body
mass index (BMI), and smoking. Here we show that diurnal patterns of PA and
their day-to-day variability provide additional information about mortality. To
do that, we introduce a class of extended functional Cox models and
corresponding inferential tools designed to quantify the association between
multiple functional and scalar predictors with time-to-event outcomes in
large-scale (large $n$) high-dimensional (large $p$) datasets. Methods are
applied to the UK Biobank study, which collected PA at every minute of the day
for up to seven days, as well as time to mortality ($93{,}370$ participants
with good quality accelerometry data and $931$ events). Simulation studies show
that methods perform well in realistic scenarios and scale up to studies an
order of magnitude larger than the UK Biobank accelerometry study. Establishing
the feasibility and scalability of these methods for such complex and large
data sets is a major milestone in applied Functional Data Analysis (FDA).
</p>
                    <p><a href="http://arxiv.org/pdf/2511.04852v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Dynamic causal discovery in Alzheimer's disease through latent
  pseudotime modelling</h3>
                    <p><strong>Authors:</strong> Natalia Glazman, Jyoti Mangal, Pedro Borges, Sebastien Ourselin, M. Jorge Cardoso</p>
                    <p>  The application of causal discovery to diseases like Alzheimer's (AD) is
limited by the static graph assumptions of most methods; such models cannot
account for an evolving pathophysiology, modulated by a latent disease
pseudotime. We propose to apply an existing latent variable model to real-world
AD data, inferring a pseudotime that orders patients along a data-driven
disease trajectory independent of chronological age, then learning how causal
relationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC
0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge
substantially improved graph accuracy and orientation. Our framework reveals
dynamic interactions between novel (NfL, GFAP) and established AD markers,
enabling practical causal discovery despite violated assumptions.
</p>
                    <p><a href="http://arxiv.org/pdf/2511.04619v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Nonparametric Safety Stock Dimensioning: A Data-Driven Approach for
  Supply Chains of Hardware OEMs</h3>
                    <p><strong>Authors:</strong> Elvis Agbenyega, Cody Quick</p>
                    <p>  Resilient supply chains are critical, especially for Original Equipment
Manufacturers (OEMs) that power today's digital economy. Safety Stock
dimensioning-the computation of the appropriate safety stock quantity-is one of
several mechanisms to ensure supply chain resiliency, as it protects the supply
chain against demand and supply uncertainties. Unfortunately, the major
approaches to dimensioning safety stock heavily assume that demand is normally
distributed and ignore future demand variability, limiting their applicability
in manufacturing contexts where demand is non-normal, intermittent, and highly
skewed. In this paper, we propose a data-driven approach that relaxes the
assumption of normality, enabling the demand distribution of each inventory
item to be analytically determined using Kernel Density Estimation. Also, we
extended the analysis from historical demand variability to forecasted demand
variability. We evaluated the proposed approach against a normal distribution
model in a near-world inventory replenishment simulation. Afterwards, we used a
linear optimization model to determine the optimal safety stock configuration.
The results from the simulation and linear optimization models showed that the
data-driven approach outperformed traditional approaches. In particular, the
data-driven approach achieved the desired service levels at lower safety stock
levels than the conventional approaches.
</p>
                    <p><a href="http://arxiv.org/pdf/2511.04616v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>TRAECR: A Tool for Preprocessing Positron Emission Tomography Imaging
  for Statistical Modeling</h3>
                    <p><strong>Authors:</strong> Akhil Ambekar, Robert Zielinski, Ani Eloyan</p>
                    <p>  Positron emission tomography (PET) imaging is widely used in a number of
clinical applications, including cancer and Alzheimer's disease (AD) diagnosis,
monitoring of disease development, and treatment effect evaluation. Statistical
modeling of PET imaging is essential to address continually emerging scientific
questions in these research fields, including hypotheses related to evaluation
of effects of disease modifying treatments on amyloid reduction in AD and
associations between amyloid reduction and cognitive function, among many
others. In this paper, we provide background information and tools for
statisticians interested in developing statistical models for PET imaging to
pre-process and prepare data for analysis. We introduce our novel
pre-processing and visualization tool TRAECR (Template registration, MRI-PET
co-Registration, Anatomical brain Extraction and COMBAT/RAVEL harmonization) to
facilitate data preparation for statistical analysis.
</p>
                    <p><a href="http://arxiv.org/pdf/2511.04458v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>