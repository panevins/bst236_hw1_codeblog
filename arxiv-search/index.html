<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 9/8/2025, 1:22:38 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Beyond Linearity and Time-homogeneity: Relational Hyper Event Models
  with Time-Varying Non-Linear Effects</h3>
                    <p><strong>Authors:</strong> Martina Boschi, JÃ¼rgen Lerner, Ernst C. Wit</p>
                    <p>  Recent technological advances have made it easier to collect large and
complex networks of time-stamped relational events connecting two or more
entities. Relational hyper-event models (RHEMs) aim to explain the dynamics of
these events by modeling the event rate as a function of statistics based on
past history and external information.
  However, despite the complexity of the data, most current RHEM approaches
still rely on a linearity assumption to model this relationship. In this work,
we address this limitation by introducing a more flexible model that allows the
effects of statistics to vary non-linearly and over time. While time-varying
and non-linear effects have been used in relational event modeling, we take
this further by modeling joint time-varying and non-linear effects using tensor
product smooths.
  We validate our methodology on both synthetic and empirical data. In
particular, we use RHEMs to study how patterns of scientific collaboration and
impact evolve over time. Our approach provides deeper insights into the dynamic
factors driving relational hyper-events, allowing us to evaluate potential
non-monotonic patterns that cannot be identified using linear models.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.05289v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Precision Dose-Finding Design for Phase I Oncology Trials by Integrating
  Pharmacology Data</h3>
                    <p><strong>Authors:</strong> Kyong Ju Lee, Yuan Ji</p>
                    <p>  Phase I oncology trials aim to identify a safe yet effective dose - often the
maximum tolerated dose (MTD) - for subsequent studies. Conventional designs
focus on population-level toxicity modeling, with recent attention on
leveraging pharmacokinetic (PK) data to improve dose selection. We propose the
Precision Dose-Finding (PDF) design, a novel Bayesian phase I framework that
integrates individual patient PK profiles into the dose-finding process. By
incorporating patient-specific PK parameters (such as volume of distribution
and elimination rate), PDF models toxicity risk at the individual level, in
contrast to traditional methods that ignore inter-patient variability. The
trial is structured in two stages: an initial training stage to update model
parameters using cohort-based dose escalation, and a subsequent test stage in
which doses for new patients are chosen based on each patient's own
PK-predicted toxicity probability. This two-stage approach enables truly
personalized dose assignment while maintaining rigorous safety oversight.
Extensive simulation studies demonstrate the feasibility of PDF and suggest
that it provides improved safety and dosing precision relative to the continual
reassessment method (CRM). The PDF design thus offers a refined dose-finding
strategy that tailors the MTD to individual patients, aligning phase I trials
with the ideals of precision medicine.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.05120v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Dynamics of Liquidity Surfaces in Uniswap v3</h3>
                    <p><strong>Authors:</strong> Jimmy Risk, Shen-Ning Tung, Tai-Ho Wang</p>
                    <p>  This paper presents a comprehensive study on the empirical dynamics of
Uniswap v3 liquidity, which we model as a time-tick surface, $L_t(x)$. Using a
combination of functional principal component analysis (FPCA) and dynamic
factor methods, we analyze three distinct pools over multiple sample periods.
Our findings offer three main contributions: a statistical characterization of
automated market maker liquidity, an interpretable and portable basis for
dimension reduction, and a robust analysis of liquidity dynamics using rolling
window metrics. For the 5 bps pools, the leading empirical eigenfunctions
explain the majority of cross-tick variation and remain stable, aligning
closely with a low-order Legendre polynomial basis. This alignment provides a
parsimonious and interpretable structure, similar to the dynamic Nelson-Siegel
method for yield curves. The factor coefficients exhibit a time series
structure well-captured by AR(1) models with clear GARCH-type
heteroskedasticity and heavy-tailed innovations.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.05013v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>An information metric for comparing and assessing informative interim
  decisions in sequential clinical trials</h3>
                    <p><strong>Authors:</strong> G. Caruso, W. F. Rosenberger, P. Mozgunov, N. Flournoy</p>
                    <p>  Group sequential designs enable interim analyses and potential early stopping
for efficacy or futility. While these adaptations improve trial efficiency and
ethical considerations, they also introduce bias into the adapted analyses. We
demonstrate how failing to account for informative interim decisions in the
analysis can substantially affect posterior estimates of the treatment effect,
often resulting in overly optimistic credible intervals aligned with the
stopping decision. Drawing on information theory, we use the Kullback-Leibler
divergence to quantify this distortion and highlight its use for post-hoc
evaluation of informative interim decisions, with a focus on end-of-study
inference. Unlike pointwise comparisons, this measure provides an integrated
summary of this distortion on the whole parameter space. By comparing
alternative decision boundaries and prior specifications, we illustrate how
this measure can improve the understanding of trial results and inform the
planning of future adaptive studies. We also introduce an expected version of
this metric to support clinicians in choosing decision boundaries. This
guidance complements traditional strategies based on type-I error rate control
by offering insights into the distortion introduced to the treatment effect at
each interim phase. The use of this pre-experimental measure is finally
illustrated in a group sequential trial for evaluating a treatment for central
nervous system disorders.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.04904v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Latent Class Bayesian Model for Multivariate Longitudinal Outcomes
  with Excess Zeros</h3>
                    <p><strong>Authors:</strong> Chitradipa Chakraborty, Kiranmoy Das</p>
                    <p>  Latent class models have been successfully used to handle complex datasets in
different disciplines. For longitudinal outcomes, we often get a trajectory of
the outcome for each individual, and on that basis, we cluster them for a
powerful statistical inference. Latent class models have been used to handle
multivariate longitudinal outcomes coming from biology, health sciences, and
economics. In this paper, we propose a Bayesian latent class model for
multivariate outcomes with excess zeros. We consider a Tobit model for
zero-inflated continuous outcomes such as out-of-pocket medical expenses
(OOPME), a two-part model for financial debt, and a ZIP model for counting
outcomes with excess zeros. We develop a Bayesian mixture model and employ an
adaptive Lasso-type shrinkage method for variable selection. We analyze data
from the Health and Retirement Study conducted by the University of Michigan
and consider modeling four important outcomes measuring the physical and
financial health of the aged individuals. Our analysis detects several latent
clusters for different outcomes. Practical usefulness of the proposed model is
validated through a simulation study.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.04804v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Inferring Piece Value in Chess and Chess Variants</h3>
                    <p><strong>Authors:</strong> Steven Pav</p>
                    <p>  We use logistic regression to estimate the value of the pieces in standard
chess and several chess variants, namely Chess 960, Atomic chess, Antichess,
and Horde chess. We perform our regressions on several years of data from
Lichess, the free and open-source internet chess server. We use the published
player ratings to control for the confounding effect of differential player
skill. We adjust for the attenuation bias in regressions due to the noise in
observed ratings. We find that major piece values, relative to the value of a
pawn, are fairly consistent with historical valuation systems. However we find
slightly higher value to bishops than knights. We find that piece values are
smaller, in absolute value, in Atomic and Antichess than standard chess. We
also present approximate values of the pieces to equalize odds when players of
varying skill face off.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.04691v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Precision Mental Health: Predicting Heterogeneous Treatment Effects for
  Depression through Data Integration</h3>
                    <p><strong>Authors:</strong> Carly L. Brantner, Trang Quynh Nguyen, Harsh Parikh, Congwen Zhao, Hwanhee Hong, Elizabeth A. Stuart</p>
                    <p>  When treating depression, clinicians are interested in determining the
optimal treatment for a given patient, which is challenging given the amount of
treatments available. To advance individualized treatment allocation,
integrating data across multiple randomized controlled trials (RCTs) can
enhance our understanding of treatment effect heterogeneity by increasing
available information. However, extending these inferences to individuals
outside of the original RCTs remains crucial for clinical decision-making. We
introduce a two-stage meta-analytic method that predicts conditional average
treatment effects (CATEs) in target patient populations by leveraging the
distribution of CATEs across RCTs. Our approach generates 95\% prediction
intervals for CATEs in target settings using first-stage models that can
incorporate parametric regression or non-parametric methods such as causal
forests or Bayesian additive regression trees (BART). We validate our method
through simulation studies and operationalize it to integrate multiple RCTs
comparing depression treatments, duloxetine and vortioxetine, to generate
prediction intervals for target patient profiles. Our analysis reveals no
strong evidence of effect heterogeneity across trials, with the exception of
potential age-related variability. Importantly, we show that CATE prediction
intervals capture broader uncertainty than study-specific confidence intervals
when warranted, reflecting both within-study and between-study variability.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.04604v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>An Interactive Tool for Analyzing High-Dimensional Clusterings</h3>
                    <p><strong>Authors:</strong> Justin Lin, Julia Fukuyama</p>
                    <p>  Technological advances have spurred an increase in data complexity and
dimensionality. We are now in an era in which data sets containing thousands of
features are commonplace. To digest and analyze such high-dimensional data,
dimension reduction techniques have been developed and advanced along with
computational power. Of these techniques, nonlinear methods are most commonly
employed because of their ability to construct visually interpretable
embeddings. Unlike linear methods, these methods non-uniformly stretch and
shrink space to create a visual impression of the high-dimensional data. Since
capturing high-dimensional structures in a significantly lower number of
dimensions requires drastic manipulation of space, nonlinear dimension
reduction methods are known to occasionally produce false structures,
especially in noisy settings. In an effort to deal with this phenomenon, we
developed an interactive tool that enables analysts to better understand and
diagnose their dimension reduction results. It uses various analytical plots to
provide a multi-faceted perspective on results to determine legitimacy. The
tool is available via an R package named DRtool.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.04603v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Incentivising Personalised Colorectal Cancer Screening: an Adversarial
  Risk Analysis Approach</h3>
                    <p><strong>Authors:</strong> Daniel Corrales, David Rios Insua</p>
                    <p>  This paper presents a framework for incentivising colorectal cancer (CRC)
screening programs from the perspective of policymakers and under the
assumption that the citizens participating in the program have misaligned
objectives. To do so, it leverages tools from adversarial risk analysis to
propose an optimal incentive scheme under uncertainty. The work relies on
previous work on modeling CRC risk and optimal screening strategies and
provides use cases regarding individual and group-based optimal incentives
based on a simple financial scheme.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.04592v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Aligning load flexibility with emissions reduction: empirical insights
  from a multi-site study of cryptocurrency data centers</h3>
                    <p><strong>Authors:</strong> Veronica M. Paez, Neda Mohammadi, John E. Taylor</p>
                    <p>  The power sector is responsible for 32 percent of global greenhouse gas
emissions. Data centers and cryptocurrencies use significant amounts of
electricity and contribute to these emissions. Demand-side flexibility of data
centers is one possible approach for reducing greenhouse gas emissions from
these industries. To explore this, we use novel data collected from the Bitcoin
mining industry to investigate the impact of load flexibility on power system
decarbonization. Employing engineered metrics to explore curtailment dynamics
and emissions alignment, we provide the first empirical analysis of
cryptocurrency data centers' capability for reducing greenhouse gas emissions
in response to real-time grid signals. Our results highlight the importance of
strategically aligning operational behaviors with emissions signals to maximize
avoided emissions. These findings offer insights for policymakers and industry
stakeholders to enhance load flexibility and meet climate goals in these
otherwise energy intensive data centers.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.04380v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>