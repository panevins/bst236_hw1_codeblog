<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 10/20/2025, 1:24:30 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Enhanced Renewable Energy Forecasting using Context-Aware Conformal
  Prediction</h3>
                    <p><strong>Authors:</strong> Alireza Moradi, Mathieu Tanneau, Reza Zandehshahvar, Pascal Van Hentenryck</p>
                    <p>  Accurate forecasting is critical for reliable power grid operations,
particularly as the share of renewable generation, such as wind and solar,
continues to grow. Given the inherent uncertainty and variability in renewable
generation, probabilistic forecasts have become essential for informed
operational decisions. However, such forecasts frequently suffer from
calibration issues, potentially degrading decision-making performance. Building
on recent advances in Conformal Predictions, this paper introduces a tailored
calibration framework that constructs context-aware calibration sets using a
novel weighting scheme. The proposed framework improves the quality of
probabilistic forecasts at the site and fleet levels, as demonstrated by
numerical experiments on large-scale datasets covering several systems in the
United States. The results demonstrate that the proposed approach achieves
higher forecast reliability and robustness for renewable energy applications
compared to existing baselines.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.15780v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Incorporating estimands into meta-analyses of clinical trials</h3>
                    <p><strong>Authors:</strong> Antonio Remiro-Az√≥car, Pepa Polavieja, Emmanuelle Boutmy, Alessandro Ghiretti, Lise Lotte Nystrup Husemoen, Khadija Rerhou Rantell, Tatsiana Vaitsiakhovich, David M. Phillippo, Jay J. H. Park, Helle Lynggaard, Robert Bauer, Antonia Morga</p>
                    <p>  The estimand framework is increasingly established to pose research questions
in confirmatory clinical trials. In evidence synthesis, the uptake of estimands
has been modest, and the PICO (Population, Intervention, Comparator, Outcome)
framework is more often applied. While PICOs and estimands have overlapping
elements, the estimand framework explicitly considers different strategies for
intercurrent events. We propose a pragmatic framework for the use of estimands
in meta-analyses of clinical trials, highlighting the value of estimands to
systematically identify and mitigate key sources of quantitative heterogeneity,
and to enhance the applicability or external validity of pooled estimates.
Focus is placed on the role of strategies for intercurrent events, within the
specific context of meta-analyses for health technology assessment. We apply
the estimand framework to a network meta-analysis of clinical trials, comparing
the efficacy of semaglutide versus dulaglutide in type 2 diabetes. We explore
the impact of a treatment policy strategy for treatment discontinuation or
initiation of rescue medication versus a hypothetical strategy for the
corresponding intercurrent events. The specification of different target
estimands at the meta-analytical level allows us to be explicit about the
source of heterogeneity, the intercurrent event strategy, driving any potential
differences in results. We advocate for the integration of estimands into the
planning of meta-analyses, while acknowledging that potential challenges exist
in the absence of subject-level data. Estimands can complement PICOs to
strengthen communication between stakeholders about what evidence syntheses
seek to demonstrate, and to ensure that the generated evidence is maximally
relevant to healthcare decision-makers.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.15762v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A nonstationary seasonal Dynamic Factor Model: an application to
  temperature time series from the state of Minas Gerais</h3>
                    <p><strong>Authors:</strong> Davi Oliveira Chaves, Chang Chiann, Pedro Alberto Morettin</p>
                    <p>  In many scientific fields, such as agriculture, temperature time series are
of interest both as explanatory variables and as objects of study in their own
right. However, at the state level, incorporating information from all possible
locations in an analysis can be overwhelming, while using a summary measure,
such as the state-wide average temperature, can result in significant
information loss. In this context, using Dynamic Factor Models (DFMs) provides
a compelling alternative for analyzing such multivariate time series, as they
allow for the extraction of a small number of common factors that capture the
majority of the variability in the data. Given that temperature series are
typically seasonal, this study applies a nonstationary seasonal DFM to analyze
a multivariate temperature time series from the state of Minas Gerais. The
results show that the data can be effectively represented by two seasonal
factors: the first captures the general seasonal pattern of the state, while
the second contrasts the months of highest annual temperatures between two
distinct regions.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.15667v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Robust Estimation of Polyserial Correlation</h3>
                    <p><strong>Authors:</strong> Max Welz</p>
                    <p>  The association between a continuous and an ordinal variable is commonly
modeled through the polyserial correlation model. However, this model, which is
based on a partially-latent normality assumption, may be misspecified in
practice, due to, for example (but not limited to), outliers or careless
responses. We demonstrate that the typically used maximum likelihood (ML)
estimator is highly susceptible to such misspecification: One single
observation not generated by partially-latent normality can suffice to produce
arbitrarily poor estimates. As a remedy, we propose a novel estimator of the
polyserial correlation model designed to be robust against the adverse effects
of observations discrepant to that model. The estimator achieves robustness by
implicitly downweighting such observations; the ensuing weights constitute a
useful tool for pinpointing potential sources of model misspecification. We
show that the proposed estimator generalizes ML and is consistent as well as
asymptotically Gaussian. As price for robustness, some efficiency must be
sacrificed, but substantial robustness can be gained while maintaining more
than 98% of ML efficiency. We demonstrate our estimator's robustness and
practical usefulness in simulation experiments and an empirical application in
personality psychology where our estimator helps identify outliers. Finally,
the proposed methodology is implemented in free open-source software.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.15632v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Adaptive Influence Diagnostics in High-Dimensional Regression</h3>
                    <p><strong>Authors:</strong> Abdul-Nasah Soale, Adewale Lukman</p>
                    <p>  An adaptive Cook's distance (ACD) for diagnosing influential observations in
high-dimensional single-index models with multicollinearity and outlier
contamination is proposed. ACD is a model-free technique built on sparse local
linear gradients to temper leverage effects. In simulations spanning low- and
high-dimensional design settings with strong correlation, ACD based on LASSO
(ACD-LASSO) and SCAD (ACD-SCAD) penalties reduced masking and swamping relative
to classical Cook's distance and local influence as well as the DF-Model and
Case-Weight adjusted solution for LASSO. Trimming points flagged by ACD
stabilizes variable selection while preserving core signals. Applications to
two datasets--the 1960 US cities pollution study and a high-dimensional
riboflavin genomics experiment show consistent gains in selection stability and
interpretability.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.15618v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Temporal Functional Factor Analysis of Brain Connectivity</h3>
                    <p><strong>Authors:</strong> Kyle Stanley, Nicole Lazar, Matthew Reimherr</p>
                    <p>  Many analyses of functional magnetic resonance imaging (fMRI) examine
functional connectivity (FC), or the statistical dependencies among distant
brain regions. These analyses are typically exploratory, guiding future
confirmatory research. In this work, we present an approach based on factor
analysis (FA) that is well-suited to studying FC. FA is appealing in this
context because its flexible model assumptions permit a guided investigation of
its target subspace consistent with the exploratory role of connectivity
analyses. However, applying FA to fMRI data poses three problems: (1) its
target subspace captures short-range spatial dependencies that should be
treated as noise, (2) it requires factorization of a massive spatial
covariance, and (3) it overlooks temporal dependencies in the data. To address
these limitations, we develop a factor model within the framework of functional
data analysis--a field which views certain data as arising from smooth
underlying curves. The proposed approach (1) uses matrix completion techniques
to filter short-range spatial dependencies out of its target subspace, (2)
employs a distributed algorithm for factorizing large-scale covariance
matrices, and (3) leverages functional regression to exploit temporal dynamics.
Together, these innovations yield a comprehensive and scalable method for
studying FC.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.15580v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Residual Kriging for Regional-Scale Canopy Height Mapping: Insights into
  GEDI-Induced Anisotropies and Sparse Sampling</h3>
                    <p><strong>Authors:</strong> Kamel Lahssini, Guerric le Maire, Nicolas Baghdadi, Ibrahim Fayad</p>
                    <p>  Quantifying aboveground biomass (AGB) is essential in the context of global
climate change. Canopy height, which is related to AGB, can be mapped using
machine learning models trained with multi-source spatial data and GEDI
measurements. In this study, a comparative analysis of canopy height estimates
derived from two models is presented: a U-Net deep learning model (CHNET) and a
Random Forest algorithm (RFH). Both models were trained using GEDI lidar data
and utilized multi-source inputs, including optical, radar, and environmental
data. While CHNET can leverage its convolutional architecture to account for
spatial correlations, we observed that it does not fully incorporate all the
spatial autocorrelation present in GEDI canopy height measurements. By
conducting a spatial analysis of the models' residuals, we also identified that
GEDI data acquisition parameters, particularly the variability in laser beam
energy combined with the azimuthal directions of the observation tracks,
introduce spatial inconsistencies in the measurements in the form of periodic
patterns. To address these anisotropies, we considered exclusively GEDI power
beams, and we conducted our spatial autocorrelation analysis in the GEDI track
azimuthal direction. Next, we employed the residual kriging (RK) spatial
interpolation technique to account for the spatial autocorrelation of canopy
heights and improve the accuracies of CHNET and RFH estimates. Adding RK
corrections improved the performance of both CHNET and RFH, with more
substantial gains observed for RFH. The corrections appeared to be localized
around the GEDI sample points and the density of usable GEDI information is
therefore an important factor in the effectiveness of spatial interpolation.
Furthermore, our findings reveal that a Random Forest model combined with
spatial interpolation can deliver performance comparable to that of a U-Net
model alone.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.15572v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>AI and analytics in sports: Leveraging BERTopic to map the past and
  chart the future</h3>
                    <p><strong>Authors:</strong> Manit Mishra</p>
                    <p>  Purpose: The purpose of this study is to map the body of scholarly literature
at the intersection of artificial intelligence (AI), analytics and sports and
thereafter, leverage the insights generated to chart guideposts for future
research. Design/methodology/approach: The study carries out systematic
literature review (SLR). Preferred Reporting Items for Systematic Reviews and
Meta-Analysis (PRISMA) protocol is leveraged to identify 204 journal articles
pertaining to utilization of AI and analytics in sports published during 2002
to 2024. We follow it up with extraction of the latent topics from sampled
articles by leveraging the topic modelling technique of BERTopic. Findings: The
study identifies the following as predominant areas of extant research on usage
of AI and analytics in sports: performance modelling, physical and mental
health, social media sentiment analysis, and tactical tracking. Each extracted
topic is further examined in terms of its relative prominence, representative
studies, and key term associations. Drawing on these insights, the study
delineates promising avenues for future inquiry. Research
limitations/implications: The study offers insights to academicians and sports
administrators on transformational impact of AI and analytics in sports.
Originality/value: The study introduces BERTopic as a novel approach for
extracting latent structures in sports research, thereby advancing both
scholarly understanding and the methodological toolkit of the field.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.15487v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Dynamic Spatial Treatment Effects as Continuous Functionals: Theory and
  Evidence from Healthcare Access</h3>
                    <p><strong>Authors:</strong> Tatsuru Kikuchi</p>
                    <p>  I develop a continuous functional framework for spatial treatment effects
grounded in Navier-Stokes partial differential equations. Rather than discrete
treatment parameters, the framework characterizes treatment intensity as
continuous functions $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous
analysis of boundary evolution, spatial gradients, and cumulative exposure.
Empirical validation using 32,520 U.S. ZIP codes demonstrates exponential
spatial decay for healthcare access ($\kappa = 0.002837$ per km, $R^2 =
0.0129$) with detectable boundaries at 37.1 km. The framework successfully
diagnoses when scope conditions hold: positive decay parameters validate
diffusion assumptions near hospitals, while negative parameters correctly
signal urban confounding effects. Heterogeneity analysis reveals 2-13 $\times$
stronger distance effects for elderly populations and substantial education
gradients. Model selection strongly favors logarithmic decay over exponential
($\Delta \text{AIC} > 10,000$), representing a middle ground between
exponential and power-law decay. Applications span environmental economics,
banking, and healthcare policy. The continuous functional framework provides
predictive capability ($d^*(t) = \xi^* \sqrt{t}$), parameter sensitivity
($\partial d^*/\partial \nu$), and diagnostic tests unavailable in traditional
difference-in-differences approaches.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.15324v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Neural Posterior Estimation for Cataloging Astronomical Images from the
  Legacy Survey of Space and Time</h3>
                    <p><strong>Authors:</strong> Yicun Duan, Xinyue Li, Camille Avestruz, Jeffrey Regier</p>
                    <p>  The Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) will
commence full-scale operations in 2026, yielding an unprecedented volume of
astronomical images. Constructing an astronomical catalog, a table of imaged
stars, galaxies, and their properties, is a fundamental step in most scientific
workflows based on astronomical image data. Traditional deterministic
cataloging methods lack statistical coherence as cataloging is an ill-posed
problem, while existing probabilistic approaches suffer from computational
inefficiency, inaccuracy, or the inability to perform inference with multiband
coadded images, the primary output format for LSST images. In this article, we
explore a recently developed Bayesian inference method called neural posterior
estimation (NPE) as an approach to cataloging. NPE leverages deep learning to
achieve both computational efficiency and high accuracy. When evaluated on the
DC2 Simulated Sky Survey -- a highly realistic synthetic dataset designed to
mimic LSST data -- NPE systematically outperforms the standard LSST pipeline in
light source detection, flux measurement, star/galaxy classification, and
galaxy shape measurement. Additionally, NPE provides well-calibrated posterior
approximations. These promising results, obtained using simulated data,
illustrate the potential of NPE in the absence of model misspecification.
Although some degree of model misspecification is inevitable in the application
of NPE to real LSST images, there are a variety of strategies to mitigate its
effects.
</p>
                    <p><a href="http://arxiv.org/pdf/2510.15315v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>