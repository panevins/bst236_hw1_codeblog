<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 7/7/2025, 1:28:06 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Strategies and statistical evaluation of Italy's regional model for
  COVID-19 restrictions</h3>
                    <p><strong>Authors:</strong> Giuseppe Drago, Giulia Marcon, Alberto Lombardo, Giuseppe Aiello</p>
                    <p>  This study presents a comprehensive assessment of the Italian risk model used
during the COVID-19 pandemic to guide regional mobility restrictions through a
colour-coded classification system. The research focuses on evaluating the
variables selected by the Italian Ministry of Health for this purpose and their
effectiveness in supporting public health decision-making. The analysis adopts
a statistical framework which combines data reduction and regression modelling
techniques to enhance interpretability and predictive accuracy. Dimensionality
reduction is applied to address multicollinearity and simplify complex variable
structures, while an ordinal regression model is employed to investigate the
relationship between the reduced set of variables and the colour regional
classifications. Model performance is evaluated using classification error
metrics, providing insights into the adequacy of the selected variables in
explaining the decision-making process. Results reveal significant redundancy
within the variables chosen by the Italian Ministry of Health, suggesting that
excessive predictors may compromise information. To address this, the study
proposes refined and robust predictive models for regional classification,
offering a reliable tool of the proposed framework and to support public health
decision-makers. This study contributes to the ongoing development of
quantitative methodologies aimed at improving the effectiveness of statistical
models in guiding public health policies. The findings offer valuable insights
for refining data-driven decision-making processes during health crises and
improving the quality of information available to policymakers.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.02504v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Variance Decomposition Approach to Inconclusives in Forensic Black Box
  Studies</h3>
                    <p><strong>Authors:</strong> Amanda Luby, Joseph B. Kadane</p>
                    <p>  In the US, `black box' studies are increasingly being used to estimate the
error rate of forensic disciplines. A sample of forensic examiner participants
are asked to evaluate a set of items whose source is known to the researchers
but not to the participants. Participants are asked to make a source
determination (typically an identification, exclusion, or some kind of
inconclusive). We study inconclusives in two black box studies, one on
fingerprints and one on bullets. Rather than treating all inconclusive
responses as functionally correct (as is the practice in reported error rates
in the two studies we address), irrelevant to reported error rates (as some
would do), or treating them all as potential errors (as others would do), we
propose that the overall pattern of inconclusives in a particular black box
study can shed light on the proportion of inconclusives that are due to
examiner variability. Raw item and examiner variances are computed, and
compared with the results of a logistic regression model that takes account of
which items were addressed by which examiner. The error rates reported in black
box studies are substantially smaller than ``failure rate" analyses that take
inconclusives into account. The magnitude of this difference is highly
dependent on the particular study at hand.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.02240v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>BACTA-GPT: An AI-Based Bayesian Adaptive Clinical Trial Architect</h3>
                    <p><strong>Authors:</strong> Krishna Padmanabhan, Danny Baker</p>
                    <p>  Bayesian adaptive clinical trials offer a flexible and efficient alternative
to traditional fixed-design trials, but their implementation is often hindered
by the complexity of Bayesian computations and the need for advanced
statistical programming expertise. The authors introduce a custom fine-tuned
LLM designed to assist with this and lower barriers to adoption of Bayesian
methods for adaptive clinical trials. This paper describes the development and
fine-tuning of BACTA-GPT, a Large Language Model (LLM)-based tool designed to
assist in the implementation of Bayesian Adaptive Clinical Trials. This engine
uses GPT-3.5 as the underlying model and takes in Natural Language input from
the Statistician or the Trialist. The fine-tuned model demonstrates a viable
proof-of-concept in its objectives. Test case evaluations show that the model
is capable of generating a fit-for-purpose Bayesian model for an adaptive trial
and evaluate its operating characteristics via simulations using R and JAGS.
The integration of AI code generation has significant potential to lower
technical barriers for the design and implementation of Bayesian Adaptive
trials. But they also require attention to important considerations regarding
validation and quality control.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.02130v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Modeling the Deterioration of Pavement Skid Resistance and Surface
  Texture After Preventive Maintenance</h3>
                    <p><strong>Authors:</strong> Lu Gao, Zia Din, Kinam Kim, Ahmed Senouci</p>
                    <p>  This study investigates the deterioration of skid resistance and surface
macrotexture following preventive maintenance using micro-milling techniques.
Field data were collected from 31 asphalt pavement sections located across four
climatic zones in Texas, encompassing a variety of surface types, milling
depths, operational speeds, and drum configurations. A standardized data
collection protocol was followed, with measurements taken before milling,
immediately after treatment, and at 3, 6, 12, and 18 months post-treatment.
Skid number and Mean Profile Depth (MPD) were used to evaluate surface friction
and texture characteristics. The dataset was reformatted into a time-series
structure with 930 observations, incorporating contextual variables such as
climatic zone, treatment parameters, and baseline surface condition. A
comparative modeling framework was applied to predict the deterioration trends
of both skid resistance and macrotexture over time. Eight regression models,
including linear, tree-based, and ensemble methods, were evaluated alongside a
sequence-to-one transformer model. Results show that the transformer model
achieved the highest prediction accuracy for skid resistance (R2=0.981), while
Random Forest performing best for macrotexture prediction (R2 = 0.838). The
findings indicate that the degradation of surface characteristics after
preventive maintenance is nonlinear and influenced by a combination of
environmental and operational factors. This study demonstrates the
effectiveness of data-driven modeling in supporting transportation agencies
with pavement performance forecasting and maintenance planning.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.01842v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Using Wavelet Domain Fingerprints to Improve Source Camera
  Identification</h3>
                    <p><strong>Authors:</strong> Xinle Tian, Matthew Nunes, Emiko Dupont, Shaunagh Downing, Freddie Lichtenstein, Matt Burns</p>
                    <p>  Camera fingerprint detection plays a crucial role in source identification
and image forensics, with wavelet denoising approaches proving to be
particularly effective in extracting sensor pattern noise (SPN). In this
article, we propose a modification to wavelet-based SPN extraction. Rather than
constructing the fingerprint as an image, we introduce the notion of a wavelet
domain fingerprint. This avoids the final inversion step of the denoising
algorithm and allows fingerprint comparisons to be made directly in the wavelet
domain. As such, our modification streamlines the extraction and comparison
process. Experimental results on real-world datasets demonstrate that our
method not only achieves higher detection accuracy but can also significantly
improve processing speed.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.01712v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Simulation and evaluation of local daily temperature and precipitation
  series derived by stochastic downscaling of ERA5 reanalysis</h3>
                    <p><strong>Authors:</strong> Silius M. Vandeskog, Thordis L. Thorarinsdottir, Alex Lenkoski</p>
                    <p>  Reanalysis products such as the ERA5 reanalysis are commonly used as proxies
for observed atmospheric conditions. These products are convenient to use due
to their global coverage, the large number of available atmospheric variables
and the physical consistency between these variables, as well as their
relatively high spatial and temporal resolutions. However, despite the
continuous improvements in accuracy and increasing spatial and temporal
resolutions of reanalysis products, they may not always capture local
atmospheric conditions, especially for highly localised variables such as
precipitation. This paper proposes a computationally efficient stochastic
downscaling of ERA5 temperature and precipitation. The method combines
information from ERA5 and surface observations from nearby stations in a
non-linear regression framework that combines generalised additive models
(GAMs) with regression splines and auto-regressive moving average (ARMA) models
to produce realistic time series of local daily temperature and precipitation.
Using a wide range of evaluation criteria that address different properties of
the data, the proposed framework is shown to improve the representation of
local temperature and precipitation compared to ERA5 at over 4000 locations in
Europe over a period of 60 years.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.01692v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>The Hybrid Renewable Energy Forecasting and Trading Competition 2024</h3>
                    <p><strong>Authors:</strong> Jethro Browell, Dennis van der Meer, Henrik KÃ¤lvegren, Sebastian Haglund, Edoardo Simioni, Ricardo J. Bessa, Yi Wang</p>
                    <p>  The Hybrid Energy Forecasting and Trading Competition challenged participants
to forecast and trade the electricity generation from a 3.6GW portfolio of wind
and solar farms in Great Britain for three months in 2024. The competition
mimicked operational practice with participants required to submit genuine
forecasts and market bids for the day-ahead on a daily basis. Prizes were
awarded for forecasting performance measured by Pinball Score, trading
performance measured by total revenue, and combined performance based on rank
in the other two tracks. Here we present an analysis of the participants'
performance and the learnings from the competition. The forecasting track
reaffirms the competitiveness of popular gradient boosted tree algorithms for
day-ahead wind and solar power forecasting, though other methods also yielded
strong results, with performance in all cases highly dependent on
implementation. The trading track offers insight into the relationship between
forecast skill and value, with trading strategy and underlying forecasts
influencing performance. All competition data, including power production,
weather forecasts, electricity market data, and participants' submissions are
shared for further analysis and benchmarking.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.01579v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Parsimonious Gaussian mixture models with piecewise-constant eigenvalue
  profiles</h3>
                    <p><strong>Authors:</strong> Tom Szwagier, Pierre-Alexandre Mattei, Charles Bouveyron, Xavier Pennec</p>
                    <p>  Gaussian mixture models (GMMs) are ubiquitous in statistical learning,
particularly for unsupervised problems. While full GMMs suffer from the
overparameterization of their covariance matrices in high-dimensional spaces,
spherical GMMs (with isotropic covariance matrices) certainly lack flexibility
to fit certain anisotropic distributions. Connecting these two extremes, we
introduce a new family of parsimonious GMMs with piecewise-constant covariance
eigenvalue profiles. These extend several low-rank models like the celebrated
mixtures of probabilistic principal component analyzers (MPPCA), by enabling
any possible sequence of eigenvalue multiplicities. If the latter are
prespecified, then we can naturally derive an expectation-maximization (EM)
algorithm to learn the mixture parameters. Otherwise, to address the
notoriously-challenging issue of jointly learning the mixture parameters and
hyperparameters, we propose a componentwise penalized EM algorithm, whose
monotonicity is proven. We show the superior likelihood-parsimony tradeoffs
achieved by our models on a variety of unsupervised experiments: density
fitting, clustering and single-image denoising.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.01542v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Bayesian Modeling of Long-Term Dynamics in Indian Temperature Extremes</h3>
                    <p><strong>Authors:</strong> Chitradipa Chakraborty</p>
                    <p>  Annual maximum temperature data provides crucial insights into the impacts of
climate change, especially for regions like India, where temperature variations
have significant implications for agriculture, health, and infrastructure. In
this study, we propose the Coupled Continuous Time Random Walk (CTRW) model to
analyze annual maximum temperature data in India from 1901 to 2017 and compare
its performance with the Bayesian Spectral Analysis Regression (BSAR) model.
The CTRW model extends the standard framework by coupling temperature changes
(jumps) and waiting times, capturing complex dynamics such as memory effects
and non-Markovian behavior. The BSAR model, in contrast, combines a linear
trend component with a non-linear isotonic function, modeled using a Gaussian
Process (GP) prior, to account for smooth and flexible non-linear variations in
temperature. By applying both models to the temperature data, we evaluate their
ability to capture long-term trends and seasonal fluctuations, offering
valuable insights into the effects of climate change on temperature dynamics in
India. The comparison highlights the strengths and limitations of each approach
in modeling temperature extremes and provides a robust framework for
understanding climate variability.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.01540v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Reduced Efficiency in the Attentional Network During Distractor
  Suppression in Mild Cognitive Impairment</h3>
                    <p><strong>Authors:</strong> Jatupong Oboun, Piyanon Charoenpoonpanich, Anna Raksapatcharawong, Chaipat Chunharas, Itthi Chatnuntawech, Chainarong Amornbunchornvej, Sirawaj Itthipuripat</p>
                    <p>  Mild Cognitive Impairment (MCI) is a critical transitional stage between
normal cognitive aging and dementia, making its early detection essential. This
study investigates the neural mechanisms of distractor suppression in MCI
patients using EEG and behavioral data during an attention-cueing Eriksen
flanker task. A cohort of 56 MCIs and 26 healthy controls (HCs) performed tasks
with congruent and incongruent stimuli of varying saliency levels. During these
tasks, EEG data were analyzed for alpha band coherence's functional
connectivity, focusing on Global Efficiency (GE), while Reaction Time (RT) and
Hit Rate (HR) were also collected.
  Our findings reveal significant interactions between congruency, saliency,
and cognitive status on GE, RT, and HR. In HCs, congruent conditions resulted
in higher GE (p = 0.0114, multivariate t-distribution correction, MVT), faster
RTs (p < 0.0001, MVT), and higher HRs (p < 0.0001, MVT) compared to incongruent
conditions. HCs also showed increased GE in salient conditions for incongruent
trials (p = 0.0406, MVT). MCIs exhibited benefits from congruent conditions
with shorter RTs and higher HRs (both p < 0.0001, MVT) compared to incongruent
conditions but showed reduced adaptability in GE, with no significant GE
differences between conditions.
  These results highlight the potential of alpha band coherence and GE as early
markers for cognitive impairment. By integrating GE, RT, and HR, this study
provides insights into the interplay between neural efficiency, processing
speed, and task accuracy. This approach offers valuable insights into cognitive
load management and interference effects, indicating benefits for interventions
aimed at improving attentional control and processing speed in MCIs.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.01433v2" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>