<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 9/1/2025, 1:24:23 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>DynaMark: A Reinforcement Learning Framework for Dynamic Watermarking in
  Industrial Machine Tool Controllers</h3>
                    <p><strong>Authors:</strong> Navid Aftabi, Abhishek Hanchate, Satish Bukkapatnam, Dan Li</p>
                    <p>  Industry 4.0's highly networked Machine Tool Controllers (MTCs) are prime
targets for replay attacks that use outdated sensor data to manipulate
actuators. Dynamic watermarking can reveal such tampering, but current schemes
assume linear-Gaussian dynamics and use constant watermark statistics, making
them vulnerable to the time-varying, partly proprietary behavior of MTCs. We
close this gap with DynaMark, a reinforcement learning framework that models
dynamic watermarking as a Markov decision process (MDP). It learns an adaptive
policy online that dynamically adapts the covariance of a zero-mean Gaussian
watermark using available measurements and detector feedback, without needing
system knowledge. DynaMark maximizes a unique reward function balancing control
performance, energy consumption, and detection confidence dynamically. We
develop a Bayesian belief updating mechanism for real-time detection confidence
in linear systems. This approach, independent of specific system assumptions,
underpins the MDP for systems with linear dynamics. On a Siemens Sinumerik 828D
controller digital twin, DynaMark achieves a reduction in watermark energy by
70% while preserving the nominal trajectory, compared to constant variance
baselines. It also maintains an average detection delay equivalent to one
sampling interval. A physical stepper-motor testbed validates these findings,
rapidly triggering alarms with less control performance decline and exceeding
existing benchmarks.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.21797v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Quantile Function-Based Models for Neuroimaging Classification Using
  Wasserstein Regression</h3>
                    <p><strong>Authors:</strong> Jie Li, Gary Green, Jian Zhang</p>
                    <p>  We propose a novel quantile function-based approach for neuroimaging
classification using Wasserstein-Fr\'echet regression, specifically applied to
the detection of mild traumatic brain injury (mTBI) based on the MEG and MRI
data. Conventional neuroimaging classification methods for mTBI detection
typically extract summary statistics from brain signals across the different
epochs, which may result in the loss of important distributional information,
such as variance, skewness, kurtosis, etc. Our approach treats complete
probability density functions of epoch space results as functional response
variables within a Wasserstein-Fr\'echet regression framework, thereby
preserving the full distributional characteristics of epoch results from
$L_{1}$ minimum norm solutions. The global Wasserstein-Fr\'echet regression
model incorporating covariates (age and gender) allows us to directly compare
the distributional patterns between healthy control subjects and mTBI patients.
The classification procedure computes Wasserstein distances between estimated
quantile functions from control and patient groups, respectively. These
distances are then used as the basis for diagnostic decisions. This framework
offers a statistically principled approach to improving diagnostic accuracy in
mTBI detection. In practical applications, the test accuracy on unseen data
from Innovision IP's dataset achieves up to 98\%.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.21523v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Causal Analysis of Health, Education, and Economic Well-Being in India
  -- Evidence from the Young Lives Survey</h3>
                    <p><strong>Authors:</strong> Anushka De, Diganta Mukherjee</p>
                    <p>  This study investigates the dynamic and potentially causal relationships
among childhood health, education, and long-term economic well-being in India
using longitudinal data from the Young Lives Survey. While prior research often
examines these domains in isolation, we adopt an integrated empirical framework
combining panel data methods, instrumental variable regression, and causal
graph analysis to disentangle their interdependencies. Our analysis spans five
survey rounds covering two cohorts of children tracked from early childhood to
young adulthood. Results indicate strong persistence in household economic
status, highlighting limited intergenerational mobility. Education, proxied by
Item Response Theory-based mathematics scores, consistently emerges as the most
robust predictor of future economic well-being, particularly in the younger
cohort. In contrast, self-reported childhood health shows limited direct impact
on either education or later wealth, though it is influenced by household
economic conditions. These findings underscore the foundational role of wealth
and the growing importance of cognitive achievement in shaping life
trajectories. The study supports policy approaches that prioritize early
investments in learning outcomes alongside targeted economic support for
disadvantaged households. By integrating statistical modeling with development
policy insights, this research contributes to understanding how early-life
conditions shape economic opportunity in low- and middle-income contexts.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.21370v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Population-Scale Network Embeddings Expose Educational Divides in
  Network Structure Related to Right-Wing Populist Voting</h3>
                    <p><strong>Authors:</strong> Malte LÃ¼ken, Javier Garcia-Bernardo, Sreeparna Deb, Flavio Hafner, Megha Khosla</p>
                    <p>  Administrative registry data can be used to construct population-scale
networks whose ties reflect shared social contexts between persons. With
machine learning, such networks can be encoded into numerical representations
-- embeddings -- that automatically capture individuals' position within the
network. We created embeddings for all persons in the Dutch population from a
population-scale network that represents five shared contexts: neighborhood,
work, family, household, and school. To assess the informativeness of these
embeddings, we used them to predict right-wing populist voting. Embeddings
alone predicted right-wing populist voting above chance-level but performed
worse than individual characteristics. Combining the best subset of embeddings
with individual characteristics only slightly improved predictions. However,
after transforming the embeddings to make their dimensions more sparse and
orthogonal, we found that one embedding dimension was strongly associated with
the outcome. Mapping this dimension back to the population network revealed
differences in network structure related to right-wing populist voting between
different school ties and achieved education levels. Our study contributes
methodologically by demonstrating how population-scale network embeddings can
be made interpretable, and substantively by linking structural network
differences in education to right-wing populist voting.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.21236v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Re-Examining the Statistical Methodology and Onomastic Claims of Gregor
  and Blais' Argument from Name Popularity</h3>
                    <p><strong>Authors:</strong> Jason Wilson, Luuk van de Weghe</p>
                    <p>  In 2024 Gregor and Blais published a JSNT article using two different
statistical methods to conclude, contra Bauckham (2017), that selected
Apocryphal texts and the Babylonian Talmud "do not correspond to the
distribution among first-century Palestinian Jews statistically significantly
worse than the distribution in Gospels-Acts" and "the two corpora paradoxically
align better in some respects". In this paper, we show that the first method is
statistically invalid, and the second is the wrong tool for the job. This is in
alignment with the critique of Van de Weghe and Wilson (2024) and in support of
their use of the chi-squared goodness-of-fit test which established name
occurrences in the Gospels and Acts, as opposed to Gregor and Blais` uniform,
apocryphal, or Talmudic corpora, "fit into their historical context at least as
well as those in the works of Josephus". Regarding this historical context,
helpful insights are provided by Gregor and Blais regarding potential
distortions within the onomastic reference distribution, and this article
suggests a way forward, addressing orthographic issues, sample biases, several
problems with the implementation of Gregor and Blais` inclusion criteria, and
87 new onomastic finds from ossuaries, ostraca, and documentary papyri that
need to be incorporated into the lexicon. While legitimate concerns are raised
by Gregor and Blais, several problems with their own onomastic datasets are
also discussed.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.21150v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Detection of collective and point anomalies at the presence of trend and
  seasonality</h3>
                    <p><strong>Authors:</strong> Yiyin Zhang, Florian Pein, Idris Eckley</p>
                    <p>  Detecting anomalies in time series data is a challenging task with broad
relevance in many applications. Existing methods work effectively only under
idealized conditions, typically focusing on point anomalies or assuming a
constant baseline. Our approach overcomes these limitations by detecting both
collective and point anomalies, while allowing for polynomial trends and
seasonal patterns. We establish statistical theory demonstrating that our
method accurately decomposes the time series into anomaly, trend, seasonality,
and a remainder component. We further show that it estimates the number of
anomalies consistently and their locations with minimal error. Simulation
studies confirm its strong detection performance with finite samples, and an
application to energy price data illustrates its practical utility. An R
package is available on request.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.21128v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>DESA: An R Package for Detecting Epidemics using a School-Absenteeism
  Surveillance Framework</h3>
                    <p><strong>Authors:</strong> Vinay Joshy, Zeny Feng, Lorna Deeth, Kayla Vanderkruk, Justin Slater</p>
                    <p>  Absenteeism of elementary school children has been shown to be effective in
the early detection of an incoming influenza epidemic within a given
population. This paper introduces DESA, an R package designed to: 1) model an
epidemic using school absenteeism data, 2) raise an alert for an incoming
epidemic using school absenteeism data, 3) evaluate the timeliness of the
raised alert using different metrics, and 4) simulate community-level household
populations, epidemics, and school absenteeism to facilitate research in
related fields. This paper provides an overview of the functions in the package
and demonstrates its complete workflow using simulated data generated within
the package. DESA offers researchers and public health officials a tool for
improving early detection of seasonal influenza epidemics or epidemics of other
diseases. The package is available on CRAN, making it readily accessible to the
R user community.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.20943v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A State Model for the Analysis of Stem Cell Proliferation and
  Differentiation</h3>
                    <p><strong>Authors:</strong> Haim Bar, Huyen Nguyen, Joanne Conover</p>
                    <p>  Stem cells are characterized by their ability to self-renew, as well as to
differentiate and give rise to new populations of cells. Stem cell divisions
are crucial for generative processes that occur during early development, and
later in adulthood to support tissue regenerative capabilities. This property
of stemness, the ability of self-renewal or tissue-specific differentiation, is
also observed in cancer cells facilitating the sustenance of tumor growth, and
in bipotent megakaryocytic-erythroid progenitors (MEPs) to produce blood cells.
We are interested in modeling the size of the stem cell population required to
adequately generate tissues or colonies of cells. We develop a state model that
characterizes stem cell divisions and the dynamic changes of the stem cell and
differentiated cell populations. In our model, the probabilities of
self-renewal and differentiation events that stem cells undergo can vary over
time instead of remaining constant throughout the process. We provide an
estimation method for the division probabilities and using a simulation study,
we show that our method provides good estimates even with a small sample size.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.20832v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Towards Enhancing Data Equity in Public Health Data Science</h3>
                    <p><strong>Authors:</strong> Yiran Wang, Alicia E. Boyd, Lillian Rountree, Yi Ren, Kate Nyhan, Ruchit Nagar, Jackson Higginbottom, Megan L. Ranney, Harsh Parikh, Bhramar Mukherjee</p>
                    <p>  Data-driven decisions shape public health policies and practice, yet
persistent disparities in data representation skew insights and undermine
interventions. To address this, we advance a structured roadmap that integrates
public health data science with computer science and is grounded in
reflexivity. We adopt data equity as a guiding concept: ensuring the fair and
inclusive representation, collection, and use of data to prevent the
introduction or exacerbation of systemic biases that could lead to invalid
downstream inference and decisions. To underscore urgency, we present three
public health cases where non-representative datasets and skewed knowledge
impede decisions across diverse subgroups. These challenges echo themes in two
literatures: public health highlights gaps in high-quality data for specific
populations, while computer science and statistics contribute criteria and
metrics for diagnosing bias in data and models. Building on these foundations,
we propose a working definition of public health data equity and a structured
self-audit framework. Our framework integrates core computational principles
(fairness, accountability, transparency, ethics, privacy, confidentiality) with
key public health considerations (selection bias, representativeness,
generalizability, causality, information bias) to guide equitable practice
across the data life cycle, from study design and data collection to
measurement, analysis, interpretation, and translation. Embedding data equity
in routine practice offers a practical path for ensuring that data-driven
policies, artificial intelligence, and emerging technologies improve health
outcomes for all. Finally, we emphasize the critical understanding that,
although data equity is an essential first step, it does not inherently
guarantee information, learning, or decision equity.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.20301v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Hierarchical Bayesian model updating using Dirichlet process mixtures
  for structural damage localization</h3>
                    <p><strong>Authors:</strong> Taro Yaoyama, Tatsuya Itoi, Jun Iyama</p>
                    <p>  Bayesian model updating provides a rigorous probabilistic framework for
calibrating finite element (FE) models with quantified uncertainties, thereby
enhancing damage assessment, response prediction, and performance evaluation of
engineering structures. Recent advances in hierarchical Bayesian model updating
(HBMU) enable robust parameter estimation under ill-posed/ill-conditioned
settings and in the presence of inherent variability in structural parameters
due to environmental and operational conditions. However, most HBMU approaches
overlook multimodality in structural parameters that often arises when a
structure experiences multiple damage states over its service life. This paper
presents an HBMU framework that employs a Dirichlet process (DP) mixture prior
on structural parameters (DP-HBMU). DP mixtures are nonparametric Bayesian
models that perform clustering without pre-specifying the number of clusters,
incorporating damage state classification into FE model updating. We formulate
the DP-HBMU framework and devise a Metropolis-within-Gibbs sampler that draws
samples from the posterior by embedding Metropolis updates for intractable
conditionals due to the FE simulator. The applicability of DP-HBMU to damage
localization is demonstrated through both numerical and experimental examples.
We consider moment-resisting frame structures with beam-end fractures and apply
the method to datasets spanning multiple damage states, from an intact state to
moderate or severe damage state. The clusters inferred by DP-HBMU align closely
with the assumed or observed damage states. The posterior distributions of
stiffness parameters agree with ground truth values or observed fractures while
exhibiting substantially reduced uncertainty relative to a non-hierarchical
baseline. These results demonstrate the effectiveness of the proposed method in
damage localization.
</p>
                    <p><a href="http://arxiv.org/pdf/2508.19753v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>