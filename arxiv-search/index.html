<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 2/13/2025, 12:08:51 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Increasing competitiveness by imbalanced groups: The example of the
  48-team FIFA World Cup</h3>
                    <p><strong>Authors:</strong> László Csató, András Gyimesi</p>
                    <p>  A match played in a sports tournament can be called stakeless if at least one
team is indifferent to its outcome because it already has qualified or has been
eliminated. Such a game threatens fairness since teams may not exert full
effort without incentives. This paper suggests a novel classification for
stakeless matches according to their expected outcome: they are more costly if
the indifferent team is more likely to win by playing honestly. Our approach is
illustrated with the 2026 FIFA World Cup, the first edition of the competition
with 48 teams. We propose a novel format based on imbalanced groups, which
drastically reduces the probability of stakeless matches played by the
strongest teams according to Monte Carlo simulations. The new design also
increases the uncertainty of match outcomes and requires fewer matches.
Governing bodies in sports are encouraged to consider our innovative idea in
order to enhance the competitiveness of their tournaments.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08565v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A comparison of Dirichlet kernel regression methods on the simplex</h3>
                    <p><strong>Authors:</strong> Hanen Daayeb, Christian Genest, Salah Khardani, Nicolas Klutchnikoff, Frédéric Ouimet</p>
                    <p>  An asymmetric Dirichlet kernel version of the Gasser-M\"uller estimator is
introduced for regression surfaces on the simplex, extending the univariate
analog proposed by Chen [Statist. Sinica, 10(1) (2000), pp. 73-91]. Its
asymptotic properties are investigated under the condition that the design
points are known and fixed, including an analysis of its mean integrated
squared error (MISE) and its asymptotic normality. The estimator is also
applicable in a random design setting. A simulation study compares its
performance with two recently proposed alternatives: the Nadaraya--Watson
estimator with Dirichlet kernel and the local linear smoother with Dirichlet
kernel. The results show that the local linear smoother consistently
outperforms the others. To illustrate its applicability, the local linear
smoother is applied to the GEMAS dataset to analyze the relationship between
soil composition and pH levels across various agricultural and grazing lands in
Europe.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08461v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Tutorial for Surrogate Endpoint Validation Using Joint modeling and
  Mediation Analysis</h3>
                    <p><strong>Authors:</strong> Quentin Le Coent, Virginie Rondeau, Catherine Legrand</p>
                    <p>  The use of valid surrogate endpoints is an important stake in clinical
research to help reduce both the duration and cost of a clinical trial and
speed up the evaluation of interesting treatments. Several methods have been
proposed in the statistical literature to validate putative surrogate
endpoints. Two main approaches have been proposed: the meta-analytic approach
and the mediation analysis approach. The former uses data from meta-analyses to
derive associations measures between the surrogate and the final endpoint at
the individual and trial levels. The latter rather uses the proportion of the
treatment effect on the final endpoint through the surrogate as a measure of
surrogacy in a causal inference framework. Both approaches have remained
separated as the meta-analytic approach does not estimate the treatment effect
on the final endpoint through the surrogate while the mediation analysis
approach have been limited to single-trial setting. However, these two
approaches are complementary. In this work we propose an approach that combines
the meta-analytic and mediation analysis approaches using joint modeling for
surrogate validation. We focus on the cases where the final endpoint is a
time-to-event endpoint (such as time-to-death) and the surrogate is either a
time-to-event or a longitudinal biomarker. Two new joint models were proposed
depending on the nature of the surrogate. These model are implemented in the R
package frailtypack. We illustrate the developed approaches in three
applications on real datasets in oncology.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08443v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Scenario analysis with multivariate Bayesian machine learning models</h3>
                    <p><strong>Authors:</strong> Michael Pfarrhofer, Anna Stelzer</p>
                    <p>  We present an econometric framework that adapts tools for scenario analysis,
such as conditional forecasts and generalized impulse response functions, for
use with dynamic nonparametric multivariate models. We demonstrate the utility
of this approach through an exercise with simulated data, and three real-world
applications: (i) scenario-based conditional forecasts aligned with Federal
Reserve Bank stress test assumptions, (ii) measuring macroeconomic risk under
varying financial conditions, and (iii) the asymmetric effects of US-based
financial shocks and their international spillovers. Our results indicate the
importance of nonlinearities and asymmetries in the dynamic relationship
between macroeconomic and financial variables.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08440v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Debiasing physico-chemical models in air quality monitoring by combining
  different pollutant concentration measures</h3>
                    <p><strong>Authors:</strong> Benjamin Auder, Camille Coron, Jean-Michel Poggi, Emma Thulliez</p>
                    <p>  Air quality monitoring requires to produce accurate estimation of nitrogen
dioxide or fine particulate matter concentration maps, at different moments. A
typical strategy is to combine different types of data. On the one hand,
concentration maps produced by deterministic physicochemical models at urban
scale, and on the other hand, concentration measures made at different points,
different moments, and by different devices. These measures are provided first
by a small number of reference stations, which give reliable measurements of
the concentration, and second by a larger number of micro-sensors, which give
biased and noisier measurements. The proposed approach consists in modeling the
bias of the physicochemical model and estimating the parameters of this bias
using all the available concentration measures. Our model relies on a partition
of the geographical space of interest into different zones within which the
bias is assumed to be modeled by a single affine transformation of the actual
concentration. Our approach allows to improve the concentration maps provided
by the deterministic models but also to understand the behavior of
micro-sensors and their contribution in improving air quality monitoring. We
introduce the model, detail its implementation and experiment it through
numerical results using datasets collected in Grenoble (France).
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08252v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Trend-encoded Probabilistic Multi-order Model: A Non-Machine Learning
  Approach for Enhanced Stock Market Forecasts</h3>
                    <p><strong>Authors:</strong> Peiwan Wang, Chenhao Cui, Yong Li</p>
                    <p>  In recent years, the dominance of machine learning in stock market
forecasting has been evident. While these models have shown decreasing
prediction errors, their robustness across different datasets has been a
concern. A successful stock market prediction model minimizes prediction errors
and showcases robustness across various data sets, indicating superior
forecasting performance. This study introduces a novel multiple lag order
probabilistic model based on trend encoding (TeMoP) that enhances stock market
predictions through a probabilistic approach. Results across different stock
indexes from nine countries demonstrate that the TeMoP outperforms the
state-of-the-art machine learning models in predicting accuracy and
stabilization.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08144v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Cost Effectiveness Analyses for Sequential Multiple Assignment
  Randomized Trials</h3>
                    <p><strong>Authors:</strong> Lina M. Montoya, Elvin H. Geng, Harriet F. Adhiambo, Eliud Akama, Starley B. Shade, Assurah Elly, Thomas Odeny, Maya L. Petersen</p>
                    <p>  Sequential multiple assignment randomized trials (SMARTs) have grown in
popularity in recent years, and many of their study protocols propose
conducting a cost effectiveness analysis of the adaptive strategies embedded
within them. The cost effectiveness of these regimes is often proposed to be
assessed using incremental cost effectiveness ratios (ICERs). In this paper, we
present an estimation and inference procedure for such cost effectiveness
measures for the embedded dynamic treatment regimes within a SMART design. In
particular, we describe a targeted maximum likelihood estimator for the ICER of
a SMART's embedded regimes with influence curve-based inference. We illustrate
the performance of these methods using simulations. Throughout, we use as
illustration a cost effectiveness analysis for the Adaptive Strategies for
Preventing and Treating Lapses of Retention in HIV Care (ADAPT-R; NCT02338739)
trial, presenting estimated ICERs (with inference) for embedded regimes aimed
at increasing HIV care adherence. This manuscript is one of the first to
present cost effectiveness analysis results from a SMART.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07973v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Bad estimation, good prediction: the Lasso in dense regimes</h3>
                    <p><strong>Authors:</strong> Andrea Bratsberg, Magne Thoresen, Jelle J. Goeman</p>
                    <p>  For high-dimensional omics data, sparsity-inducing regularization methods
such as the Lasso are widely used and often yield strong predictive
performance, even in settings when the assumption of sparsity is likely
violated. We demonstrate that under a specific dense model, namely the
high-dimensional joint latent variable model, the Lasso produces sparse
prediction rules with favorable prediction error bounds, even when the
underlying regression coefficient vector is not sparse at all. We further argue
that this model better represents many types of omics data than sparse linear
regression models. We prove that the prediction bound under this model in fact
decreases with increasing number of predictors, and confirm this through
simulation examples. These results highlight the need for caution when
interpreting sparse prediction rules, as strong prediction accuracy of a sparse
prediction rule may not imply underlying biological significance of the
individual predictors.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07959v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Filtered Markovian Projection: Dimensionality Reduction in Filtering for
  Stochastic Reaction Networks</h3>
                    <p><strong>Authors:</strong> Chiheb Ben Hammouda, Maksim Chupin, Sophia Münker, Raúl Tempone</p>
                    <p>  Stochastic reaction networks (SRNs) model stochastic effects for various
applications, including intracellular chemical or biological processes and
epidemiology. A typical challenge in practical problems modeled by SRNs is that
only a few state variables can be dynamically observed. Given the measurement
trajectories, one can estimate the conditional probability distribution of
unobserved (hidden) state variables by solving a stochastic filtering problem.
In this setting, the conditional distribution evolves over time according to an
extensive or potentially infinite-dimensional system of coupled ordinary
differential equations with jumps, known as the filtering equation. The current
numerical filtering techniques, such as the Filtered Finite State Projection
(DAmbrosio et al., 2022), are hindered by the curse of dimensionality,
significantly affecting their computational performance. To address these
limitations, we propose to use a dimensionality reduction technique based on
the Markovian projection (MP), initially introduced for forward problems (Ben
Hammouda et al., 2024). In this work, we explore how to adapt the existing MP
approach to the filtering problem and introduce a novel version of the MP, the
Filtered MP, that guarantees the consistency of the resulting estimator. The
novel method combines a particle filter with reduced variance and solving the
filtering equations in a low-dimensional space, exploiting the advantages of
both approaches. The analysis and empirical results highlight the superior
computational efficiency of projection methods compared to the existing
filtered finite state projection in the large dimensional setting.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07918v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Nonparametric and Functional Wombling Methodology</h3>
                    <p><strong>Authors:</strong> Luke A. Barratt, John A. D. Aston</p>
                    <p>  Wombling methods, first introduced in 1951, have been widely applied to
detect boundaries and variations across spatial domains, particularly in
biological, public health and meteorological studies. Traditional applications
focus on finite-dimensional observations, where significant changes in
measurable traits indicate structural boundaries. In this work, wombling
methodologies are extended to functional data, enabling the identification of
spatial variation in infinite-dimensional settings. Proposed is a nonparametric
approach that accommodates functional observations without imposing strict
distributional assumptions. This methodology successfully captures complex
spatial structures and discontinuities, demonstrating superior sensitivity and
robustness compared to existing finite-dimensional techniques. This methodology
is then applied to analyse regional epidemiological disparities between London
and the rest of the UK, identifying key spatial boundaries in the shape of the
first trajectory of Covid-19 incidence in 2020. Through extensive simulations
and empirical validation, demonstrated is the method's effectiveness in
uncovering meaningful spatial variations, with potential applications in a wide
variety of fields.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.07740v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>