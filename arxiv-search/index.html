<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 7/21/2025, 1:32:14 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>On the importance of tail assumptions in climate extreme event
  attribution</h3>
                    <p><strong>Authors:</strong> Mengran Li, Daniela Castro-Camilo</p>
                    <p>  Extreme weather events are becoming more frequent and intense, posing serious
threats to human life, biodiversity, and ecosystems. A key objective of extreme
event attribution (EEA) is to assess whether and to what extent anthropogenic
climate change influences such events. Central to EEA is the accurate
statistical characterization of atmospheric extremes, which are inherently
multivariate or spatial due to their measurement over high-dimensional grids.
Within the counterfactual causal inference framework of Pearl, we evaluate how
tail assumptions affect attribution conclusions by comparing three multivariate
modeling approaches for estimating causation metrics. These include: (i) the
multivariate generalized Pareto distribution, which imposes an invariant tail
dependence structure; (ii) the factor copula model of Castro-Camilo and Huser
(2020), which offers flexible subasymptotic behavior; and (iii) the model of
Huser and Wadsworth (2019), which smoothly transitions between different forms
of extremal dependence. We assess the implications of these modeling choices in
both simulated scenarios (under varying forms of model misspecification) and
real data applications, using weekly winter maxima over Europe from the
M\'et\'eo-France CNRM model and daily precipitation from the ACCESS-CM2 model
over the U.S. Our findings highlight that tail assumptions critically shape
causality metrics in EEA. Misspecification of the extremal dependence structure
can lead to substantially different and potentially misleading attribution
conclusions, underscoring the need for careful model selection and evaluation
when quantifying the influence of climate change on extreme events.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.14019v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Using off-treatment sequential multiple imputation for binary outcomes
  to address intercurrent events handled by a treatment policy strategy</h3>
                    <p><strong>Authors:</strong> Sunita Rehal, Nicky Best, Sarah Watts, Thomas Drury</p>
                    <p>  The estimand framework proposes different strategies to address intercurrent
events. The treatment policy strategy seems to be the most favoured as it is
closely aligned with the pre-addendum intention-to-treat principle. All data
for all patients should ideally be collected, however, in reality patients may
withdraw from a study leading to missing data. This needs to be dealt with as
part of the estimation. Several areas of research have been conducted exploring
models to estimate the estimand when intercurrent events are handled using a
treatment policy strategy, however the research is limited for binary
endpoints. We explore different retrieved dropout models, where
post-intercurrent event, the observed data can be used to multiply impute the
missing post-intercurrent event data. We compare our proposed models to a
simple imputation model that makes no distinction between the pre- and
post-intercurrent event data, and assess varying statistical properties through
a simulation study. We then provide an example how retrieved dropout models
were used in practice for Phase 3 clinical trials in rheumatoid arthritis. From
the models explored, we conclude that a simple retrieved dropout model
including an indicator for whether or not the intercurrent event occurred is
the most pragmatic choice. However, at least 50% of observed post-intercurrent
event data is required for these models to work well. Therefore, the
suitability of implementing this model in practice will depend on the amount of
observed post-intercurrent event data available and missing data.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.14006v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Intellectual Up-streams of Percentage Scale ($ps$) and Percentage
  Coefficient ($b_p$) -- Effect Size Analysis (Theory Paper 2)</h3>
                    <p><strong>Authors:</strong> Xinshu Zhao, Qinru Ruby Ju, Piper Liping Liu, Dianshi Moses Li, Luxi Zhang, Jizhou Francis Ye, Song Harris Ao, Ming Milano Li</p>
                    <p>  Percentage thinking, i.e., assessing quantities as parts per hundred, has
traveled widely from Roman tax ledgers to modern algorithms. Building on early
decimalization by Simon Stevin in La Thiende (1585) and the 19th-century
metrication movement that institutionalized base-10 measurement worldwide
(Cajori, 1925), this article traces the intellectual trails through which
base-10 normalization, especially 0~1 percentage scale. We discuss
commonalities between those Wisconsin-Carolina experiments and classic indices,
especially the plus minus 1 Pearson (1895) correlation (r) and 0~1 coefficient
of determination, aka r squared (Wright, 1920). We pay tribute to the
influential percent of maximum possible (POMP) coefficient by Cohen et al.
(1999). The history of the 0~100 or 0~1 scales goes back far and wide. Roman
fiscal records, early American grading experiments at Yale and Harvard, and
contemporary analysis of percent scales (0~100) and percentage scales (0~1, or
-1~1) show the tendency to rediscover the scales and the indices based on the
scales (Durm, 1993; Schneider & and Hutt, 2014). Data mining and machine
learning since the last century adopted the same logic: min-max normalization,
which maps any feature to [0, 1] (i.e., 0-100%), equalizing the scale ranges.
Because 0~1 percentage scale assigns the entire scale to be the unit,
equalizing the scales also equalizes the units of all percentized scales.
Equitable units are necessary and sufficient for comparability of two indices,
according to the percentage theory of measurement indices (Cohen et al., 1999;
Zhao et al., 2024; Zhao & Zhang, 2014). Thus, the success of modern AI serves
as a large scale test confirming the comparability of percentage-based indices,
foremost among them the percentage coefficient ($b_p$).
</p>
                    <p><a href="http://arxiv.org/pdf/2507.13695v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A mixture distribution approach for assessing genetic impact from twin
  study</h3>
                    <p><strong>Authors:</strong> Zonghui Hu, Pengfei Li, Dean Follmann, Jing Qin</p>
                    <p>  This work was motivated by a twin study with the goal of assessing the
genetic control of immune traits. We propose a mixture bivariate distribution
to model twin data where the underlying order within a pair is unclear. Though
estimation from mixture distribution is usually subject to low convergence
rate, the combined likelihood, which is constructed over monozygotic and
dizygotic twins combined, reaches root-n consistency and allows effective
statistical inference on the genetic impact. The method is applicable to
general unordered pairs.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.13605v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>mNARX+: A surrogate model for complex dynamical systems using
  manifold-NARX and automatic feature selection</h3>
                    <p><strong>Authors:</strong> S. Schär, S. Marelli, B. Sudret</p>
                    <p>  We propose an automatic approach for manifold nonlinear autoregressive with
exogenous inputs (mNARX) modeling that leverages the feature-based structure of
functional-NARX (F-NARX) modeling. This novel approach, termed mNARX+,
preserves the key strength of the mNARX framework, which is its expressivity
allowing it to model complex dynamical systems, while simultaneously addressing
a key limitation: the heavy reliance on domain expertise to identify relevant
auxiliary quantities and their causal ordering. Our method employs a
data-driven, recursive algorithm that automates the construction of the mNARX
model sequence. It operates by sequentially selecting temporal features based
on their correlation with the model prediction residuals, thereby automatically
identifying the most critical auxiliary quantities and the order in which they
should be modeled. This procedure significantly reduces the need for prior
system knowledge. We demonstrate the effectiveness of the mNARX+ algorithm on
two case studies: a Bouc-Wen oscillator with strong hysteresis and a complex
aero-servo-elastic wind turbine simulator. The results show that the algorithm
provides a systematic, data-driven method for creating accurate and stable
surrogate models for complex dynamical systems.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.13301v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Short-term CO2 emissions forecasting: insight from the Italian
  electricity market</h3>
                    <p><strong>Authors:</strong> Pierdomenico Duttilo, Francesco Lisi</p>
                    <p>  This study investigates the short-term forecasting of carbon emissions from
electricity generation in the Italian power market. Using hourly data from 2021
to 2023, several statistical models and forecast combination methods are
evaluated and compared at the national and zonal levels. Four main model
classes are considered: (i) linear parametric models, such as seasonal
autoregressive integrated moving average and its exogenousvariable extension;
(ii) functional parametric models, including seasonal functional autoregressive
models, with and without exogenous variables; (iii) (semi) non-parametric and
possibly non-linear models, notably the generalised additive model (GAM) and
TBATS (trigonometric seasonality, Box-Cox transformation, ARMA errors, trend,
and seasonality); and (iv) a semi-functional approach based on the K-nearest
neighbours. Forecast combinations are also considered including simple
averaging, the optimal Bates and Granger weighting scheme, and a
selection-based strategy that chooses the best model for each hour. The overall
findings indicate that GAM reports the most accurate forecasts during the
daytime hours, while functional parametric models perform best during the early
morning period. GAM can also be considered the best individual model according
to the hourly average root mean square error and the Diebold-Mariano (DM) test.
Among the combination methods, the selection-based approach consistently
outperforms all individual models and forecast combinations, resulting in a
substantial reduction in the root mean square error compared to single models
and a primary choice for the DM test. These findings underline the value of
hybrid forecasting frameworks in improving the accuracy and reliability of
short-term carbon emissions predictions in power systems.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.12992v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Bayesian Spatio-Temporal Model of Temperature- and Humidity-Related
  Mortality Using High-Resolution Climate Data</h3>
                    <p><strong>Authors:</strong> Corinna Perchtold, Julia Eisenberg, Philipp Otto</p>
                    <p>  In this study, we introduce a novel and comprehensive extension of a Bayesian
spatio-temporal disease mapping model that explicitly accounts for
gender-specific effects of meteorological exposures. Leveraging fine-scale
weekly mortality and high-resolution climate data from Austria (2002 to 2019),
we assess how interactions between temperature, humidity, age, and gender
influence mortality patterns. Our approach goes beyond conventional modelling
by capturing complex dependencies through structured interactions across
space-time, space-age, and age-time dimensions, allowing us to capture complex
demographic and environmental dependencies. The analysis identifies
district-level mortality patterns and quantifies climate-related risks on a
weekly basis, offering new insights for public health surveillance.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.12643v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Ranking Vectors Clustering: Theory and Applications</h3>
                    <p><strong>Authors:</strong> Ali Fattahi, Ali Eshragh, Babak Aslani, Meysam Rabiee</p>
                    <p>  We study the problem of clustering ranking vectors, where each vector
represents preferences as an ordered list of distinct integers. Specifically,
we focus on the k-centroids ranking vectors clustering problem (KRC), which
aims to partition a set of ranking vectors into k clusters and identify the
centroid of each cluster. Unlike classical k-means clustering (KMC), KRC
constrains both the observations and centroids to be ranking vectors. We
establish the NP-hardness of KRC and characterize its feasible set. For the
single-cluster case, we derive a closed-form analytical solution for the
optimal centroid, which can be computed in linear time. To address the
computational challenges of KRC, we develop an efficient approximation
algorithm, KRCA, which iteratively refines initial solutions from KMC, referred
to as the baseline solution. Additionally, we introduce a branch-and-bound
(BnB) algorithm for efficient cluster reconstruction within KRCA, leveraging a
decision tree framework to reduce computational time while incorporating a
controlling parameter to balance solution quality and efficiency. We establish
theoretical error bounds for KRCA and BnB. Through extensive numerical
experiments on synthetic and real-world datasets, we demonstrate that KRCA
consistently outperforms baseline solutions, delivering significant
improvements in solution quality with fast computational times. This work
highlights the practical significance of KRC for personalization and
large-scale decision making, offering methodological advancements and insights
that can be built upon in future studies.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.12583v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Hierarchical Temporal Point Process Modeling of Aggressive Behavior
  Onset in Psychiatric Inpatient Youth with Autism for Branching Factor
  Estimation</h3>
                    <p><strong>Authors:</strong> Michael Potter, Michael Everett, Deniz Erdogmus, Yuna Watanabe, Tales Imbiriba, Matthew S. Goodwin</p>
                    <p>  Aggressive behavior in autistic inpatient youth often arises in temporally
clustered bursts complicating efforts to distinguish external triggers from
internal escalation. The sample population branching factor-the expected number
of new onsets triggered by a given event-is a key summary of self-excitation in
behavior dynamics. Prior pooled models overestimate this quantity by ignoring
patient-specific variability. We addressed this using a hierarchical Hawkes
process with an exponential kernel and edge-effect correction allowing partial
pooling across patients. This approach reduces bias from high-frequency
individuals and stabilizes estimates for those with sparse data. Bayesian
inference was performed using the No U-Turn Sampler with model evaluation via
convergence diagnostics, power-scaling sensitivity analysis, and multiple
Goodness-of-Fit (GOF) metrics: PSIS-LOO the Lewis test with Durbin's
modification and residual analysis based on the Random Time Change Theorem
(RTCT). The hierarchical model yielded a significantly lower and more precise
branching factor estimate mean (0.742 +- 0.026) than the pooled model (0.899 +-
0.015) and narrower intervals than the unpooled model (0.717 +- 0.139). This
led to a threefold smaller cascade of events per onset under the hierarchical
model. Sensitivity analyses confirmed robustness to prior and likelihood
perturbations while the unpooled model showed instability for sparse
individuals. GOF measures consistently favored or on par to the hierarchical
model. Hierarchical Hawkes modeling with edge-effect correction provides robust
estimation of branching dynamics by capturing both within- and between-patient
variability. This enables clearer separation of endogenous from exogenous
events supports linkage to physiological signals and enhances early warning
systems individualized treatment and resource allocation in inpatient care.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.12424v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Surrogate modeling for uncertainty quantification in nonlinear dynamics</h3>
                    <p><strong>Authors:</strong> S. Marelli, S. Schär, B. Sudret</p>
                    <p>  Predicting the behavior of complex systems in engineering often involves
significant uncertainty about operating conditions, such as external loads,
environmental effects, and manufacturing variability. As a result, uncertainty
quantification (UQ) has become a critical tool in modeling-based engineering,
providing methods to identify, characterize, and propagate uncertainty through
computational models. However, the stochastic nature of UQ typically requires
numerous evaluations of these models, which can be computationally expensive
and limit the scope of feasible analyses. To address this, surrogate models,
i.e., efficient functional approximations trained on a limited set of
simulations, have become central in modern UQ practice. This book chapter
presents a concise review of surrogate modeling techniques for UQ, with a focus
on the particularly challenging task of capturing the full time-dependent
response of dynamical systems. It introduces a classification of time-dependent
problems based on the complexity of input excitation and discusses
corresponding surrogate approaches, including combinations of principal
component analysis with polynomial chaos expansions, time warping techniques,
and nonlinear autoregressive models with exogenous inputs (NARX models). Each
method is illustrated with simple application examples to clarify the
underlying ideas and practical use.
</p>
                    <p><a href="http://arxiv.org/pdf/2507.12358v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>