<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 1/26/2026, 12:36:17 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Probabilistic Graphical Models in Astronomy</h3>
                    <p><strong>Authors:</strong> Abigail Sheerin, Giuseppe Vinci</p>
                    <p>The field of astronomy is experiencing a data explosion driven by significant advances in observational instrumentation, and classical methods often fall short of addressing the complexity of modern astronomical datasets. Probabilistic graphical models offer powerful tools for uncovering the dependence structures and data-generating processes underlying a wide array of cosmic variables. By representing variables as nodes in a network, these models allow for the visualization and analysis of the intricate relationships that underpin theories of hierarchical structure formation within the universe. We highlight the value that graphical models bring to astronomical research by demonstrating their practical application to the study of exoplanets and host stars.</p>
                    <p><a href="https://arxiv.org/pdf/2601.16959v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Identifying heat-related diagnoses in emergency department visits among adults in Chicago: a heat-wide association study</h3>
                    <p><strong>Authors:</strong> Hyojung Jang, Peter M. Graffy, Benjamin W. Barrett, Daniel E. Horton, Jennifer L. Chan, Abel N. Kho</p>
                    <p>Extreme heat is an escalating public health concern. Although prior studies have examined heat-health associations, their reliance on restricted diagnoses and diagnostic categories misses or misclassifies heat-related illness. We conducted a heat-wide association study to identify acute-care diagnoses associated with extreme heat in Chicago, Illinois. Using 916,904 acute-care visits -- including emergency department and urgent care encounters -- among 372,140 adults across five healthcare systems from 2011-2023, we applied a two-stage analytic approach: quasi-Poisson regression to screen 1,803 diagnosis codes for heat-related risks, followed by distributed lag non-linear models in a time-stratified case-crossover design to refine the list of heat-related diagnoses and estimate same-day and short-term cumulative odds ratios of acute-care visits during extreme heat versus reference temperature. We observed same-day increases in visits for heat illness, volume depletion, hypotension, edema, acute kidney failure, and multiple injuries. By analyzing the full diagnostic spectrum of acute-care services, this study comprehensively characterizes heat-associated morbidity, reinforcing and advancing existing literature.</p>
                    <p><a href="https://arxiv.org/pdf/2601.16932v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Distributional Instruments: Identification and Estimation with Quantile Least Squares</h3>
                    <p><strong>Authors:</strong> Rowan Cherodian, Guy Tchuente</p>
                    <p>We study instrumental-variable designs where policy reforms strongly shift the distribution of an endogenous variable but only weakly move its mean. We formalize this by introducing distributional relevance: instruments may be purely distributional. Within a triangular model, distributional relevance suffices for nonparametric identification of average structural effects via a control function. We then propose Quantile Least Squares (Q-LS), which aggregates conditional quantiles of X given Z into an optimal mean-square predictor and uses this projection as an instrument in a linear IV estimator. We establish consistency, asymptotic normality, and the validity of standard 2SLS variance formulas, and we discuss regularization across quantiles. Monte Carlo designs show that Q-LS delivers well-centered estimates and near-correct size when mean-based 2SLS suffers from weak instruments. In Health and Retirement Study data, Q-LS exploits Medicare Part D-induced distributional shifts in out-of-pocket risk to sharpen estimates of its effects on depression.</p>
                    <p><a href="https://arxiv.org/pdf/2601.16865v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Stochastic Modeling and Resource Dimensioning of Multi-Cellular Edge Intelligent Systems</h3>
                    <p><strong>Authors:</strong> Jaume Anguera Peris, Joakim Jaldén</p>
                    <p>Edge intelligence enables AI inference at the network edge, co-located with or near the radio access network, rather than in centralized clouds or on mobile devices. It targets low-latency, resource-constrained applications with large data volumes, requiring tight integration of wireless access and on-site computing. Yet system performance and cost-efficiency hinge on joint pre-deployment dimensioning of radio and computational resources, especially under spatial and temporal uncertainty. Prior work largely emphasizes run-time allocation or relies on simplified models that decouple radio and computing, missing end-to-end correlations in large-scale deployments. This paper introduces a unified stochastic framework to dimension multi-cell edge-intelligent systems. We model network topology with Poisson point processes, capturing random user and base-station locations, inter-cell interference, distance-based fractional power control, and peak-power constraints. By combining this with queueing theory and empirical AI inference workload profiling, we derive tractable expressions for end-to-end offloading delay. These enable a non-convex joint optimization that minimizes deployment cost under statistical QoS guarantees, expressed through strict tail-latency and inference-accuracy constraints. We prove the problem decomposes into convex subproblems, yielding global optimality. Numerical results in noise- and interference-limited regimes identify cost-efficient design regions and configurations that cause under-utilization or user unfairness. Smaller cells reduce transmission delay but raise per-request computing cost due to weaker server multiplexing, whereas larger cells show the opposite trend. Densification reduces computational costs only when frequency reuse scales with base-station density; otherwise, sparser deployments improve fairness and efficiency in interference-limited settings.</p>
                    <p><a href="https://arxiv.org/pdf/2601.16848v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Spillovers and Co-movements in Multivariate Volatility: A Vector Multiplicative Error Model</h3>
                    <p><strong>Authors:</strong> Edoardo Otranto, Luca Scaffidi Domianello</p>
                    <p>Recent developments in financial time series focus on modeling volatility across multiple assets or indices in a multivariate framework, accounting for potential interactions such as spillover effects. Furthermore, the increasing integration of global financial markets provides a similar dynamics (referred to as comovement). In this context, we introduce a novel model for volatility vectors within the Multiplicative Error Model (MEM) class. This framework accommodates both spillover and co-movement effects through a distinct latent component. By adopting a specific parameterization, the model remains computationally feasible even for high-dimensional volatility vectors. To reduce the number of unknown coefficients, we propose a simple model-based clustering procedure. We illustrate the effectiveness of the proposed approach through an empirical application to 29 assets of the Dow Jones Industrial Average index, providing insight into volatility spillovers and shared market dynamics. Comparative analysis against alternative vector MEMs, including a fully parameterized version of the proposed model, demonstrates its superior or at least comparable performance across multiple evaluation criteria.</p>
                    <p><a href="https://arxiv.org/pdf/2601.16837v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Directional Asymmetry in Edge BasedSpatial Models via a Skew Normal Prior</h3>
                    <p><strong>Authors:</strong> Danna L. Cruz-Reyes, Renato M. Assunção, Reinaldo B. Arellano-Valle, Rosangela H. Loschi</p>
                    <p>We introduce a skewed edge based spatial prior, named RENeGe sk that extends the Gaussian RENeGe framework by incorporating directional asymmetry through a skew normal distribution. Skewness is defined on the edge graph and propagated to the node space, aligning asymmetric behavior with transitions across neighboring regions rather than with marginal node effects. The model is formulated within the skew normal framework and employs identifiable hierarchical priors together with low rank parameterizations to ensure scalability. The skew normal's stochastic representation is considered to facilitate the computational implementation. Simulation studies show that RENeGe sk recovers compact, edge-aligned directional structure more accurately than symmetric Gaussian priors, while remaining competitive under irregular spatial patterns. An application to cancer incidence data in Southern Brazil illustrates how the proposed approach yields stable area-level estimates while preserving localized, directionally driven spatial variation.</p>
                    <p><a href="https://arxiv.org/pdf/2601.16829v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Directional-Shift Dirichlet ARMA Models for Compositional Time Series with Structural Break Intervention</h3>
                    <p><strong>Authors:</strong> Harrison Katz</p>
                    <p>Compositional time series, vectors of proportions summing to unity observed over time, frequently exhibit structural breaks due to external shocks, policy changes, or market disruptions. Standard methods either ignore such breaks or handle them through ad-hoc dummy variables that cannot extrapolate beyond the estimation sample. We develop a Bayesian Dirichlet ARMA model augmented with a directional-shift intervention mechanism that captures structural breaks through three interpretable parameters: a unit direction vector specifying which components gain or lose share, an amplitude controlling the magnitude of redistribution, and a logistic gate governing the timing and speed of transition. The model preserves compositional constraints by construction, maintains innovation-form DARMA dynamics for short-run dependence, and produces coherent probabilistic forecasts during and after structural breaks. We establish that the directional shift corresponds to geodesic motion on the simplex and is invariant to the choice of ILR basis. A comprehensive simulation study with 400 fits across 8 scenarios demonstrates that when the shift direction is correctly identified (77.5% of cases), amplitude and timing parameters are recovered with near-zero bias, and credible intervals for the mean composition achieve nominal 80% coverage; we address the sign identification challenge through a hemisphere constraint. An empirical application to fee recognition lead-time distributions during COVID-19 compares baseline, fixed-effects, and intervention specifications in rolling forecast evaluation, demonstrating the intervention model's superior point accuracy (Aitchison distance 0.83 vs. 0.90) and calibration (87% vs. 71% coverage) during structural transitions.</p>
                    <p><a href="https://arxiv.org/pdf/2601.16821v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>LLM-powered Real-time Patent Citation Recommendation for Financial Technologies</h3>
                    <p><strong>Authors:</strong> Tianang Deng, Yu Deng, Tianchen Gao, Yonghong Hu, Rui Pan</p>
                    <p>Rapid financial innovation has been accompanied by a sharp increase in patenting activity, making timely and comprehensive prior-art discovery more difficult. This problem is especially evident in financial technologies, where innovations develop quickly, patent collections grow continuously, and citation recommendation systems must be updated as new applications arrive. Existing patent retrieval and citation recommendation methods typically rely on static indexes or periodic retraining, which limits their ability to operate effectively in such dynamic settings. In this study, we propose a real-time patent citation recommendation framework designed for large and fast-changing financial patent corpora. Using a dataset of 428,843 financial patents granted by the China National Intellectual Property Administration (CNIPA) between 2000 and 2024, we build a three-stage recommendation pipeline. The pipeline uses large language model (LLM) embeddings to represent the semantic content of patent abstracts, applies efficient approximate nearest-neighbor search to construct a manageable candidate set, and ranks candidates by semantic similarity to produce top-k citation recommendations. In addition to improving recommendation accuracy, the proposed framework directly addresses the dynamic nature of patent systems. By using an incremental indexing strategy based on hierarchical navigable small-world (HNSW) graphs, newly issued patents can be added without rebuilding the entire index. A rolling day-by-day update experiment shows that incremental updating improves recall while substantially reducing computational cost compared with rebuild-based indexing. The proposed method also consistently outperforms traditional text-based baselines and alternative nearest-neighbor retrieval approaches.</p>
                    <p><a href="https://arxiv.org/pdf/2601.16775v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>From Noisy News Sentiment Scores to Interpretable Temporal Dynamics: A Bayesian State-Space Model</h3>
                    <p><strong>Authors:</strong> Ian Carbó Casals</p>
                    <p>Text-based sentiment indicators are widely used to monitor public and market mood, but weekly sentiment series are noisy by construction. A main reason is that the amount of relevant news changes over time and across categories. As a result, some weekly averages are based on many articles, while others rely on only a few. Existing approaches do not explicitly account for changes in data availability when measuring uncertainty. We present a Bayesian state-space framework that turns aggregated news sentiment into a smoothed time series with uncertainty. The model treats each weekly sentiment value as a noisy measurement of an underlying sentiment process, with observation uncertainty scaled by the effective information weight $n_{tj}$: when coverage is high, latent sentiment is anchored more strongly to the observed aggregate; when coverage is low, inference relies more on the latent dynamics and uncertainty increases. Using news data grouped into multiple categories, we find broadly similar latent dynamics across categories, while larger differences appear in observation noise. The framework is designed for descriptive monitoring and can be extended to other text sources where information availability varies over time.</p>
                    <p><a href="https://arxiv.org/pdf/2601.16769v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>PanopMamba: Vision State Space Modeling for Nuclei Panoptic Segmentation</h3>
                    <p><strong>Authors:</strong> Ming Kang, Fung Fung Ting, Raphaël C. -W. Phan, Zongyuan Ge, Chee-Ming Ting</p>
                    <p>Nuclei panoptic segmentation supports cancer diagnostics by integrating both semantic and instance segmentation of different cell types to analyze overall tissue structure and individual nuclei in histopathology images. Major challenges include detecting small objects, handling ambiguous boundaries, and addressing class imbalance. To address these issues, we propose PanopMamba, a novel hybrid encoder-decoder architecture that integrates Mamba and Transformer with additional feature-enhanced fusion via state space modeling. We design a multiscale Mamba backbone and a State Space Model (SSM)-based fusion network to enable efficient long-range perception in pyramid features, thereby extending the pure encoder-decoder framework while facilitating information sharing across multiscale features of nuclei. The proposed SSM-based feature-enhanced fusion integrates pyramid feature networks and dynamic feature enhancement across different spatial scales, enhancing the feature representation of densely overlapping nuclei in both semantic and spatial dimensions. To the best of our knowledge, this is the first Mamba-based approach for panoptic segmentation. Additionally, we introduce alternative evaluation metrics, including image-level Panoptic Quality ($i$PQ), boundary-weighted PQ ($w$PQ), and frequency-weighted PQ ($fw$PQ), which are specifically designed to address the unique challenges of nuclei segmentation and thereby mitigate the potential bias inherent in vanilla PQ. Experimental evaluations on two multiclass nuclei segmentation benchmark datasets, MoNuSAC2020 and NuInsSeg, demonstrate the superiority of PanopMamba for nuclei panoptic segmentation over state-of-the-art methods. Consequently, the robustness of PanopMamba is validated across various metrics, while the distinctiveness of PQ variants is also demonstrated. Code is available at https://github.com/mkang315/PanopMamba.</p>
                    <p><a href="https://arxiv.org/pdf/2601.16631v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>