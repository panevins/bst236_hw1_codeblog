<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 12/22/2025, 12:29:28 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life</h3>
                    <p><strong>Authors:</strong> Corey M. Abramson</p>
                    <p>This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.</p>
                    <p><a href="https://arxiv.org/pdf/2512.17850v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Day-Ahead Electricity Price Forecasting Using Merit-Order Curves Time Series</h3>
                    <p><strong>Authors:</strong> Guillaume Koechlin, Filippo Bovera, Piercesare Secchi</p>
                    <p>We introduce a general, simple, and computationally efficient framework for predicting day-ahead supply and demand merit-order curves, from which both point and probabilistic electricity price forecasts can be derived. Specifically, we leverage functional principal component analysis to efficiently represent a pair of supply and demand curves in a low-dimensional vector space and employ regularized vector autoregressive models for their prediction. We conduct a rigorous empirical comparison of price forecasting performance between the proposed curve-based model, i.e., derived from predicted merit-order curves, and state-of-the-art price-based models that directly forecast the clearing price, using data from the Italian day-ahead market over the 2023-2024 period. Our results show that the proposed curve-based approach significantly improves both point and probabilistic price forecasting accuracy relative to price-based approaches, with average gains of approximately 5%, and improvements of up to 10% during mid-day hours, when prices occasionally drop due to high renewable generation and low demand.</p>
                    <p><a href="https://arxiv.org/pdf/2512.17758v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Perceptions of the Metaverse at the Peak of the Hype Cycle: A Cross-Sectional Study Among Turkish University Students</h3>
                    <p><strong>Authors:</strong> Mehmet Ali Erkan, Halil Eren Koçak</p>
                    <p>During the height of the hype in late 2021, the Metaverse drew more attention from around the world than ever before. It promised new ways to interact with people in three-dimensional digital spaces. This cross-sectional study investigates the attitudes, perceptions, and predictors of the willingness to engage with the Metaverse among 381 Turkish university students surveyed in December 2021. The study employs Fisher's Exact Tests and binary logistic regression to assess the influence of demographic characteristics, prior digital experience, and perception-based factors. The results demonstrate that demographic factors, such as gender, educational attainment, faculty association, social media engagement, and previous virtual reality exposure, do not significantly forecast the propensity to participate in the Metaverse. Instead, the main things that affect people's intentions to adopt are how they see things. Belief in the Metaverse's capacity to revolutionize societal frameworks, especially human rights, surfaced as the most significant positive predictor of willingness. Conversely, apprehensions regarding psychological harm, framed as a possible 'cyber syndrome' represented a significant obstacle to participation. Perceptions of technical compatibility and ethical considerations showed complex effects, showing that optimism, uncertainty, and indifference affect willingness in different ways. In general, the results show that early adoption of the Metaverse is based on how people see it, not on their demographics. The research establishes a historically informed benchmark of user skepticism and prudent assessment during the advent of Web 3.0, underscoring the necessity of addressing collective psychological, ethical, and normative issues to promote future engagement.</p>
                    <p><a href="https://arxiv.org/pdf/2512.17750v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Estimation and model errors in Gaussian-process-based Sensitivity Analysis of functional outputs</h3>
                    <p><strong>Authors:</strong> Yuri Taglieri Sáo, Olivier Roustant, Geraldo de Freitas Maciel</p>
                    <p>Global sensitivity analysis (GSA) of functional-output models is usually performed by combining statistical techniques, such as basis expansions, metamodeling and sampling based estimation of sensitivity indices. By neglecting truncation error from basis expansion, two main sources of errors propagate to the final sensitivity indices: the metamodeling related error and the sampling-based, or pick-freeze (PF), estimation error. This work provides an efficient algorithm to estimate these errors in the frame of Gaussian processes (GP), based on the approach of Le Gratiet et al. [16]. The proposed algorithm takes advantage of the fact that the number of basis coefficients of expanded model outputs is significantly smaller than output dimensions. Basis coefficients are fitted by GP models and multiple conditional GP trajectories are sampled. Then, vector-valued PF estimation is used to speed-up the estimation of Sobol indices and generalized sensitivity indices (GSI). We illustrate the methodology on an analytical test case and on an application in non-Newtonian hydraulics, modelling an idealized dam-break flow. Numerical tests show an improvement of 15 times in the computational time when compared to the application of Le Gratiet et al. [16] algorithm separately over each output dimension.</p>
                    <p><a href="https://arxiv.org/pdf/2512.17635v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis</h3>
                    <p><strong>Authors:</strong> Dishantkumar Sutariya, Eike Petersen</p>
                    <p>Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.</p>
                    <p><a href="https://arxiv.org/pdf/2512.17409v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease</h3>
                    <p><strong>Authors:</strong> Carter H. Nakamoto, Lucia Lushi Chen, Agata Foryciarz, Sherri Rose</p>
                    <p>Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.</p>
                    <p><a href="https://arxiv.org/pdf/2512.17340v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Uncovering latent territorial structure in ICFES Saber 11 performance with Bayesian multilevel spatial models</h3>
                    <p><strong>Authors:</strong> Laura Pardo, Juan Sosa</p>
                    <p>This article develops a Bayesian hierarchical framework to analyze academic performance in the 2022 second semester Saber 11 examination in Colombia. Our approach combines multilevel regression with municipal and departmental spatial random effects, and it incorporates Ridge and Lasso regularization priors to compare the contribution of sociodemographic covariates. Inference is implemented in a fully open source workflow using Markov chain Monte Carlo methods, and model behavior is assessed through synthetic data that mirror key features of the observed data. Simulation results indicate that Ridge provides the most balanced performance in parameter recovery, predictive accuracy, and sampling efficiency, while Lasso shows weaker fit and posterior stability, with gains in predictive accuracy under stronger multicollinearity. In the application, posterior rankings show a strong centralization of performance, with higher scores in central departments and lower scores in peripheral territories, and the strongest correlates of scores are student level living conditions, maternal education, access to educational resources, gender, and ethnic background, while spatial random effects capture residual regional disparities. A hybrid Bayesian segmentation based on K means propagates posterior uncertainty into clustering at departmental, municipal, and spatial scales, revealing multiscale territorial patterns consistent with structural inequalities and informing territorial targeting in education policy.</p>
                    <p><a href="https://arxiv.org/pdf/2512.17119v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Do Generalized=Gamma Scale Mixtures of Normals Fit Large Image Data-Sets?</h3>
                    <p><strong>Authors:</strong> Brandon Marks, Yash Dave, Zixun Wang, Hannah Chung, Riya Patwa, Simon Cha, Michael Murphy, Alexander Strang</p>
                    <p>A scale mixture of normals is a distribution formed by mixing a collection of normal distributions with fixed mean but different variances. A generalized gamma scale mixture draws the variances from a generalized gamma distribution. Generalized gamma scale mixtures of normals have been proposed as an attractive class of parametric priors for Bayesian inference in inverse imaging problems. Generalized gamma scale mixtures have two shape parameters, one that controls the behavior of the distribution about its mode, and the other that controls its tail decay. In this paper, we provide the first demonstration that the prior model is realistic for multiple large imaging data sets. We draw data from remote sensing, medical imaging, and image classification applications. We study the realism of the prior when applied to Fourier and wavelet (Haar and Gabor) transformations of the images, as well as to the coefficients produced by convolving the images against the filters used in the first layer of AlexNet, a popular convolutional neural network trained for image classification. We discuss data augmentation procedures that improve the fit of the model, procedures for identifying approximately exchangeable coefficients, and characterize the parameter regions that best describe the observed data sets. These regions are significantly broader than the region of primary focus in computational work. We show that this prior family provides a substantially better fit to each data set than any of the standard priors it contains. These include Gaussian, Laplace, $\ell_p$, and Student's $t$ priors. Finally, we identify cases where the prior is unrealistic and highlight characteristic features of images that suggest the model will fit poorly.</p>
                    <p><a href="https://arxiv.org/pdf/2512.17038v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Quantile-based causal inference for spatio-temporal processes: Assessing the impacts of wildfires on US air quality</h3>
                    <p><strong>Authors:</strong> Zipei Geng, Jordan Richards, Raphael Huser, Marc G. Genton</p>
                    <p>Wildfires pose an increasingly severe threat to air quality, yet quantifying their causal impact remains challenging due to unmeasured meteorological and geographic confounders. Moreover, wildfire impacts on air quality may exhibit heterogeneous effects across pollution levels, which conventional mean-based causal methods fail to capture. To address these challenges, we develop a Quantile-based Latent Spatial Confounder Model (QLSCM) that substitutes conditional expectations with conditional quantiles, enabling causal analysis across the entire outcome distribution. We establish the causal interpretation of QLSCM theoretically, prove the identifiability of causal effects, and demonstrate estimator consistency under mild conditions. Simulations confirm the bias correction capability and the advantage of quantile-based inference over mean-based approaches. Applying our method to contiguous US wildfire and air quality data, we uncover important heterogeneous effects: fire radiative power exerts significant positive causal effects on aerosol optical depth at high quantiles in Western states like California and Oregon, while insignificant at lower quantiles. This indicates that wildfire impacts on air quality primarily manifest during extreme pollution events. Regional analyses reveal that Western and Northwestern regions experience the strongest causal effects during such extremes. These findings provide critical insights for environmental policy by identifying where and when mitigation efforts would be most effective.</p>
                    <p><a href="https://arxiv.org/pdf/2512.16603v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Opening the House: Datasets for Mixed Doubles Curling</h3>
                    <p><strong>Authors:</strong> Robyn Ritchie, Alexandre Leblanc, Thomas Loughin</p>
                    <p>We introduce the most comprehensive publicly available datasets for mixed doubles curling, constructed from eleven top-level tournaments from the CurlIT (https://curlit.com/results) Results Booklets spanning 53 countries, 1,112 games, and nearly 70,000 recorded shots. While curling analytics has grown in recent years, mixed doubles remains under-served due to limited access to data. Using a combined text-scraping and image-processing pipeline, we extract and standardize detailed game- and shot-level information, including player statistics, hammer possession, Power Play usage, stone coordinates, and post-shot scoring states. We describe the data engineering workflow, highlight challenges in parsing historical records, and derive additional contextual features that enable rigorous strategic analysis. Using these datasets, we present initial insights into shot selection and success rates, scoring distributions, and team efficiencies, illustrating key differences between mixed doubles and traditional 4-player curling. We highlight various ways to analyze this type of data including from a shot-, end-, game- or team-level to display its versatilely. The resulting resources provide a foundation for advanced performance modeling, strategic evaluation, and future research in mixed doubles curling analytics, supporting broader analytical engagement with this rapidly growing discipline.</p>
                    <p><a href="https://arxiv.org/pdf/2512.16574v2" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>