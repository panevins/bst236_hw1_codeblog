<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 6/23/2025, 1:26:52 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction</h3>
                    <p><strong>Authors:</strong> Jiekai Ma, Yikai Zhao</p>
                    <p>  Large Language Models (LLMs) have recently enabled natural language
interfaces that translate user queries into executable SQL, offering a powerful
solution for non-technical stakeholders to access structured data. However, one
of the limitation that LLMs do not natively express uncertainty makes it
difficult to assess the reliability of their generated queries. This paper
presents a case study that evaluates multiple approaches to estimate confidence
scores for LLM-generated SQL in supply chain data retrieval. We investigated
three strategies: (1) translation-based consistency checks; (2) embedding-based
semantic similarity between user questions and generated SQL; and (3)
self-reported confidence scores directly produced by the LLM. Our findings
reveal that LLMs are often overconfident in their own outputs, which limits the
effectiveness of self-reported confidence. In contrast, embedding-based
similarity methods demonstrate strong discriminative power in identifying
inaccurate SQL.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.17203v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Walking Fingerprinting Using Wrist Accelerometry During Activities of
  Daily Living in NHANES</h3>
                    <p><strong>Authors:</strong> Lily Koffman, John Muschelli III, Ciprian Crainiceanu</p>
                    <p>  We propose a method for identifying individuals based on their continuously
monitored wrist-worn accelerometry during activities of daily living. The
method consists of three steps: (1) using Adaptive Empirical Pattern
Transformation (ADEPT), a highly specific method to identify walking; (2)
transforming the accelerometry time series into an image that corresponds to
the joint distribution of the time series and its lags; and (3) using the
resulting images to construct a person-specific walking fingerprint. The method
is applied to 15,000 individuals from the National Health and Nutrition
Examination Survey (NHANES) with up to 7 days of wrist accelerometry data
collected at 80 Hertz. The resulting dataset contains more than 10 terabytes,
is roughly 2 to 3 orders of magnitude larger than previous datasets used for
activity recognition, is collected in the free living environment, and does not
contain labels for walking periods. Using extensive cross-validation studies,
we show that our method is highly predictive and can be successfully extended
to a large, heterogeneous sample representative of the U.S. population: in the
highest-performing model, the correct participant is in the top 1% of
predictions 96% of the time.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.17160v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Skewness-Kurtosis: small samples and power-law behavior</h3>
                    <p><strong>Authors:</strong> Carlo De Michele, Samuele De Bartolo</p>
                    <p>  Skewness and kurtosis are fundamental statistical moments commonly used to
quantify asymmetry and tail behavior in probability distributions. Despite
their widespread application in statistical mechanics, condensed matter
physics, and complex systems, important aspects of their empirical behavior
remain unclear, particularly in small samples and in relation to their
hypothesized power law scaling. In this work, we address both issues using a
combination of empirical and synthetic data. First, we establish a lower bound
for sample kurtosis as a function of sample size and skewness. Second, we
examine the conditions under which the 4/3 power law relationship between
kurtosis and skewness emerges, effectively extending Taylor power law to higher
order moments. Our results show that this scaling behavior predominantly occurs
in data sampled from heavy tailed distributions and medium, large sample sizes.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.16906v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised
  Defect Detection on Low-Quality Industrial Images</h3>
                    <p><strong>Authors:</strong> Sebastian HÃ¶nel, Jonas Nordqvist</p>
                    <p>  The detection and localization of quality-related problems in industrially
mass-produced products has historically relied on manual inspection, which is
costly and error-prone. Machine learning has the potential to replace manual
handling. As such, the desire is to facilitate an unsupervised (or
self-supervised) approach, as it is often impossible to specify all conceivable
defects ahead of time. A plethora of prior works have demonstrated the aptitude
of common reconstruction-, embedding-, and synthesis-based methods in
laboratory settings. However, in practice, we observe that most methods do not
handle low data quality well or exude low robustness in unfavorable, but
typical real-world settings. For practitioners it may be very difficult to
identify the actual underlying problem when such methods underperform. Worse,
often-reported metrics (e.g., AUROC) are rarely suitable in practice and may
give misleading results. In our setting, we attempt to identify subtle
anomalies on the surface of blasted forged metal parts, using rather
low-quality RGB imagery only, which is a common industrial setting. We
specifically evaluate two types of state-of-the-art models that allow us to
identify and improve quality issues in production data, without having to
obtain new data. Our contribution is to provide guardrails for practitioners
that allow them to identify problems related to, e.g., (lack of) robustness or
invariance, in either the chosen model or the data reliably in similar
scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of
likelihood-based approaches and outline a framework for proper empirical risk
estimation that is more suitable for real-world scenarios.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.16890v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Quantifying Flow State Dynamics: A Prefrontal Cortex EEG-Based Model
  Validation Study. Unveiling the Prefrontal Cortex's Role in Flow State
  Experience: An Empirical EEG Analysis</h3>
                    <p><strong>Authors:</strong> Gianluca Rosso, Raffaella Ricci, Lorenzo Pia, Giovanni Rebaudo, Michele Guindani, Alberto Marocchino, Giorgio De Pieri, Andrea Filippo Rosso</p>
                    <p>  This article aims to explore the optimization of mental performance through
the analysis of metrics associated with the psychological state known as flow.
Several clinical studies have shown a correlation between the mental state of
flow (characterized by deep and relaxed concentration and high psychophysical
efficiency) and brain activity measured through electroencephalography (EEG).
This study confirms such a correlation, focusing in particular on the sports
field, where the flow state tends to occur more frequently. To conduct the
study, Sporthype developed proprietary software that integrates several
predictive models, in particular the Flow State Index (FSI), implemented within
the Holytics system. An analytical protocol was established, including mental
exercises and data collection sessions using the portable EEG device Muse,
accompanied by a questionnaire to gather athletes' subjective perceptions of
their mental state. The results revealed a significant alignment between the
EEG data and the subjective experiences reported in the questionnaires,
confirming the feasibility of detecting the flow state through prefrontal
cortex activity. Furthermore, the psychological exercises included in the study
protocol showed a tangible positive effect in enhancing flow during athletic
performance. Flow improves performance through a more harmonious
synchronization between mind and body. Although golf was the main context of
the experimentation, the mathematical models developed within Holytics were
designed to be applicable to a wide range of sports. In addition to golf,
preliminary tests have been conducted in other sports such as tennis, as well
as in non-sport contexts, including gaming and mental training practices such
as mindfulness, concentration, and visualization.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.16838v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Self-Organized Criticality Model of Extreme Events and Cascading
  Disasters of Hub and Spoke Air Traffic Networks</h3>
                    <p><strong>Authors:</strong> Mary Lai O. SalvaÃ±a, Harold Jay M. Bolingot, Gregory L. Tangonan</p>
                    <p>  Critical infrastructure networks--including transportation, power grids, and
communication systems--exhibit complex interdependencies that can lead to
cascading failures with catastrophic consequences. These disasters often
originate from failures at critical points in the network, where single-node
disruptions can propagate rapidly due to structural dependencies and
high-impact linkages. Such vulnerabilities are exacerbated in systems that have
been highly optimized for efficiency or have self-organized into fragile
configurations over time. The U.S. air transportation system, built on a
hub-and-spoke model, exemplifies this type of critical infrastructure. Its
reliance on a small number of high-throughput hubs means that even localized
disruptions--especially those triggered by increasingly frequent and extreme
weather events--can initiate cascades with nationwide impact. We introduce a
novel application of Self-Organized Criticality (SOC) theory to model and
analyze cascading failures in such systems. Through a detailed case study of
U.S. airline operations, we show how the SOC model captures the power-law
distribution of disruptions and the long-tail risk of systemic failures,
reflecting the interplay between structural fragility and climate shocks. Our
approach enables quantitative assessment of network vulnerability,
identification of critical nodes, and evaluation of proactive strategies for
disaster risk reduction. The results demonstrate that the SOC model replicates
the observed statistical patterns--frequent small events and rare, severe
failures--offering a powerful systems-level framework for infrastructure
resilience planning and emergency response.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.16727v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Improving Outbreak Forecasts Through Model Augmentation</h3>
                    <p><strong>Authors:</strong> Graham C. Gibson, Spencer J. Fox, Emily Javan, Susan E. Ptak, Oluwasegun M. Ibrahim, Michael Lachmann, Lauren Ancel Meyers</p>
                    <p>  Accurate forecasts of disease outbreaks are critical for effective public
health responses, management of healthcare surge capacity, and communication of
public risk. There are a growing number of powerful forecasting methods that
fall into two broad categories -- empirical models that extrapolate from
historical data, and mechanistic models based on fixed epidemiological
assumptions. However, these methods often underperform precisely when reliable
predictions are most urgently needed -- during periods of rapid epidemic
escalation. Here, we introduce epimodulation, a hybrid approach that integrates
fundamental epidemiological principles into existing predictive models to
enhance forecasting accuracy, especially around epidemic peaks. When applied to
simple empirical forecasting methods (ARIMA, Holt--Winters, and spline models),
epimodulation improved overall prediction accuracy by an average of 9.1\%
(range: 8.2--12.5\%) for COVID-19 hospital admissions and by 19.5\% (range:
17.6--23.2\%) for influenza hospital admissions; accuracy during epidemic peaks
improved even further, by an average of 20.7\% and 25.4\%, respectively.
Epimodulation also substantially enhanced the performance of complex
forecasting methods, including the COVID-19 Forecast Hub ensemble model,
demonstrating its broad utility in improving forecast reliability at critical
moments in disease outbreaks.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.16410v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>AI labeling reduces the perceived accuracy of online content but has
  limited broader effects</h3>
                    <p><strong>Authors:</strong> Chuyao Wang, Patrick Sturgis, Daniel de Kadt</p>
                    <p>  Explicit labeling of online content produced by artificial intelligence (AI)
is a widely mooted policy for ensuring transparency and promoting public
confidence. Yet little is known about the scope of AI labeling effects on
public assessments of labeled content. We contribute new evidence on this
question from a survey experiment using a high-quality nationally
representative probability sample (n = 3,861). First, we demonstrate that
explicit AI labeling of a news article about a proposed public policy reduces
its perceived accuracy. Second, we test whether there are spillover effects in
terms of policy interest, policy support, and general concerns about online
misinformation. We find that AI labeling reduces interest in the policy, but
neither influences support for the policy nor triggers general concerns about
online misinformation. We further find that increasing the salience of AI use
reduces the negative impact of AI labeling on perceived accuracy, while
one-sided versus two-sided framing of the policy has no moderating effect.
Overall, our findings suggest that the effects of algorithm aversion induced by
AI labeling of online content are limited in scope.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.16202v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Intelligent Operation and Maintenance and Prediction Model Optimization
  for Improving Wind Power Generation Efficiency</h3>
                    <p><strong>Authors:</strong> Xun Liu, Xiaobin Wu, Jiaqi He, Rajan Das Gupta</p>
                    <p>  This study explores the effectiveness of predictive maintenance models and
the optimization of intelligent Operation and Maintenance (O&M) systems in
improving wind power generation efficiency. Through qualitative research,
structured interviews were conducted with five wind farm engineers and
maintenance managers, each with extensive experience in turbine operations.
Using thematic analysis, the study revealed that while predictive maintenance
models effectively reduce downtime by identifying major faults, they often
struggle with detecting smaller, gradual failures. Key challenges identified
include false positives, sensor malfunctions, and difficulties in integrating
new models with older turbine systems. Advanced technologies such as digital
twins, SCADA systems, and condition monitoring have significantly enhanced
turbine maintenance practices. However, these technologies still require
improvements, particularly in AI refinement and real-time data integration. The
findings emphasize the need for continuous development to fully optimize wind
turbine performance and support the broader adoption of renewable energy.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.16095v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Leveraging Optimal Transport for Distributed Two-Sample Testing: An
  Integrated Transportation Distance-based Framework</h3>
                    <p><strong>Authors:</strong> Zhengqi Lin, Yan Chen</p>
                    <p>  This paper introduces a novel framework for distributed two-sample testing
using the Integrated Transportation Distance (ITD), an extension of the Optimal
Transport distance. The approach addresses the challenges of detecting
distributional changes in decentralized learning or federated learning
environments, where data privacy and heterogeneity are significant concerns. We
provide theoretical foundations for the ITD, including convergence properties
and asymptotic behavior. A permutation test procedure is proposed for practical
implementation in distributed settings, allowing for efficient computation
while preserving data privacy. The framework's performance is demonstrated
through theoretical power analysis and extensive simulations, showing robust
Type I error control and high power across various distributions and
dimensions. The results indicate that ITD effectively aggregates information
across distributed clients, detecting subtle distributional shifts that might
be missed when examining individual clients. This work contributes to the
growing field of distributed statistical inference, offering a powerful tool
for two-sample testing in modern, decentralized data environments.
</p>
                    <p><a href="http://arxiv.org/pdf/2506.16047v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>