<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 4/28/2025, 1:32:02 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Adapting Probabilistic Risk Assessment for AI</h3>
                    <p><strong>Authors:</strong> Anna Katariina Wisakanto, Joe Rogero, Avyay M. Casheekar, Richard Mallah</p>
                    <p>  Modern general-purpose artificial intelligence (AI) systems present an urgent
risk management challenge, as their rapidly evolving capabilities and potential
for catastrophic harm outpace our ability to reliably assess their risks.
Current methods often rely on selective testing and undocumented assumptions
about risk priorities, frequently failing to make a serious attempt at
assessing the set of pathways through which Al systems pose direct or indirect
risks to society and the biosphere. This paper introduces the probabilistic
risk assessment (PRA) for AI framework, adapting established PRA techniques
from high-reliability industries (e.g., nuclear power, aerospace) for the new
challenges of advanced AI. The framework guides assessors in identifying
potential risks, estimating likelihood and severity, and explicitly documenting
evidence, underlying assumptions, and analyses at appropriate granularities.
The framework's implementation tool synthesizes the results into a risk report
card with aggregated risk estimates from all assessed risks. This systematic
approach integrates three advances: (1) Aspect-oriented hazard analysis
provides systematic hazard coverage guided by a first-principles taxonomy of AI
system aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk
pathway modeling analyzes causal chains from system aspects to societal impacts
using bidirectional analysis and incorporating prospective techniques; and (3)
Uncertainty management employs scenario decomposition, reference scales, and
explicit tracing protocols to structure credible projections with novelty or
limited data. Additionally, the framework harmonizes diverse assessment methods
by integrating evidence into comparable, quantified absolute risk estimates for
critical decisions. We have implemented this as a workbook tool for AI
developers, evaluators, and regulators, available on the project website.
</p>
                    <p><a href="http://arxiv.org/pdf/2504.18536v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Bridging the Gap: Introducing Joint Models for Longitudinal and
  Time-to-event Data in the Social Sciences</h3>
                    <p><strong>Authors:</strong> Sophie Potts, Anja Rappl, Karin Kurz, Elisabeth Bergherr</p>
                    <p>  In time-to-event analyses in social sciences, there often exist endogenous
time-varying variables, where the event status is correlated with the
trajectory of the covariate itself. Ignoring this endogeneity will result in
biased estimates. In the field of biostatistics this issue is tackled by
estimating a joint model for longitudinal and time-to-event data as it handles
endogenous covariates properly. This method is underused in the social sciences
even though it is very useful to model longitudinal and time-to-event processes
appropriately. Therefore, this paper provides a gentle introduction to the
method of joint models and highlights its advantages for social science
research questions. We demonstrate its usage on an example on marital
satisfaction and marriage dissolution and compare the results with classical
approaches such as a time-to-event model with a time-varying covariate. In
addition to demonstrating the method, our results contribute to the
understanding of the relationship between marriage satisfaction, marriage
dissolution and other covariates.
</p>
                    <p><a href="http://arxiv.org/pdf/2504.18288v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Auto-Regressive Standard Precipitation Index: A Bayesian Approach for
  Drought Characterization</h3>
                    <p><strong>Authors:</strong> Soham Ghosh, Sujay Mukhoti, Pritee Sharma</p>
                    <p>  This study proposes Auto-Regressive Standardized Precipitation Index (ARSPI)
as a novel alternative to the traditional Standardized Precipitation Index
(SPI) for measuring drought by relaxing the assumption of independent and
identical rainfall distribution over time. ARSPI utilizes an auto-regressive
framework to tackle the auto-correlated characteristics of precipitation,
providing a more precise depiction of drought dynamics. The proposed model
integrates a spike-and-slab log-normal distribution for zero rainfall seasons.
The Bayesian Monte Carlo Markov Chain (MCMC) approach simplifies the SPI
computation using the non-parametric predictive density estimation of total
rainfall across various time windows from simulated samples. The MCMC
simulations further ensure robust estimation of severity, duration, peak and
return period with greater precision. This study also provides a comparison
between the performances of ARSPI and SPI using the precipitation data from the
Colorado River Basin (1893-1991). ARSPI emerges to be more efficient than the
benchmark SPI in terms of model fit. ARSPI shows enhanced sensitivity to
climatic extremes, making it a valuable tool for hydrological research and
water resource management.
</p>
                    <p><a href="http://arxiv.org/pdf/2504.18197v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Bernstein Polynomial Processes for Continuous Time Change Detection</h3>
                    <p><strong>Authors:</strong> Dan Cunha, Mark Friedl, Luis Carvalho</p>
                    <p>  There is a lack of methodological results for continuous time change
detection due to the challenges of noninformative prior specification and
efficient posterior inference in this setting. Most methodologies to date
assume data are collected according to uniformly spaced time intervals. This
assumption incurs bias in the continuous time setting where, a priori, two
consecutive observations measured closely in time are less likely to change
than two consecutive observations that are far apart in time. Models proposed
in this setting have required MCMC sampling which is not ideal. To address
these issues, we derive the heterogeneous continuous time Markov chain that
models change point transition probabilities noninformatively. By construction,
change points under this model can be inferred efficiently using the forward
backward algorithm and do not require MCMC sampling. We then develop a novel
loss function for the continuous time setting, derive its Bayes estimator, and
demonstrate its performance on synthetic data. A case study using time series
of remotely sensed observations is then carried out on three change detection
applications. To reduce falsely detected changes in this setting, we develop a
semiparametric mean function that captures interannual variability due to
weather in addition to trend and seasonal components.
</p>
                    <p><a href="http://arxiv.org/pdf/2504.17876v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Probabilistic modeling of delays for train journeys with transfers</h3>
                    <p><strong>Authors:</strong> Nikolaus Stratil-Sauer, Nils Breyer</p>
                    <p>  Reliability plays a key role in the experience of a rail traveler. The
reliability of journeys involving transfers is affected by the reliability of
the transfers and the consequences of missing a transfer, as well as the
possible delay of the train used to reach the destination. In this paper, we
propose a flexible method to model the reliability of train journeys with any
number of transfers. The method combines a transfer reliability model based on
gradient boosting responsible for predicting the reliability of transfers
between trains and a delay model based on probabilistic Bayesian regression,
which is used to model train arrival delays. The models are trained on delay
data from four Swedish train stations and evaluated on delay data from another
two stations, in order to evaluate the generalization performance of the
models. We show that the probabilistic delay model, which models train delays
following a mixture distribution with two lognormal components, allows to much
more realistically model the distribution of actual train delays compared to a
standard lognormal model. Finally, we show how these models can be used
together to sample the arrival delay at the final destination of the entire
journey. The results indicate that the method accurately predicts the
reliability for nine out of ten tested journeys. The method could be used to
improve journey planners by providing reliability information to travelers.
Further applications include timetable planning and transport modeling.
</p>
                    <p><a href="http://arxiv.org/pdf/2504.17479v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Target trial emulation without matching: a more efficient approach for
  evaluating vaccine effectiveness using observational data</h3>
                    <p><strong>Authors:</strong> Emily Wu, Elizabeth Rogawski McQuade, Mats Stensrud, Razieh Nabi, David Benkeser</p>
                    <p>  Real-world vaccine effectiveness has increasingly been studied using
matching-based approaches, particularly in observational cohort studies
following the target trial emulation framework. Although matching is appealing
in its simplicity, it suffers important limitations in terms of clarity of the
target estimand and the efficiency or precision with which is it estimated.
Scientifically justified causal estimands of vaccine effectiveness may be
difficult to define owing to the fact that vaccine uptake varies over calendar
time when infection dynamics may also be rapidly changing. We propose a causal
estimand of vaccine effectiveness that summarizes vaccine effectiveness over
calendar time, similar to how vaccine efficacy is summarized in a randomized
controlled trial. We describe the identification of our estimand, including its
underlying assumptions, and propose simple-to-implement estimators based on two
hazard regression models. We apply our proposed estimator in simulations and in
a study to assess the effectiveness of the Pfizer-BioNTech COVID-19 vaccine to
prevent infections with SARS-CoV2 in children 5-11 years old. In both settings,
we find that our proposed estimator yields similar scientific inferences while
providing significant efficiency gains over commonly used matching-based
estimators.
</p>
                    <p><a href="http://arxiv.org/pdf/2504.17104v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>MOOSE ProbML: Parallelized Probabilistic Machine Learning and
  Uncertainty Quantification for Computational Energy Applications</h3>
                    <p><strong>Authors:</strong> Somayajulu L. N. Dhulipala, Peter German, Yifeng Che, Zachary M. Prince, Pierre-Clement A. Simon, Xianjian Xie, Vincent M. Laboure, Hao Yan</p>
                    <p>  This paper presents the development and demonstration of massively parallel
probabilistic machine learning (ML) and uncertainty quantification (UQ)
capabilities within the Multiphysics Object-Oriented Simulation Environment
(MOOSE), an open-source computational platform for parallel finite element and
finite volume analyses. In addressing the computational expense and
uncertainties inherent in complex multiphysics simulations, this paper
integrates Gaussian process (GP) variants, active learning, Bayesian inverse
UQ, adaptive forward UQ, Bayesian optimization, evolutionary optimization, and
Markov chain Monte Carlo (MCMC) within MOOSE. It also elaborates on the
interaction among key MOOSE systems -- Sampler, MultiApp, Reporter, and
Surrogate -- in enabling these capabilities. The modularity offered by these
systems enables development of a multitude of probabilistic ML and UQ
algorithms in MOOSE. Example code demonstrations include parallel active
learning and parallel Bayesian inference via active learning. The impact of
these developments is illustrated through five applications relevant to
computational energy applications: UQ of nuclear fuel fission product release,
using parallel active learning Bayesian inference; very rare events analysis in
nuclear microreactors using active learning; advanced manufacturing process
modeling using multi-output GPs (MOGPs) and dimensionality reduction; fluid
flow using deep GPs (DGPs); and tritium transport model parameter optimization
for fusion energy, using batch Bayesian optimization.
</p>
                    <p><a href="http://arxiv.org/pdf/2504.17101v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model
  for Predicting Cryptocurrency Prices</h3>
                    <p><strong>Authors:</strong> Esam Mahdi, C. Martin-Barreiro, X. Cabezas</p>
                    <p>  In this article, we introduce a novel deep learning hybrid model that
integrates attention Transformer and Gated Recurrent Unit (GRU) architectures
to improve the accuracy of cryptocurrency price predictions. By combining the
Transformer's strength in capturing long-range patterns with the GRU's ability
to model short-term and sequential trends, the hybrid model provides a
well-rounded approach to time series forecasting. We apply the model to predict
the daily closing prices of Bitcoin and Ethereum based on historical data that
include past prices, trading volumes, and the Fear and Greed index. We evaluate
the performance of our proposed model by comparing it with four other machine
learning models: two are non-sequential feedforward models: Radial Basis
Function Network (RBFN) and General Regression Neural Network (GRNN), and two
are bidirectional sequential memory-based models: Bidirectional Long-Short-Term
Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance
of the model is assessed using several metrics, including Mean Squared Error
(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean
Absolute Percentage Error (MAPE), along with statistical validation through the
nonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.
The results demonstrate that our hybrid model consistently achieves superior
accuracy, highlighting its effectiveness for financial prediction tasks. These
findings provide valuable insights for improving real-time decision making in
cryptocurrency markets and support the growing use of hybrid deep learning
models in financial analytics.
</p>
                    <p><a href="http://arxiv.org/pdf/2504.17079v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Sensitivity Analysis Framework for Quantifying Confidence in Decisions
  in the Presence of Data Uncertainty</h3>
                    <p><strong>Authors:</strong> Adway S. Wadekar, Jerome P. Reiter</p>
                    <p>  Nearly all statistical analyses that inform policy-making are based on
imperfect data. As examples, the data may suffer from measurement errors,
missing values, sample selection bias, or record linkage errors. Analysts have
to decide how to handle such data imperfections, e.g., analyze only the
complete cases or impute values for the missing items via some posited model.
Their choices can influence estimates and hence, ultimately, policy decisions.
Thus, it is prudent for analysts to evaluate the sensitivity of estimates and
policy decisions to the assumptions underlying their choices. To facilitate
this goal, we propose that analysts define metrics and visualizations that
target the sensitivity of the ultimate decision to the assumptions underlying
their approach to handling the data imperfections. Using these visualizations,
the analyst can assess their confidence in the policy decision under their
chosen analysis. We illustrate metrics and corresponding visualizations with
two examples, namely considering possible measurement error in the inputs of
predictive models of presidential vote share and imputing missing values when
evaluating the percentage of children exposed to high levels of lead.
</p>
                    <p><a href="http://arxiv.org/pdf/2504.17043v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Weighted-likelihood framework for class imbalance in Bayesian
  prediction models</h3>
                    <p><strong>Authors:</strong> Stanley E. Lazic</p>
                    <p>  Class imbalance occurs when data used for training classification models has
a different number of observations or samples within each category or class.
Models built on such data can be biased towards the majority class and have
poor predictive performance and generalisation for the minority class. We
propose a Bayesian weighted-likelihood (power-likelihood) approach to deal with
class imbalance: each observation's likelihood is raised to a weight inversely
proportional to its class proportion, with weights normalized to sum to the
number of samples. This embeds cost-sensitive learning directly into Bayesian
updating and is applicable to binary, multinomial and ordered logistic
prediction models. Example models are implemented in Stan, PyMC, and Turing.jl,
and all code and reproducible scripts are archived on Github:
https://github.com/stanlazic/weighted_likelihoods. This approach is simple to
implement and extends naturally to arbitrary error-cost matrices.
</p>
                    <p><a href="http://arxiv.org/pdf/2504.17013v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>