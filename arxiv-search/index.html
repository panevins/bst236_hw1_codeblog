<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 3/17/2025, 1:22:14 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Quantifying sleep apnea heterogeneity using hierarchical Bayesian
  modeling</h3>
                    <p><strong>Authors:</strong> Glenn Palmer, David B. Dunson</p>
                    <p>  Obstructive Sleep Apnea (OSA) is a breathing disorder during sleep that
affects millions of people worldwide. The diagnosis of OSA often occurs through
an overnight polysomnogram (PSG) sleep study that generates a massive amount of
physiological data. However, despite the evidence of substantial heterogeneity
in the expression and symptoms of OSA, diagnosis and scientific analysis of
severity typically focus on a single summary statistic, the Apnea-Hypopnea
Index (AHI). To address the limitations inherent in such analyses, we propose a
hierarchical Bayesian modeling approach to analyze PSG data. Our approach
produces an interpretable vector of random effect parameters for each patient
that govern sleep-stage dynamics, rates of OSA events, and impacts of OSA
events on subsequent sleep-stage dynamics. We propose a novel approach for
using these random effects to produce a Bayes optimal cluster of patients under
K-means loss. We use the proposed approach to analyze data from the APPLES
study. This analysis produces clinically interesting groups of patients with
sleep apnea and a novel finding of an association between OSA expression and
cognitive performance that is missed by an AHI-based analysis.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.11599v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Data-Driven Construction of Age-Structured Contact Networks</h3>
                    <p><strong>Authors:</strong> Luke Murray Kearney, Emma L. Davis, Matt J. Keeling</p>
                    <p>  Capturing the structure of a population and characterising contacts within
the population are key to reliable projections of infectious disease. Two main
elements of population structure -- contact heterogeneity and age -- have been
repeatedly demonstrated to be key in infection dynamics, yet are rarely
combined. Regarding individuals as nodes and contacts as edges within a network
provides a powerful and intuitive method to fully realise this population
structure. While there are a few key examples of contact networks being
measured explicitly, in general we need to construct the appropriate networks
from individual-level data. Here, using data from social contact surveys, we
develop a generic and robust algorithm to generate an extrapolated network that
preserves both age-structured mixing and heterogeneity in the number of
contacts. We then use these networks to simulate the spread of infection
through the population, constrained to have a given basic reproduction number
($R_0$) and hence a given early growth rate. Given the over-dominant role that
highly connected nodes (`superspreaders') would otherwise play in early
dynamics, we scale transmission by the average duration of contacts, providing
a better match to surveillance data for numbers of secondary cases. This
network-based model shows that, for COVID-like parameters, including both
heterogeneity and age-structure reduces both peak height and epidemic size
compared to models that ignore heterogeneity. Our robust methodology therefore
allows for the inclusion of the full wealth of data commonly collected by
surveys but frequently overlooked to be incorporated into more realistic
transmission models of infectious diseases.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.11527v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>CRPS-Based Targeted Sequential Design with Application in Chemical Space</h3>
                    <p><strong>Authors:</strong> Lea Friedli, Athénaïs Gautier, Anna Broccard, David Ginsbourger</p>
                    <p>  Sequential design of real and computer experiments via Gaussian Process (GP)
models has proven useful for parsimonious, goal-oriented data acquisition
purposes. In this work, we focus on acquisition strategies for a GP model that
needs to be accurate within a predefined range of the response of interest.
Such an approach is useful in various fields including synthetic chemistry,
where finding molecules with particular properties is essential for developing
useful materials and effective medications. GP modeling and sequential design
of experiments have been successfully applied to a plethora of domains,
including molecule research. Our main contribution here is to use the
threshold-weighted Continuous Ranked Probability Score (CRPS) as a basic
building block for acquisition functions employed within sequential design. We
study pointwise and integral criteria relying on two different weighting
measures and benchmark them against competitors, demonstrating improved
performance with respect to considered goals. The resulting acquisition
strategies are applicable to a wide range of fields and pave the way to further
developing sequential design relying on scoring rules.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.11250v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>The pushed beta distribution and contaminated binary sampling</h3>
                    <p><strong>Authors:</strong> Ben O'Neill</p>
                    <p>  We examine a generalisation of the beta distribution that we call the pushed
beta distribution. This is a continuous univariate distribution on the unit
interval which generalises the beta distribution by "pushing" the density in a
particular direction using an additional multiplicative term in the density
kernel. We examine the properties of this distribution and compare it to the
beta distribution. We also examine the use of this distribution in contaminated
binary sampling using Bayesian inference. We find that this distribution arises
as the appropriate posterior distribution for inference in certain kinds of
contaminated binary models. We derive a broad range of properties of the
distribution and we also establish some computational methods to compute
various functions for the distribution.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.11128v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Configuration Design of Mechanical Assemblies using an Estimation of
  Distribution Algorithm and Constraint Programming</h3>
                    <p><strong>Authors:</strong> Hyunmin Cheong, Mehran Ebrahimi, Adrian Butscher, Francesco Iorio</p>
                    <p>  A configuration design problem in mechanical engineering involves finding an
optimal assembly of components and joints that realizes some desired
performance criteria. Such a problem is a discrete, constrained, and black-box
optimization problem. A novel method is developed to solve the problem by
applying Bivariate Marginal Distribution Algorithm (BMDA) and constraint
programming (CP). BMDA is a type of Estimation of Distribution Algorithm (EDA)
that exploits the dependency knowledge learned between design variables without
requiring too many fitness evaluations, which tend to be expensive for the
current application. BMDA is extended with adaptive chi-square testing to
identify dependencies and Gibbs sampling to generate new solutions. Also,
repair operations based on CP are used to deal with infeasible solutions found
during search. The method is applied to a vehicle suspension design problem and
is found to be more effective in converging to good solutions than a genetic
algorithm and other EDAs. These contributions are significant steps towards
solving the difficult problem of configuration design in mechanical engineering
with evolutionary computation.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.11002v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Surviving the frailty of time to event analysis in massive datasets with
  Generalized Additive Models (and the help of Simon Laplace)</h3>
                    <p><strong>Authors:</strong> Christos Argyropoulos, Hamza Mir, Maria-Eleni Roumelioti, Pablo Garcia</p>
                    <p>  Analyses of time to event datasets have been invariably based on the Cox
proportional hazards model (PHM). Reformulations of the PHM as a Poisson
Generalized Additive Model (GAM) or as a Generalized Linear Mixed Model (GLMM)
have been proposed in the literature, aiming to increase the flexibility of the
PHM and allow its use in situations in which complex spatiotemporal
relationships have to be taken into account when modeling survival. In this
report, we provide a unified framework for considering these previous attempts
and consider the implementation in software for GAM and GLMM in the R
programming language. The connection between GAM/GLMM and the PHM is leveraged
to provide computationally efficient implementations for a subclass of survival
models that incorporate individual random effects ('frailty models'). Frailty
models provide a unified method to address repeated events, correlated outcomes
and also time varying visitation schedules when analyzing Electronic Health
Record data. However the current implementation of frailty models in software
facilities for the Cox model does not scale because of long computation times;
conversely the direct implementation of individual random effects in GAM/GLMM
software does not scale well with memory usage. We propose a two stage method
for survival models with frailty based on the Laplace approximation. Using a
D-optimal experimental design to simulate the performance of the proposed
method across simulated datasets we illustrate that the proposed method can
circumvent the limitations of existing implementations, opening up the
possibility to model datasets of hundred of thousands to million individuals
using high end workstations from within R.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.10823v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Bayes factors for partial correlation</h3>
                    <p><strong>Authors:</strong> Saptati Datta, Valen E. Johnson</p>
                    <p>  Partial correlation coefficients are widely applied in the social sciences to
evaluate the relationship between two variables after accounting for the
influence of others. In this article, we present Bayes Factor Functions (BFFs)
for assessing the presence of partial correlation. BFFs represent Bayes factors
derived from test statistics and are expressed as functions of a standardized
effect size. While traditional frequentist methods based on $p$-values have
been criticized for their inability to provide cumulative evidence in favor of
the true hypothesis, Bayesian approaches are often challenged due to their
computational demands and sensitivity to prior distributions. BFFs overcome
these limitations and offer summaries of hypothesis tests as alternative
hypotheses are varied over a range of prior distributions on standardized
effects. They also enable the integration of evidence across multiple studies.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.10787v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>On the Proportional Principal Stratum Hazards Model</h3>
                    <p><strong>Authors:</strong> Jiren Sun, Thomas D. Cook</p>
                    <p>  In clinical trials involving both mortality and morbidity, an active
treatment can influence the observed risk of the first non-fatal event either
directly, through its effect on the non-fatal event process, or indirectly,
through its effect on the death process, or both. Discerning the direct effect
of treatment on the first non-fatal event holds clinical interest. However,
with the competing risk of death, the Cox proportional hazards model that
treats death as non-informative censoring and evaluates treatment effects on
time to the first non-fatal event provides an estimate of the cause-specific
hazard ratio, which may not correspond to the direct effect. To obtain the
direct effect on the first non-fatal event, within the principal stratification
framework, we define the principal stratum hazard and introduce the
Proportional Principal Stratum Hazards model. This model estimates the
principal stratum hazard ratio, which reflects the direct effect on the first
non-fatal event in the presence of death and simplifies to the hazard ratio in
the absence of death. The principal stratum membership is identified using the
shared frailty model, which assumes independence between the first non-fatal
event process and the potential death process from the counterfactual arm,
conditional on per-subject random frailty. Simulation studies are conducted to
verify the reliability of our estimators. We illustrate the method using the
Carvedilol Prospective Randomized Cumulative Survival trial which involves
heart-failure events.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.10481v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Estimating relapse time distribution from longitudinal biomarker
  trajectories using iterative regression and continuous time Markov processes</h3>
                    <p><strong>Authors:</strong> Alice Cleynen, Benoîte de Saporta, Amélie Vernay</p>
                    <p>  Biomarker measurements obtained by blood sampling are often used as a
non-invasive means of monitoring tumour progression in cancer patients.
Diseases evolve dynamically over time, and studying longitudinal observations
of specific biomarkers can help to understand patients response to treatment
and predict disease progression. We propose a novel iterative regression-based
method to estimate changes in patients status within a cohort that includes
censored patients, and illustrate it on clinical data from myeloma cases. We
formulate the relapse time estimation problem in the framework of Piecewise
Deterministic Markov processes (PDMP), where the Euclidean component is a
surrogate biomarker for patient state. This approach enables continuous-time
estimation of the status-change dates, which in turn allows for accurate
inference of the relapse time distribution. A key challenge lies in the partial
observability of the process, a complexity that has been rarely addressed in
previous studies. . We evaluate the performance of our procedure through a
simulation study and compare it with different approaches. This work is a proof
of concept on biomarker trajectories with simple behaviour, but our method can
easily be extended to more complex dynamics.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.10448v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Generalized network autoregressive modelling of longitudinal networks
  with application to presidential elections in the USA</h3>
                    <p><strong>Authors:</strong> Guy Nason, Daniel Salnikov, Mario Cortina-Borja</p>
                    <p>  Longitudinal networks are becoming increasingly relevant in the study of
dynamic processes characterised by known or inferred community structure.
Generalised Network Autoregressive (GNAR) models provide a parsimonious
framework for exploiting the underlying network and multivariate time series.
We introduce the community-$\alpha$ GNAR model with interactions that exploits
prior knowledge or exogenous variables for analysing interactions within and
between communities, and can describe serial correlation in longitudinal
networks. We derive new explicit finite-sample error bounds that validate
analysing high-dimensional longitudinal network data with GNAR models, and
provide insights into their attractive properties. We further illustrate our
approach by analysing the dynamics of $\textit{Red, Blue}$ and $\textit{Swing}$
states throughout presidential elections in the USA from 1976 to 2020, that is,
a time series of length twelve on 51 time series (US states and Washington DC).
Our analysis connects network autocorrelation to eight-year long terms,
highlights a possible change in the system after the 2016 election, and a
difference in behaviour between $\textit{Red}$ and $\textit{Blue}$ states.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.10433v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>