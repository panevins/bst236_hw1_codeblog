<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 2/16/2025, 12:07:42 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Project portfolio planning in the pharmaceutical industry - strategic
  objectives and quantitative optimization</h3>
                    <p><strong>Authors:</strong> Stig Johan Wiklund, Magnus Ytterstad, Frank Miller</p>
                    <p>  Many pharmaceutical companies face concerns with the maintenance of desired
revenue levels. Sales forecasts for the current portfolio of products and
projects may indicate a decline in revenue as the marketed products approach
patent expiry. To counteract the potential downturn in revenue, and to
establish revenue growth, an in-flow of new projects into the development
phases is required. In this article, we devise an approach with which the
in-flow of new projects could be optimized, while adhering to the objectives
and constraints set on revenue targets, budget limitations and strategic
considerations on the composition of the company's portfolio.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.09527v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Bayesian Matrix Factor Models for Demographic Analysis Across Age and
  Time</h3>
                    <p><strong>Authors:</strong> Gregor Zens</p>
                    <p>  Analyzing demographic data collected across multiple populations, time
periods, and age groups is challenging due to the interplay of high
dimensionality, demographic heterogeneity among groups, and stochastic
variability within smaller groups. This paper proposes a Bayesian matrix factor
model to address these challenges. By factorizing count data matrices as the
product of low-dimensional latent age and time factors, the model achieves a
parsimonious representation that mitigates overfitting and remains
computationally feasible even when hundreds of subpopulations are involved.
Smoothness in age factors and a dynamic evolution of time factors are achieved
through informative priors, and an efficient Markov chain Monte Carlo algorithm
is developed for posterior inference. Applying the model to Austrian
district-level emigration data from 2002 to 2023 demonstrates its ability to
reconstruct demographic processes using only a fraction of the parameters
required by conventional factor models. Extensive cross-validation and
out-of-sample forecasting exercises show that the proposed matrix factor model
consistently outperforms standard benchmarks. Beyond statistical demography,
the framework holds promise for a wide range of applications involving noisy,
heterogeneous, and high-dimensional non-Gaussian matrix-valued data.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.09255v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Sequential Covariance Fitting for InSAR Phase Linking</h3>
                    <p><strong>Authors:</strong> Dana El Hajjar, Guillaume Ginolhac, Yajing Yan, Mohammed Nabil El Korso</p>
                    <p>  Traditional Phase-Linking (PL) algorithms are known for their high cost,
especially with the huge volume of Synthetic Aperture Radar (SAR) images
generated by Sentinel-1 SAR missions. Recently, a COvariance Fitting
Interferometric Phase Linking (COFI-PL) approach has been proposed, which can
be seen as a generic framework for existing PL methods. Although this method is
less computationally expensive than traditional PL approaches, COFI-PL exploits
the entire covariance matrix, which poses a challenge with the increasing time
series of SAR images. However, COFI-PL, like traditional PL approaches, cannot
accommodate the efficient inclusion of newly acquired SAR images. This paper
overcomes this drawback by introducing a sequential integration of a block of
newly acquired SAR images. Specifically, we propose a method for effectively
addressing optimization problems associated with phase-only complex vectors on
the torus based on the Majorization-Minimization framework.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.09248v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Differential Index Measuring Rater's Capability in Educational
  Assessment</h3>
                    <p><strong>Authors:</strong> Y. -G. Wang, J. Wu, X. Qiu</p>
                    <p>  A rater's ability to assign accurate scores can significantly impact the
outcomes of educational assessments. However, common indices for evaluating
rater characteristics typically focus on either their severity or their
discrimination ability (i.e., skills to differentiate between students).
Additionally, these indices are often developed without considering the rater's
accuracy in scoring students at different ability levels. To address the
limitations, this study proposes a single-value measure to assess a rater's
capability of assigning accurate scores to students with varying ability
levels. The measure is derived from the partial derivatives of each rater's
passing rate concerning student ability. Mathematical derivations of the index
under generalized multi-facet models and hierarchical rater models are
provided. To ease the implementation of the index, this study develops
parameter estimation using marginal likelihood and its Laplacian approximation
which allows for efficient evaluation and processing of large datasets
involving numerous students and raters. Simulation studies demonstrate the
accuracy of parameter recovery using the approximate likelihood and show how
the capability indices vary with different levels of rater severity. An
empirical study further tests the practical applicability of the new measure,
where raters evaluate essays on four topics: "family," "school," "sport," and
"work." Results show that raters are most capable when rating the topic of
family and least capable when rating sport, with individual raters displaying
different capabilities across the various topics.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.09099v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Warnings based on risk matrices: a coherent framework with consistent
  evaluation</h3>
                    <p><strong>Authors:</strong> Robert J. Taggart, David J. Wilke</p>
                    <p>  Risk matrices are widely used across a range of fields and have found
increasing utility in warning decision practices globally. However, their
application in this context presents challenges, which range from potentially
perverse warning outcomes to a lack of objective verification (i.e.,
evaluation) methods. This paper introduces a coherent framework for generating
multi-level warnings from risk matrices to address these challenges. The
proposed framework is general, is based on probabilistic forecasts of hazard
severity or impact and is compatible with the Common Alerting Protocol (CAP).
Moreover, it includes a family of consistent scoring functions for objectively
evaluating the predictive performance of risk matrix assessments and the
warnings they produce. These scoring functions enable the ranking of
forecasters or warning systems and the tracking of system improvements by
rewarding accurate probabilistic forecasts and compliance with warning service
directives. A synthetic experiment demonstrates the efficacy of these scoring
functions, while the framework is illustrated through warnings for heavy
rainfall based on operational ensemble prediction system forecasts for Tropical
Cyclone Jasper (Queensland, Australia, 2023). This work establishes a robust
foundation for enhancing the reliability and verifiability of risk-based
warning systems.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08891v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Statistical inference for Levy-driven graph supOU processes: From short-
  to long-memory in high-dimensional time series</h3>
                    <p><strong>Authors:</strong> Shreya Mehta, Almut E. D. Veraart</p>
                    <p>  This article introduces Levy-driven graph supOU processes, offering a
parsimonious parametrisation for high-dimensional time-series, where
dependencies between the individual components are governed via a graph
structure. Specifically, we propose a model specification that allows for a
smooth transition between short- and long-memory settings while accommodating a
wide range of marginal distributions.
  We further develop an inference procedure based on the generalised method of
moments, establish its asymptotic properties and demonstrate its strong finite
sample performance through a simulation study.
  Finally, we illustrate the practical relevance of our new model and
estimation method in an empirical study of wind capacity factors in an European
electricity network context.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08838v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Mortality simulations for insured and general populations</h3>
                    <p><strong>Authors:</strong> Asmik Nalmpatian, Christian Heumann</p>
                    <p>  This study presents a framework for high-resolution mortality simulations
tailored to insured and general populations. Due to the scarcity of detailed
demographic-specific mortality data, we leverage Iterative Proportional Fitting
(IPF) and Monte Carlo simulations to generate refined mortality tables that
incorporate age, gender, smoker status, and regional distributions. This
methodology enhances public health planning and actuarial analysis by providing
enriched datasets for improved life expectancy projections and insurance
product development.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08814v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Increasing competitiveness by imbalanced groups: The example of the
  48-team FIFA World Cup</h3>
                    <p><strong>Authors:</strong> László Csató, András Gyimesi</p>
                    <p>  A match played in a sports tournament can be called stakeless if at least one
team is indifferent to its outcome because it already has qualified or has been
eliminated. Such a game threatens fairness since teams may not exert full
effort without incentives. This paper suggests a novel classification for
stakeless matches according to their expected outcome: they are more costly if
the indifferent team is more likely to win by playing honestly. Our approach is
illustrated with the 2026 FIFA World Cup, the first edition of the competition
with 48 teams. We propose a novel format based on imbalanced groups, which
drastically reduces the probability of stakeless matches played by the
strongest teams according to Monte Carlo simulations. The new design also
increases the uncertainty of match outcomes and requires fewer matches.
Governing bodies in sports are encouraged to consider our innovative idea in
order to enhance the competitiveness of their tournaments.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08565v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A comparison of Dirichlet kernel regression methods on the simplex</h3>
                    <p><strong>Authors:</strong> Hanen Daayeb, Christian Genest, Salah Khardani, Nicolas Klutchnikoff, Frédéric Ouimet</p>
                    <p>  An asymmetric Dirichlet kernel version of the Gasser-M\"uller estimator is
introduced for regression surfaces on the simplex, extending the univariate
analog proposed by Chen [Statist. Sinica, 10(1) (2000), pp. 73-91]. Its
asymptotic properties are investigated under the condition that the design
points are known and fixed, including an analysis of its mean integrated
squared error (MISE) and its asymptotic normality. The estimator is also
applicable in a random design setting. A simulation study compares its
performance with two recently proposed alternatives: the Nadaraya--Watson
estimator with Dirichlet kernel and the local linear smoother with Dirichlet
kernel. The results show that the local linear smoother consistently
outperforms the others. To illustrate its applicability, the local linear
smoother is applied to the GEMAS dataset to analyze the relationship between
soil composition and pH levels across various agricultural and grazing lands in
Europe.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08461v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Tutorial for Surrogate Endpoint Validation Using Joint modeling and
  Mediation Analysis</h3>
                    <p><strong>Authors:</strong> Quentin Le Coent, Virginie Rondeau, Catherine Legrand</p>
                    <p>  The use of valid surrogate endpoints is an important stake in clinical
research to help reduce both the duration and cost of a clinical trial and
speed up the evaluation of interesting treatments. Several methods have been
proposed in the statistical literature to validate putative surrogate
endpoints. Two main approaches have been proposed: the meta-analytic approach
and the mediation analysis approach. The former uses data from meta-analyses to
derive associations measures between the surrogate and the final endpoint at
the individual and trial levels. The latter rather uses the proportion of the
treatment effect on the final endpoint through the surrogate as a measure of
surrogacy in a causal inference framework. Both approaches have remained
separated as the meta-analytic approach does not estimate the treatment effect
on the final endpoint through the surrogate while the mediation analysis
approach have been limited to single-trial setting. However, these two
approaches are complementary. In this work we propose an approach that combines
the meta-analytic and mediation analysis approaches using joint modeling for
surrogate validation. We focus on the cases where the final endpoint is a
time-to-event endpoint (such as time-to-death) and the surrogate is either a
time-to-event or a longitudinal biomarker. Two new joint models were proposed
depending on the nature of the surrogate. These model are implemented in the R
package frailtypack. We illustrate the developed approaches in three
applications on real datasets in oncology.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.08443v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>