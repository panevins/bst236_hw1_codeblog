<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 2/20/2025, 12:08:53 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Methods of multi-indication meta-analysis for health technology
  assessment: a simulation study</h3>
                    <p><strong>Authors:</strong> David Glynn, Pedro Saramago, Janharpreet Singh, Sylwia Bujkiewicz, Sofia Dias, Stephen Palmer, Marta Soares</p>
                    <p>  A growing number of oncology treatments, such as bevacizumab, are used across
multiple indications. However, in health technology assessment (HTA), their
clinical and cost-effectiveness are typically appraised within a single target
indication. This approach excludes a broader evidence base across other
indications. To address this, we explored multi-indication meta-analysis
methods that share evidence across indications.
  We conducted a simulation study to evaluate alternative multi-indication
synthesis models. This included univariate (mixture and non-mixture) methods
synthesizing overall survival (OS) data and bivariate surrogacy models jointly
modelling treatment effects on progression-free survival (PFS) and OS, pooling
surrogacy parameters across indications. Simulated datasets were generated
using a multistate disease progression model under various scenarios, including
different levels of heterogeneity within and between indications, outlier
indications, and varying data on OS for the target indication. We evaluated the
performance of the synthesis models applied to the simulated datasets, in terms
of their ability to predict overall survival (OS) in a target indication.
  The results showed univariate multi-indication methods could reduce
uncertainty without increasing bias, particularly when OS data were available
in the target indication. Compared with univariate methods, mixture models did
not significantly improve performance and are not recommended for HTA. In
scenarios where OS data in the target indication is absent and there were also
outlier indications, bivariate surrogacy models showed promise in correcting
bias relative to univariate models, though further research under realistic
conditions is needed.
  Multi-indication methods are more complex than traditional approaches but can
potentially reduce uncertainty in HTA decisions.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13844v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Zero-Inflated Poisson Latent Position Cluster Model</h3>
                    <p><strong>Authors:</strong> Chaoyi Lu, Riccardo Rastelli, Nial Friel</p>
                    <p>  The latent position network model (LPM) is a popular approach for the
statistical analysis of network data. A central aspect of this model is that it
assigns nodes to random positions in a latent space, such that the probability
of an interaction between each pair of individuals or nodes is determined by
their distance in this latent space. A key feature of this model is that it
allows one to visualize nuanced structures via the latent space representation.
The LPM can be further extended to the Latent Position Cluster Model (LPCM), to
accommodate the clustering of nodes by assuming that the latent positions are
distributed following a finite mixture distribution. In this paper, we extend
the LPCM to accommodate missing network data and apply this to non-negative
discrete weighted social networks. By treating missing data as ``unusual'' zero
interactions, we propose a combination of the LPCM with the zero-inflated
Poisson distribution. Statistical inference is based on a novel partially
collapsed Markov chain Monte Carlo algorithm, where a
Mixture-of-Finite-Mixtures (MFM) model is adopted to automatically determine
the number of clusters and optimal group partitioning. Our algorithm features a
truncated absorb-eject move, which is a novel adaptation of an idea commonly
used in collapsed samplers, within the context of MFMs. Another aspect of our
work is that we illustrate our results on 3-dimensional latent spaces,
maintaining clear visualizations while achieving more flexibility than
2-dimensional models. The performance of this approach is illustrated via two
carefully designed simulation studies, as well as four different publicly
available real networks, where some interesting new perspectives are uncovered.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13790v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>On noncentral Wishart mixtures of noncentral Wisharts and their use for
  testing random effects in factorial design models</h3>
                    <p><strong>Authors:</strong> Christian Genest, Anne MacKay, Frédéric Ouimet</p>
                    <p>  It is shown that a noncentral Wishart mixture of noncentral Wishart
distributions with the same degrees of freedom yields a noncentral Wishart
distribution, thereby extending the main result of Jones and Marchand [Stat 10
(2021), Paper No. e398, 7 pp.] from the chi-square to the Wishart setting. To
illustrate its use, this fact is then employed to derive the finite-sample
distribution of test statistics for random effects in a two-factor factorial
design model with $d$-dimensional normal data, thereby broadening the findings
of Bilodeau [ArXiv (2022), 6 pp.], who treated the case $d = 1$. The same
approach makes it possible to test random effects in more general factorial
design models.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13711v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Multi-view biclustering via non-negative matrix tri-factorisation</h3>
                    <p><strong>Authors:</strong> Ella S. C. Orme, Theodoulos Rodosthenous, Marina Evangelou</p>
                    <p>  Multi-view data is ever more apparent as methods for production, collection
and storage of data become more feasible both practically and fiscally.
However, not all features are relevant to describe the patterns for all
individuals. Multi-view biclustering aims to simultaneously cluster both rows
and columns, discovering clusters of rows as well as their view-specific
identifying features. A novel multi-view biclustering approach based on
non-negative matrix factorisation is proposed (ResNMTF). Demonstrated through
extensive experiments on both synthetic and real datasets, ResNMTF successfully
identifies both overlapping and non-exhaustive biclusters, without pre-existing
knowledge of the number of biclusters present, and is able to incorporate any
combination of shared dimensions across views. Further, to address the lack of
a suitable bicluster-specific intrinsic measure, the popular silhouette score
is extended to the bisilhouette score. The bisilhouette score is demonstrated
to align well with known extrinsic measures, and proves useful as a tool for
hyperparameter tuning as well as visualisation.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13698v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Study on Monthly Marine Heatwave Forecasts in New Zealand: An
  Investigation of Imbalanced Regression Loss Functions with Neural Network
  Models</h3>
                    <p><strong>Authors:</strong> Ding Ning, Varvara Vetrova, Sébastien Delaux, Rachael Tappenden, Karin R. Bryan, Yun Sing Koh</p>
                    <p>  Marine heatwaves (MHWs) are extreme ocean-temperature events with significant
impacts on marine ecosystems and related industries. Accurate forecasts (one to
six months ahead) of MHWs would aid in mitigating these impacts. However,
forecasting MHWs presents a challenging imbalanced regression task due to the
rarity of extreme temperature anomalies in comparison to more frequent moderate
conditions. In this study, we examine monthly MHW forecasts for 12 locations
around New Zealand. We use a fully-connected neural network and compare
standard and specialized regression loss functions, including the mean squared
error (MSE), the mean absolute error (MAE), the Huber, the weighted MSE, the
focal-R, the balanced MSE, and a proposed scaling-weighted MSE. Results show
that (i) short lead times (one month) are considerably more predictable than
three- and six-month leads, (ii) models trained with the standard MSE or MAE
losses excel at forecasting average conditions but struggle to capture
extremes, and (iii) specialized loss functions such as the balanced MSE and our
scaling-weighted MSE substantially improve forecasting of MHW and suspected MHW
events. These findings underscore the importance of tailored loss functions for
imbalanced regression, particularly in forecasting rare but impactful events
such as MHWs.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13495v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>BISON: Bi-clustering of spatial omics data with feature selection</h3>
                    <p><strong>Authors:</strong> Bencong Zhu, Alberto Cassese, Marina Vannucci, Michele Guindani, Qiwei Li</p>
                    <p>  The advent of next-generation sequencing-based spatially resolved
transcriptomics (SRT) techniques has reshaped genomic studies by enabling
high-throughput gene expression profiling while preserving spatial and
morphological context. Understanding gene functions and interactions in
different spatial domains is crucial, as it can enhance our comprehension of
biological mechanisms, such as cancer-immune interactions and cell
differentiation in various regions. It is necessary to cluster tissue regions
into distinct spatial domains and identify discriminating genes that elucidate
the clustering result, referred to as spatial domain-specific discriminating
genes (DGs). Existing methods for identifying these genes typically rely on a
two-stage approach, which can lead to the phenomenon known as
\textit{double-dipping}. To address the challenge, we propose a unified
Bayesian latent block model that simultaneously detects a list of DGs
contributing to spatial domain identification while clustering these DGs and
spatial locations. The efficacy of our proposed method is validated through a
series of simulation experiments, and its capability to identify DGs is
demonstrated through applications to benchmark SRT datasets.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13453v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Performance Evaluation of Large Language Models in Statistical
  Programming</h3>
                    <p><strong>Authors:</strong> Xinyi Song, Kexin Xie, Lina Lee, Ruizhe Chen, Jared M. Clark, Hao He, Haoran He, Jie Min, Xinlei Zhang, Simin Zheng, Zhiyang Zhang, Xinwei Deng, Yili Hong</p>
                    <p>  The programming capabilities of large language models (LLMs) have
revolutionized automatic code generation and opened new avenues for automatic
statistical analysis. However, the validity and quality of these generated
codes need to be systematically evaluated before they can be widely adopted.
Despite their growing prominence, a comprehensive evaluation of statistical
code generated by LLMs remains scarce in the literature. In this paper, we
assess the performance of LLMs, including two versions of ChatGPT and one
version of Llama, in the domain of SAS programming for statistical analysis.
Our study utilizes a set of statistical analysis tasks encompassing diverse
statistical topics and datasets. Each task includes a problem description,
dataset information, and human-verified SAS code. We conduct a comprehensive
assessment of the quality of SAS code generated by LLMs through human expert
evaluation based on correctness, effectiveness, readability, executability, and
the accuracy of output results. The analysis of rating scores reveals that
while LLMs demonstrate usefulness in generating syntactically correct code,
they struggle with tasks requiring deep domain understanding and may produce
redundant or incorrect results. This study offers valuable insights into the
capabilities and limitations of LLMs in statistical programming, providing
guidance for future advancements in AI-assisted coding systems for statistical
analysis.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13117v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Bridging the Data Gap in AI Reliability Research and Establishing
  DR-AIR, a Comprehensive Data Repository for AI Reliability</h3>
                    <p><strong>Authors:</strong> Simin Zheng, Jared M. Clark, Fatemeh Salboukh, Priscila Silva, Karen da Mata, Fenglian Pan, Jie Min, Jiayi Lian, Caleb B. King, Lance Fiondella, Jian Liu, Xinwei Deng, Yili Hong</p>
                    <p>  Artificial intelligence (AI) technology and systems have been advancing
rapidly. However, ensuring the reliability of these systems is crucial for
fostering public confidence in their use. This necessitates the modeling and
analysis of reliability data specific to AI systems. A major challenge in AI
reliability research, particularly for those in academia, is the lack of
readily available AI reliability data. To address this gap, this paper focuses
on conducting a comprehensive review of available AI reliability data and
establishing DR-AIR: a data repository for AI reliability. Specifically, we
introduce key measurements and data types for assessing AI reliability, along
with the methodologies used to collect these data. We also provide a detailed
description of the currently available datasets with illustrative examples.
Furthermore, we outline the setup of the DR-AIR repository and demonstrate its
practical applications. This repository provides easy access to datasets
specifically curated for AI reliability research. We believe these efforts will
significantly benefit the AI research community by facilitating access to
valuable reliability data and promoting collaboration across various academic
domains within AI. We conclude our paper with a call to action, encouraging the
research community to contribute and share AI reliability data to further
advance this critical field of study.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.12386v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Locally-Deployed Chain-of-Thought (CoT) Reasoning Model in Chemical
  Engineering: Starting from 30 Experimental Data</h3>
                    <p><strong>Authors:</strong> Tianhang Zhou, Yingchun Niu, Xingying Lan, Chunming Xu</p>
                    <p>  In the field of chemical engineering, traditional data-processing and
prediction methods face significant challenges. Machine-learning and
large-language models (LLMs) also have their respective limitations. This paper
explores the application of the Chain-of-Thought (CoT) reasoning model in
chemical engineering, starting from 30 experimental data points. By integrating
traditional surrogate models like Gaussian processes and random forests with
powerful LLMs such as DeepSeek-R1, a hierarchical architecture is proposed. Two
CoT-building methods, Large Language Model-Chain of Thought (LLM-CoT) and
Machine Learning-Large Language Model-Chain of Thought (ML-LLM-CoT), are
studied. The LLM-CoT combines local models DeepSeek-r1:14b and Qwen2:7b with
Ollama. The ML-LLM-CoT integrates a pre-trained Gaussian ML model with the
LLM-based CoT framework. Our results show that during construction, ML-LLM-CoT
is more efficient. It only has 2 points that require rethink and a total of 4
rethink times, while LLM-CoT has 5 points that need to be re-thought and 34
total rethink times. In predicting the solubility of 20 molecules with
dissimilar structures, the number of molecules with a prediction deviation
higher than 100\% for the Gaussian model, LLM-CoT, and ML-LLM-CoT is 7, 6, and
4 respectively. These results indicate that ML-LLM-CoT performs better in
controlling the number of high-deviation molecules, optimizing the average
deviation, and achieving a higher success rate in solubility judgment,
providing a more reliable method for chemical engineering and molecular
property prediction. This study breaks through the limitations of traditional
methods and offers new solutions for rapid property prediction and process
optimization in chemical engineering.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.12383v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>The impact of job stability on monetary poverty in Italy: causal small
  area estimation</h3>
                    <p><strong>Authors:</strong> Katarzyna Reluga, Dehan Kong, Setareh Ranjbar, Nicola Salvati, Mark van der Laan</p>
                    <p>  Job stability - encompassing secure contracts, adequate wages, social
benefits, and career opportunities - is a critical determinant in reducing
monetary poverty, as it provides households with reliable income and enhances
economic well-being. This study leverages EU-SILC survey and census data to
estimate the causal effect of job stability on monetary poverty across Italian
provinces, quantifying its influence and analyzing regional disparities. We
introduce a novel causal small area estimation (CSAE) framework that integrates
global and local estimation strategies for heterogeneous treatment effect
estimation, effectively addressing data sparsity at the provincial level.
Furthermore, we develop a general bootstrap scheme to construct reliable
confidence intervals, applicable regardless of the method used for estimating
nuisance parameters. Extensive simulation studies demonstrate that our proposed
estimators outperform classical causal inference methods in terms of stability
while maintaining computational scalability for large datasets. Applying this
methodology to real-world data, we uncover significant relationships between
job stability and poverty across six Italian regions, offering critical
insights into regional disparities and their implications for evidence-based
policy design.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.12376v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>