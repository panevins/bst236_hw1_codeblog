<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 2/26/2025, 8:03:33 PM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Attractiveness and equal treatment in a group draw</h3>
                    <p><strong>Authors:</strong> László Csató, Dóra Gréta Petróczy</p>
                    <p>  National teams from different continents can play against each other only in
afew sports competitions. Therefore, a reasonable aim is maximising the number
of intercontinental games in world cups, as done in basketball and football, in
contrast to handball and volleyball. However, this objective requires
additional draw constraints that imply the violation of equal treatment. In
addition, the standard draw mechanism is non-uniformly distributed on the set
of valid assignments, which may lead to further distortions. Our paper analyses
this novel trade-off between attractiveness and fairness through the example of
the 2025 World Men's Handball Championship. We introduce a measure of
inequality, which enables considering 32 sets of reasonable geographical
restrictions to determine the Pareto frontier. The proposed methodology can be
used by policy-makers to select the optimal set of draw constraints.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18332v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Smart and Efficient IoT-Based Irrigation System Design: Utilizing a
  Hybrid Agent-Based and System Dynamics Approach</h3>
                    <p><strong>Authors:</strong> Taha Ahmadi Pargo, Mohsen Akbarpour Shirazi, Dawud Fadai</p>
                    <p>  Regarding problems like reduced precipitation and an increase in population,
water resource scarcity has become one of the most critical problems in
modern-day societies, as a consequence, there is a shortage of available water
resources for irrigation in arid and semi-arid countries. On the other hand, it
is possible to utilize modern technologies to control irrigation and reduce
water loss. One of these technologies is the Internet of Things (IoT). Despite
the possibility of using the IoT in irrigation control systems, there are
complexities in designing such systems. Considering this issue, it is possible
to use agent-oriented software engineering (AOSE) methodologies to design
complex cyber-physical systems such as IoT-based systems. In this research, a
smart irrigation system is designed based on Prometheus AOSE methodology, to
reduce water loss by maintaining soil moisture in a suitable interval. The
designed system comprises sensors, a central agent, and irrigation nodes. These
agents follow defined rules to maintain soil moisture at a desired level
cooperatively. For system simulation, a hybrid agent-based and system dynamics
model was designed. In this hybrid model, soil moisture dynamics were modeled
based on the system dynamics approach. The proposed model, was implemented in
AnyLogic computer simulation software. Utilizing the simulation model,
irrigation rules were examined. The system's functionality in automatic
irrigation mode was tested based on a 256-run, fractional factorial design, and
the effects of important factors such as soil properties on total irrigated
water and total operation time were analyzed. Based on the tests, the system
consistently irrigated nearly optimal water amounts in all tests. Moreover, the
results were also used to minimize the system's energy consumption by reducing
the system's operational time.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18298v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Enhancing External Validity of Experiments with Ongoing Sampling</h3>
                    <p><strong>Authors:</strong> Chen Wang, Shichao Han, Shan Huang</p>
                    <p>  Participants in online experiments often enroll over time, which can
compromise sample representativeness due to temporal shifts in covariates. This
issue is particularly critical in A/B tests, online controlled experiments
extensively used to evaluate product updates, since these tests are
cost-sensitive and typically short in duration. We propose a novel framework
that dynamically assesses sample representativeness by dividing the ongoing
sampling process into three stages. We then develop stage-specific estimators
for Population Average Treatment Effects (PATE), ensuring that experimental
results remain generalizable across varying experiment durations. Leveraging
survival analysis, we develop a heuristic function that identifies these stages
without requiring prior knowledge of population or sample characteristics,
thereby keeping implementation costs low. Our approach bridges the gap between
experimental findings and real-world applicability, enabling product decisions
to be based on evidence that accurately represents the broader target
population. We validate the effectiveness of our framework on three levels: (1)
through a real-world online experiment conducted on WeChat; (2) via a synthetic
experiment; and (3) by applying it to 600 A/B tests on WeChat in a
platform-wide application. Additionally, we provide practical guidelines for
practitioners to implement our method in real-world settings.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18253v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Differentially private synthesis of Spatial Point Processes</h3>
                    <p><strong>Authors:</strong> Dangchan Kim, Chae Young Lim</p>
                    <p>  This paper proposes a method to generate synthetic data for spatial point
patterns within the differential privacy (DP) framework. Specifically, we
define a differentially private Poisson point synthesizer (PPS) and Cox point
synthesizer (CPS) to generate synthetic point patterns with the concept of the
$\alpha$-neighborhood that relaxes the original definition of DP. We present
three example models to construct a differentially private PPS and CPS,
providing sufficient conditions on their parameters to ensure the DP given a
specified privacy budget. In addition, we demonstrate that the synthesizers can
be applied to point patterns on the linear network. Simulation experiments
demonstrate that the proposed approaches effectively maintain the privacy and
utility of synthetic data.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18198v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Measuring Interlayer Dependence of Large Degrees in Multilayer
  Inhomogeneous Random Graphs</h3>
                    <p><strong>Authors:</strong> Zhuoye Han, Tiandong Wang</p>
                    <p>  Accurately capturing interlayer dependence is essential for understanding the
structure of complex multilayer networks. We propose an upper tail dependence
estimator specifically designed for multilayer networks, leveraging multilayer
inhomogeneous random graphs and multivariate regular variation to model
extremal dependence. We establish the consistency of the estimator and
demonstrate its practical effectiveness through real-data analysis of Reddit.
Our findings reveal how financial market dynamics influence user interactions
in the BitcoinMarkets subreddit and how seasonal trends shape engagement in
sports-related subreddits. This work provides a rigorous and practical tool for
quantifying extremal dependence across network layers, offering valuable
insights into risk propagation and interaction patterns in multilayer systems.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.17934v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>AI-driven 3D Spatial Transcriptomics</h3>
                    <p><strong>Authors:</strong> Cristina Almagro-Pérez, Andrew H. Song, Luca Weishaupt, Ahrong Kim, Guillaume Jaume, Drew F. K. Williamson, Konstantin Hemker, Ming Y. Lu, Kritika Singh, Bowen Chen, Long Phi Le, Alexander S. Baras, Sizun Jiang, Ali Bashashati, Jonathan T. C. Liu, Faisal Mahmood</p>
                    <p>  A comprehensive three-dimensional (3D) map of tissue architecture and gene
expression is crucial for illuminating the complexity and heterogeneity of
tissues across diverse biomedical applications. However, most spatial
transcriptomics (ST) approaches remain limited to two-dimensional (2D) sections
of tissue. Although current 3D ST methods hold promise, they typically require
extensive tissue sectioning, are complex, are not compatible with
non-destructive 3D tissue imaging technologies, and often lack scalability.
Here, we present VOlumetrically Resolved Transcriptomics EXpression (VORTEX),
an AI framework that leverages 3D tissue morphology and minimal 2D ST to
predict volumetric 3D ST. By pretraining on diverse 3D
morphology-transcriptomic pairs from heterogeneous tissue samples and then
fine-tuning on minimal 2D ST data from a specific volume of interest, VORTEX
learns both generic tissue-related and sample-specific morphological correlates
of gene expression. This approach enables dense, high-throughput, and fast 3D
ST, scaling seamlessly to large tissue volumes far beyond the reach of existing
3D ST techniques. By offering a cost-effective and minimally destructive route
to obtaining volumetric molecular insights, we anticipate that VORTEX will
accelerate biomarker discovery and our understanding of morphomolecular
associations and cell states in complex tissues. Interactive 3D ST volumes can
be viewed at https://vortex-demo.github.io/
</p>
                    <p><a href="http://arxiv.org/pdf/2502.17761v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Dynamic Dirichlet Process Mixture Model for the Partisan Realignment
  of Civil Rights Issues in the U.S. House of Representatives</h3>
                    <p><strong>Authors:</strong> Nuannuan Xiang, Yuki Shiraito</p>
                    <p>  Evolutionary societal changes often prompt a debate. The positions of the two
major political parties in the United States on civil rights issues underwent a
reversal in the 20th century. The conventional view holds that this shift was a
structural break in the 1960s, driven by party elites, while recent studies
argue that the change was a more gradual process that began as early as the
1930s, driven by local rank-and-file party members. Motivated by this
controversy, this paper develops a nonparametric Bayesian model that
incorporates a hidden Markov model into the Dirichlet process mixture model. A
distinctive feature of the proposed approach is that it models a process in
which multiple latent clusters emerge and diminish as a continuing process so
that it uncovers any of steady, sudden, and repeated shifts in analysing
longitudinal data. Our model estimates each party's positions on civil rights
in each state based on the legislative activities of their Congressional
members, identifying cross- and within-party coalitions over time. We find
evidence of gradual racial realignment in the 20th century, with two periods of
fast changes during the 1948 election and the Civil Rights Movement.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.17733v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Unified Model of Text and Citations for Topic-Specific Citation
  Networks</h3>
                    <p><strong>Authors:</strong> ByungKoo Kim, Saki Kuzushima, Yuki Shiraito</p>
                    <p>  Social scientists analyze citation networks to study how documents influence
subsequent work across various domains such as judicial politics and
international relations. However, conventional approaches that summarize
document attributes in citation networks often overlook the diverse semantic
contexts in which citations occur. This paper develops the paragraph-citation
topic model (PCTM), which analyzes citation networks and document texts
jointly. The PCTM extends conventional topic models by assigning topics to
paragraphs of citing documents, allowing citations to share topics with their
embedding paragraphs. Our empirical analysis of U.S. Supreme Court opinions in
the privacy issue domain, which includes cases on reproductive rights,
demonstrates that citations within individual documents frequently span
multiple substantive areas, and citations to individual documents show
considerable topical diversity.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.17708v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Protocol For An Observational Study On The Effects Of Combinations Of
  Adverse Childhood Experiences On Adult Depression</h3>
                    <p><strong>Authors:</strong> Ruizhe Zhang, Jooyoung Kong, Dylan S. Small, William Bekerman</p>
                    <p>  Adverse childhood experiences (ACEs) have been linked to a wide range of
negative health outcomes in adulthood. However, few studies have investigated
what specific combinations of ACEs most substantially impact mental health. In
this article, we provide the protocol for our observational study of the
effects of combinations of ACEs on adult depression. We use data from the 2023
Behavioral Risk Factor Surveillance System (BRFSS) to assess these effects. We
will evaluate the replicability of our findings by splitting the sample into
two discrete subpopulations of individuals. We employ data turnover for this
analysis, enabling a single team of statisticians and domain experts to
collaboratively evaluate the strength of evidence, and also integrating both
qualitative and quantitative insights from exploratory data analysis. We
outline our analysis plan using this method and conclude with a brief
discussion of several specifics for our study.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.17679v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>StatLLM: A Dataset for Evaluating the Performance of Large Language
  Models in Statistical Analysis</h3>
                    <p><strong>Authors:</strong> Xinyi Song, Lina Lee, Kexin Xie, Xueying Liu, Xinwei Deng, Yili Hong</p>
                    <p>  The coding capabilities of large language models (LLMs) have opened up new
opportunities for automatic statistical analysis in machine learning and data
science. However, before their widespread adoption, it is crucial to assess the
accuracy of code generated by LLMs. A major challenge in this evaluation lies
in the absence of a benchmark dataset for statistical code (e.g., SAS and R).
To fill in this gap, this paper introduces StatLLM, an open-source dataset for
evaluating the performance of LLMs in statistical analysis. The StatLLM dataset
comprises three key components: statistical analysis tasks, LLM-generated SAS
code, and human evaluation scores. The first component includes statistical
analysis tasks spanning a variety of analyses and datasets, providing problem
descriptions, dataset details, and human-verified SAS code. The second
component features SAS code generated by ChatGPT 3.5, ChatGPT 4.0, and Llama
3.1 for those tasks. The third component contains evaluation scores from human
experts in assessing the correctness, effectiveness, readability,
executability, and output accuracy of the LLM-generated code. We also
illustrate the unique potential of the established benchmark dataset for (1)
evaluating and enhancing natural language processing metrics, (2) assessing and
improving LLM performance in statistical coding, and (3) developing and testing
of next-generation statistical software - advancements that are crucial for
data science and machine learning research.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.17657v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>