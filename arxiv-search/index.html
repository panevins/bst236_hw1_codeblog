<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 3/26/2025, 6:45:21 PM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Combining predictive distributions for time-to-event outcomes in
  meteorology</h3>
                    <p><strong>Authors:</strong> Céline Cunen, Thea Roksvåg, Claudio Heinrich-Mertsching, Alex Lenkoski</p>
                    <p>  Combining forecasts from multiple numerical weather prediction (NWP) models
have shown substantial benefit over the use of individual forecast products.
Although combination, in a broad sense, is widely used in meteorological
forecasting, systematic studies of combination methodology in meteorology are
scarce. In this article, we study several combination methods, both
state-of-the-art and of our own making, with a particular emphasis on
situations where one seeks to predict when a particular event of interest will
occur. Such time-to-event forecasts require particular methodology and care. We
conduct a careful comparison of the different combination methods through an
extensive simulation study, where we investigate the conditions under which the
combined forecast will outperform the individual forecasting products. Further,
we investigate the performance of the methods in a case-study modelling the
time to first hard freeze in Norway and parts of Fennoscandia.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.19534v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A novel forecasting framework combining virtual samples and enhanced
  Transformer models for tourism demand forecasting</h3>
                    <p><strong>Authors:</strong> Tingting Diao, Xinzhang Wu, Lina Yang, Ling Xiao, Yunxuan Dong</p>
                    <p>  Accurate tourism demand forecasting is hindered by limited historical data
and complex spatiotemporal dependencies among tourist origins. A novel
forecasting framework integrating virtual sample generation and a novel
Transformer predictor addresses constraints arising from restricted data
availability. A spatiotemporal GAN produces realistic virtual samples by
dynamically modeling spatial correlations through a graph convolutional
network, and an enhanced Transformer captures local patterns with causal
convolutions and long-term dependencies with self-attention,eliminating
autoregressive decoding. A joint training strategy refines virtual sample
generation based on predictor feedback to maintain robust performance under
data-scarce conditions. Experimental evaluations on real-world daily and
monthly tourism demand datasets indicate a reduction in average MASE by 18.37%
compared to conventional Transformer-based models, demonstrating improved
forecasting accuracy. The integration of adaptive spatiotemporal sample
augmentation with a specialized Transformer can effectively address
limited-data forecasting scenarios in tourism management.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.19423v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Stochastic ecohydrological perspective on semi-distributed
  rainfall-runoff dynamics</h3>
                    <p><strong>Authors:</strong> Mark S. Bartlett, Elizabeth Cultra, Nathan Geldner, Amilcare Porporato</p>
                    <p>  Quantifying watershed process variability consistently with climate change
and ecohydrological dynamics remains a central challenge in hydrology.
Stochastic ecohydrology characterizes hydrologic variability through
probability distributions that link climate, hydrology, and ecology. However,
these approaches are often limited to small spatial scales (e.g., point or plot
level) or focus on specific fluxes (e.g., streamflow), without accounting for
the entire water balance at the basin scale. While semi-distributed models
account for spatial heterogeneity and upscaled hydrologic fluxes, they lack the
analytical simplicity of stochastic ecohydrology or the SCS-CN method and,
perhaps more importantly, do not integrate the effects of past random
variability in hydroclimatic conditions. This hinders an efficient
characterization of hydrological statistics at the watershed scale. To overcome
these limitations, we merge stochastic ecohydrology, the spatial upscaling of
semi-distributed modeling, and the SCS-CN rainfall-runoff partitioning. The
resulting unified model analytically characterizes watershed ecohydrological
and hydrological statistics using probability density functions (PDFs) that are
functions of climate and watershed attributes -- something unattainable with
the Monte Carlo methods of traditional stochastic hydrology. Calibrated across
81 watersheds in Florida and southern Louisiana, the model PDFs precisely
capture the long-term average water balance and runoff variance, as well as the
runoff quantiles with a median normalized Nash-Sutcliffe (NNSE) efficiency of
0.95. These results also advance the SCS-CN method by providing an analytical
PDF for the Curve Number (CN), explicitly linked to climate variables,
baseflow, and the long-term water balance partitioning described by the Budyko
curve.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.19220v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Statistical Design and Rationale of the Biomarkers for Evaluating Spine
  Treatments (BEST) Trial</h3>
                    <p><strong>Authors:</strong> John Sperger, Kelley M. Kidwell, Matthew C. Mauck, Beibo Zhao, Kevin J. Anstrom, Anna Batorsky, Timothy S. Carey, Daniel J. Clauw, Nikki L. B. Freeman, Carol M. Greco, Anastasia Ivanova, Sara Jones Berkeley, Samuel A. McLean, Matthew A. Psioda, Bryce Rowland, Gwendolyn A. Sowa, Ajay D. Wasan, Joshua P Zitovsky, Michael R. Kosorok</p>
                    <p>  Chronic low back pain (cLBP) is a prevalent condition with profound impacts
on functioning and quality of life. While multiple evidence-based treatments
exist, they all have modest average treatment
effects$\unicode{x2013}$potentially due to individual variation in treatment
response and the diverse etiologies of cLBP. This multi-site sequential,
multiple-assignment randomized trial (SMART) investigated four treatment
modalities with two stages of randomization and aimed to enroll 630 protocol
completers. The primary objective was to develop a precision medicine approach
by estimating optimal treatment or treatment combinations based on patient
characteristics and initial treatment response. The analysis strategy focuses
on estimating interpretable dynamic treatment regimes and identifying subgroups
most responsive to specific interventions. Broad eligibility criteria were
implemented to enhance generalizability and recruitment, most notably that
participants could be eligible to enroll even if they could not be assigned to
one (but no more) of the study interventions. Enrolling participants with
restrictions on the treatment they could be assigned necessitated modifications
to standard minimization methods for balancing covariates. The BEST trial
represents one of the largest SMARTs focused on clinical decision-making to
date and the largest in cLBP. By collecting an extensive array of biomarker and
phenotypic measures, this trial may identify potential treatment mechanisms and
establish a more evidence-based approach to individualizing cLBP treatment in
clinical practice.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.19127v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Causal Links Between Anthropogenic Emissions and Air Pollution Dynamics
  in Delhi</h3>
                    <p><strong>Authors:</strong> Sourish Das, Sudeep Shukla, Alka Yadav, Anirban Chakraborti</p>
                    <p>  Air pollution poses significant health and environmental challenges,
particularly in rapidly urbanizing regions. Delhi-National Capital Region
experiences air pollution episodes due to complex interactions between
anthropogenic emissions and meteorological conditions. Understanding the causal
drivers of key pollutants such as $PM_{2.5}$ and ground $O_3$ is crucial for
developing effective mitigation strategies. This study investigates the causal
links of anthropogenic emissions on $PM_{2.5}$ and $O_3$ concentrations using
predictive modeling and causal inference techniques. Integrating
high-resolution air quality data from Jan 2018 to Aug 2023 across 32 monitoring
stations, we develop predictive regression models that incorporate
meteorological variables (temperature and relative humidity), pollutant
concentrations ($NO_2, SO_2, CO$), and seasonal harmonic components to capture
both diurnal and annual cycles. Here, we show that reductions in anthropogenic
emissions lead to significant decreases in $PM_{2.5}$ levels, whereas their
effect on $O_3$ remains marginal and statistically insignificant. To address
spatial heterogeneity, we employ Gaussian Process modeling. Further, we use
Granger causality analysis and counterfactual simulation to establish direct
causal links. Validation using real-world data from the COVID-19 lockdown
confirms that reduced emissions led to a substantial drop in $PM_{2.5}$ but
only a slight, insignificant change in $O_3$. The findings highlight the
necessity of targeted emission reduction policies while emphasizing the need
for integrated strategies addressing both particulate and ozone pollution.
These insights are crucial for policymakers designing air pollution
interventions in other megacities, and offer a scalable methodology for
tackling complex urban air pollution through data-driven decision-making.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.18912v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Calibration Bands for Mean Estimates within the Exponential Dispersion
  Family</h3>
                    <p><strong>Authors:</strong> Łukasz Delong, Selim Gatti, Mario V. Wüthrich</p>
                    <p>  A statistical model is said to be calibrated if the resulting mean estimates
perfectly match the true means of the underlying responses. Aiming for
calibration is often not achievable in practice as one has to deal with finite
samples of noisy observations. A weaker notion of calibration is
auto-calibration. An auto-calibrated model satisfies that the expected value of
the responses being given the same mean estimate matches this estimate. Testing
for auto-calibration has only been considered recently in the literature and we
propose a new approach based on calibration bands. Calibration bands denote a
set of lower and upper bounds such that the probability that the true means lie
simultaneously inside those bounds exceeds some given confidence level. Such
bands were constructed by Yang-Barber (2019) for sub-Gaussian distributions.
Dimitriadis et al. (2023) then introduced narrower bands for the Bernoulli
distribution and we use the same idea in order to extend the construction to
the entire exponential dispersion family that contains for example the
binomial, Poisson, negative binomial, gamma and normal distributions. Moreover,
we show that the obtained calibration bands allow us to construct various tests
for calibration and auto-calibration, respectively.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.18896v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Stochastic modeling of particle structures in spray fluidized bed
  agglomeration using methods from machine learning</h3>
                    <p><strong>Authors:</strong> Lukas Fuchs, Sabrina Weber, Jialin Men, Niklas Eiermann, Orkun Furat, Andreas Bück, Volker Schmidt</p>
                    <p>  Agglomeration is an industrially relevant process for the production of bulk
materials in which the product properties depend on the morphology of the
agglomerates, e.g., on the distribution of size and shape descriptors. Thus,
accurate characterization and control of agglomerate morphologies is essential
to ensure high and consistent product quality. This paper presents a pipeline
for image-based inline agglomerate characterization and prediction of their
time-dependent multivariate morphology distributions within a spray fluidized
bed process with transparent glass beads as primary particles. The framework
classifies observed objects in image data into three distinct morphological
classes--primary particles, chain-like agglomerates and raspberry-like
agglomerates--using various size and shape descriptors. To this end, a fast and
robust random forest classifier is trained. Additionally, the fraction of
primary particles belonging to each of these classes, either as individual
primary particles or as part of a larger structure in the form of chain-like or
raspberry-like agglomerates, is described using parametric regression
functions. Finally, the temporal evolution of bivariate size and shape
descriptor distributions of these classes is modeled using low-parametric
regression functions and Archimedean copulas. This approach improves the
understanding of agglomerate formation and allows the prediction of process
kinetics, facilitating precise control over class fractions and morphology
distributions.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.18882v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A primer on inference and prediction with epidemic renewal models and
  sequential Monte Carlo</h3>
                    <p><strong>Authors:</strong> Nicholas Steyn, Kris V. Parag, Robin N. Thompson, Christl A. Donnelly</p>
                    <p>  Renewal models are widely used in statistical epidemiology as
semi-mechanistic models of disease transmission. While primarily used for
estimating the instantaneous reproduction number, they can also be used for
generating projections, estimating elimination probabilities, modelling the
effect of interventions, and more. We demonstrate how simple sequential Monte
Carlo methods (also known as particle filters) can be used to perform inference
on these models. Our goal is to acquaint a reader who has a working knowledge
of statistical inference with these methods and models and to provide a
practical guide to their implementation. We focus on these methods' flexibility
and their ability to handle multiple statistical and other biases
simultaneously. We leverage this flexibility to unify existing methods for
estimating the instantaneous reproduction number and generating projections. A
companion website "SMC and epidemic renewal models" provides additional worked
examples, self-contained code to reproduce the examples presented here, and
additional materials.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.18875v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Probabilistic Assessment of West Nile Virus Spillover Risk Using a
  Compartmental Mechanistic Model</h3>
                    <p><strong>Authors:</strong> Saman Hosseini, Lee W. Cohnstaedt, Matin Marjani, Caterina Scoglio</p>
                    <p>  This paper presents a novel probabilistic approach for assessing the risk of
West Nile Disease (WND) spillover to the human population. The assessment has
been conducted under two different scenarios: (1) assessment of the onset of
spillover, and (2) assessment of the severity of the epidemic after the onset
of the disease. A compartmental model of differential equations is developed to
describe the disease transmission mechanism, and a probability density function
for pathogen spillover to humans is derived based on the model for the
assessment of the risk of the spillover onset and the severity of the epidemic.
The prediction strategy involves making a long-term forecast and then updating
it with a short-term (lead time of two weeks or daily). The methodology is
demonstrated using detailed outbreak data from high-case counties in
California, including Orange County, Los Angeles County, and Kern County. The
predicted results are compared with actual infection dates reported by the
California Department of Public Health for 2022-2024 to assess prediction
accuracy. The performance accuracy is evaluated using a logarithmic scoring
system and compared with one of the most renowned predictive models to assess
its effectiveness. In all prediction scenarios, the model demonstrated strong
performance. Lastly, the method is applied to explore the impact of global
warming on spillover risk, revealing an increasing trend in the number of
high-risk days and a shift toward a greater proportion of these days over time
for the onset of the disease.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.18433v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Efficient Inference in First Passage Time Models</h3>
                    <p><strong>Authors:</strong> Sicheng Liu, Alexander Fengler, Michael J. Frank, Matthew T. Harrison</p>
                    <p>  First passage time models describe the time it takes for a random process to
exit a region of interest and are widely used across various scientific fields.
Fast and accurate numerical methods for computing the likelihood function in
these models are essential for efficient statistical inference. Specifically,
in mathematical psychology, generalized drift diffusion models (GDDMs) are an
important class of first passage time models that describe the latent
psychological processes underlying simple decision-making scenarios. GDDMs
model the joint distribution over choices and response times as the first
hitting time of a one-dimensional stochastic differential equation (SDE) to
possibly time-varying upper and lower boundaries. They are widely applied to
extract parameters associated with distinct cognitive and neural mechanisms.
However, current likelihood computation methods struggle with common scenarios
where drift rates covary dynamically with exogenous covariates in each trial,
such as in the attentional drift diffusion model (aDDM). In this work, we
propose a fast and flexible algorithm for computing the likelihood function of
GDDMs based on a large class of SDEs satisfying the Cherkasov condition. Our
method divides each trial into discrete stages, employs fast analytical results
to compute stage-wise densities, and integrates these to compute the overall
trial-wise likelihood. Numerical examples demonstrate that our method not only
yields accurate likelihood evaluations for efficient statistical inference, but
also significantly outperforms existing approaches in terms of speed.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.18381v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>