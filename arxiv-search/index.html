<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 9/15/2025, 1:23:02 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Bayesian Optimum Warranty Region for Right Censored Two Dimensional
  Dependent Data</h3>
                    <p><strong>Authors:</strong> Tanmay Sen, Rathin Das, Ritwik Bhattacharya</p>
                    <p>  Warranty policies play a crucial role in balancing customer satisfaction and
cost of the manufacturer. Traditional one-dimensional warranty frameworks,
based solely on either age or usage, often fail to capture the joint effect of
product life factors. This article investigates two-dimensional warranty
policies by combining Free Replacement Warranty, Pro-Rata Warranty, and
Combination FRW-PRW Warranty schemes across both age and usage scales. A
dissatisfaction cost function is proposed alongside the economic benefit and
warranty cost functions, and the expected utility framework is employed to
derive optimal warranty parameters. The expectation is taken with respect to
the posterior predictive distribution of product lifetime and usage data,
ensuring a data-driven approach. Finally, the methodology is validated using an
open-source dataset, and a new two-dimensional starter motor dataset is
introduced to demonstrate the practical advantages of adopting two-dimensional
warranty policies.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.10421v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Spatial Modeling and Risk Zoning of Global Extreme Precipitation via
  Graph Neural Networks and r-Pareto Processes</h3>
                    <p><strong>Authors:</strong> Zimu Wang, Yifan Wu, Daning Bi</p>
                    <p>  Extreme precipitation events occurring over large spatial domains pose
substantial threats to societies because they can trigger compound flooding,
landslides, and infrastructure failures across wide areas. A hybrid framework
for spatial extreme precipitation modeling and risk zoning is proposed that
integrates graph neural networks with r-Pareto processes (GNN-rP). Unlike
traditional statistical spatial extremes models, this approach learns
nonlinear, nonstationary dependence structures from precipitation-derived
spatial graphs and applies a data-driven tail functional to model joint
exceedances in a low-dimensional embedding space. Using NASA's IMERG
observations (2000-2021) and CMIP6 SSP5-8.5 projections, the framework
delineates coherent high-risk zones, quantifies their temporal persistence, and
detects emerging hotspots under climate change. Compared with two baseline
approaches, the GNN-rP pipeline substantially improves pointwise detection of
high-risk grid cells while yielding comparable clustering stability. Results
highlight persistent high-risk regions in the tropical belt, especially monsoon
and convective zones, and reveal decadal-scale persistence that is punctuated
by episodic reconfigurations under high-emission scenarios. By coupling machine
learning with extreme value theory, GNN-rP offers a scalable, interpretable
tool for adaptive climate risk zoning, with direct applications in
infrastructure planning, disaster preparedness, and climate-resilient policy
design.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.10362v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A sampling method based on highest density regions: Applications to
  surrogate models for rare events estimation</h3>
                    <p><strong>Authors:</strong> Jocelyn Minini, Micha Wasem</p>
                    <p>  This paper introduces a practical sampling method for training surrogate
models in the context of uncertainty propagation. We propose a heuristic method
to uniformly draw samples within highest density regions of the density given
by the random vector describing the uncertainty of the model parameters. The
resulting experimental design aims to provide a better approximation of the
underlying true model compared to the cases where experimental designs have
been drawn according to the distribution of the random vector itself. To assess
the quality of our approach, three error metrics are considered: The first is
the leave-one-out error, the second the relative mean square error and the
third is the error generated by the surrogate model when estimating the
probability of failure of the system compared to its reference value. The
highest density region-based designs are shown to globally outperform the
random vector-based designs both in terms of relative mean square error as well
as in estimating the probability of failure. The proposed method is applicable
within a black-box context and is compatible with existing uncertainty
quantification frameworks for low dimensional and moderately correlated inputs.
It may thus be useful in case of reliability problems, Bayesian inverse
analysis, or whenever the surrogate model is used in a predictor mode.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.10149v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Benchmarking Classical, Machine Learning, and Bayesian Survival Models
  for Clinical Prediction</h3>
                    <p><strong>Authors:</strong> Irving Gómez-Méndez, Sivakorn Phromsiri, Ittiphat Kijpaisansak, Settawut Chaithurdthum</p>
                    <p>  Survival analysis is a statistical framework for modeling time-to-event data,
particularly valuable in healthcare for predicting outcomes like patient
discharge or recurrence. This study implements and compares several survival
models - including Weibull, Weibull AFT, Weibull AFT with Gamma Frailty, Cox
Proportional Hazards (CoxPH), Random Survival Forest (RSF), and DeepSurv -
using a publicly available breast cancer dataset. This study aims to benchmark
classical, machine learning, and Bayesian survival models in terms of their
predictive performance, interpretability, and suitability for clinical
deployment. The models are evaluated using performance metrics such as the
Concordance Index (C-index) and the Root Mean Squared Error (RMSE). DeepSurv
showed the highest predictive performance, while interpretable models like RSF
and Weibull AFT with Gamma Frailty offered competitive results. We also
explored the implementation of statistical models from a Bayesian perspective,
including frailty models, due to their ability to properly quantify
uncertainty. Notably, frailty models are not readily available in standard
survival analysis libraries, necessitating custom implementation. Our results
demonstrate that interpretable statistical models, when correctly implemented
using parameters that are effectively estimated using a Bayesian approach, can
perform competitively with modern black-box models. These findings illustrate
the trade-offs between model complexity, interpretability, and predictive
power, highlighting the potential of Bayesian survival models in clinical
decision-making settings.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.10073v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair
  Population Health Management</h3>
                    <p><strong>Authors:</strong> Sanjay Basu, Sadiq Y. Patel, Parth Sheth, Bhairavi Muralidharan, Namrata Elamaran, Aakriti Kinra, Rajaie Batniji</p>
                    <p>  Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.09772v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for
  Medicaid Care Management</h3>
                    <p><strong>Authors:</strong> Sanjay Basu, Sadiq Y. Patel, Parth Sheth, Bhairavi Muralidharan, Namrata Elamaran, Aakriti Kinra, Rajaie Batniji</p>
                    <p>  We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.09655v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Path Signature Framework for Detecting Creative Fatigue in Digital
  Advertising</h3>
                    <p><strong>Authors:</strong> Charles Shaw</p>
                    <p>  The finite lifespan of advertising effectiveness, commonly known as "creative
fatigue", presents a significant challenge in computational marketing.
Identifying the onset of performance degradation is critical for optimising
media spend and maximising campaign returns, yet traditional methods often lack
the sensitivity and scalability required in modern advertising ecosystems. This
paper introduces a novel methodological framework for detecting creative
fatigue by applying path signature analysis, a sophisticated technique from
stochastic analysis, to performance time-series data. We treat an ad's
performance trajectory as a path in two-dimensional space and use its signature
as a rich feature descriptor. By calculating the distance between the
signatures of consecutive time windows, our method identifies statistically
significant change points in performance dynamics. We further translate these
statistical events into a direct financial metric by quantifying the
opportunity cost of continued investment in a fatigued creative. Through
synthetic experiments and case studies, we demonstrate that this
signature-based approach provides a mathematically principled and complementary
framework for analysing creative fatigue patterns. To the best of our
knowledge, this represents the first application of path signature methods to
the advertising fatigue detection problem, opening new avenues for geometric
approaches in marketing analytics.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.09758v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death
  Determination</h3>
                    <p><strong>Authors:</strong> Yiqun T. Chen, Tyler H. McCormick, Li Liu, Abhirup Datta</p>
                    <p>  Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.09602v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Measuring football fever through wearable technology: A case study on
  the German cup final</h3>
                    <p><strong>Authors:</strong> Timo Adam, Jonas Bauer, Christian Deutscher, Christiane Fuchs, Tamara Schamberger, David Winkelmann</p>
                    <p>  Football is the world's most popular sport, evoking strong physiological and
emotional responses among its fans. Yet, the specific dynamics of fan
attachment to matches have received little attention in the literature. In this
paper, we quantify these dynamics through a unique case study from professional
football: the 2025 cup final of the German Football Association (DFB) between
first-division club VfB Stuttgart and third-division club Arminia Bielefeld. We
collected high-resolution smartwatch data, including heart rate and stress
level, from 229 Arminia Bielefeld fans over approximately 12 weeks,
complemented by survey responses on club attachment, match attendance, and
personal characteristics from a subset of 37 participants. By combining
physiological data with survey information, we analyse variations in emotional
engagement across individuals and contexts, as well as physiological reactions
to key match events. This approach provides rare, data-driven insights into the
football fever that captivates fans during high-stakes competitions.
Furthermore, we compare the vital parameters recorded on the day of the match
with baseline levels on non-matchdays throughout the entire observation period.
Our findings reveal pronounced physiological responses among fans, beginning
hours before the match and peaking at kick-off.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.09569v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A Zero-Inflated Spatio-Temporal Model for Integrating Fishery-Dependent
  and Independent Data under Preferential Sampling</h3>
                    <p><strong>Authors:</strong> Daniela Silva, Raquel Menezes, Gonçalo Araújo, Ana Machado, Renato Rosa, Ana Moreno, Alexandra Silva, Susana Garrido</p>
                    <p>  Sustainable management of marine ecosystems is vital for maintaining healthy
fishery resources, and benefits from advanced scientific tools to accurately
assess species distribution patterns. In fisheries science, two primary data
sources are used: fishery-independent data (FID), collected through systematic
surveys, and fishery-dependent data (FDD), obtained from commercial fishing
activities. While these sources provide complementary information, their
distinct sampling schemes - systematic for FID and preferential for FDD - pose
significant integration challenges. This study introduces a novel
spatio-temporal model that integrates FID and FDD, addressing challenges
associated with zero-inflation and preferential sampling (PS) common in
ecological data. The model employs a six-layer structure to differentiate
between presence-absence and biomass observations, offering a robust framework
for ecological studies affected by PS biases. Simulation results demonstrate
the model's accuracy in parameter estimation across diverse PS scenarios and
its ability to detect preferential signals. Application to the study of the
distribution patterns of the European sardine populations along the southern
Portuguese continental shelf illustrates the model's effectiveness in
integrating diverse data sources and incorporating environmental and
vessel-specific covariates. The model reveals spatio-temporal variability in
sardine presence and biomass, providing actionable insights for fisheries
management. Beyond ecology, this framework offers broad applicability to data
integration challenges in other disciplines.
</p>
                    <p><a href="http://arxiv.org/pdf/2509.09336v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>