<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Search: Applied Statistics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Latest ArXiv Papers on Applied Statistics</h1>
    <p> This page displays the 10 most recents papers on <a href="https://arxiv.org/">ArXiv</a> in the category of "applied statistics". To see all most recent papers under this category, visit ArXiv's website <a href="https://arxiv.org/list/stat.AP/recent">here</a>. This page uses GitHub Actions and the ArXiv API to update each day at approximately midnight.</p>
    <p id="last-updated">Last updated: 3/24/2025, 1:22:13 AM</p>
    <button onclick="window.location.href='../index.html'" style="text-align: center;">Go to Homepage</button>
    <div id="papers">
                <div class="paper">
                    <h3>Leveraging statistical models to improve pre-season forecasting and
  in-season management of a recreational fishery</h3>
                    <p><strong>Authors:</strong> A. Challen Hyman, Chloe Ramsay, Tiffanie A. Cross, Beverly Sauls, Thomas K. Frazer</p>
                    <p>  Effective management of recreational fisheries requires accurate forecasting
of future harvests and real-time monitoring of ongoing harvests. Traditional
methods that rely on historical catch data to predict short-term harvests can
be unreliable, particularly if changes in management regulations alter angler
behavior. In contrast, statistical modeling approaches can provide faster, more
flexible, and potentially more accurate predictions, enhancing management
outcomes. In this study, we developed and tested models to improve predictions
of Gulf of Mexico gag harvests for both pre-season planning and in-season
monitoring. Our best-fitting model outperformed traditional methods (i.e.,
estimates derived from historical average harvest) for both cumulative
pre-season projections and in-season monitoring. Notably, our modeling
framework appeared to be more accurate in more recent, shorter seasons due to
its ability to account for effort compression. A key advantage of our framework
is its ability to explicitly quantify the probability of exceeding harvest
quotas for any given season duration. This feature enables managers to evaluate
trade-offs between season duration and conservation goals. This is especially
critical for vulnerable, highly targeted stocks. Our findings also underscore
the value of statistical models to complement and advance traditional fisheries
management approaches.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.17293v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>ML-Based Bidding Price Prediction for Pay-As-Bid Ancillary Services
  Markets: A Use Case in the German Control Reserve Market</h3>
                    <p><strong>Authors:</strong> Vincent Bezold, Lukas Baur, Alexander Sauer</p>
                    <p>  The increasing integration of renewable energy sources has led to greater
volatility and unpredictability in electricity generation, posing challenges to
grid stability. Ancillary service markets, such as the German control reserve
market, allow industrial consumers and producers to offer flexibility in their
power consumption or generation, contributing to grid stability while earning
additional income. However, many participants use simple bidding strategies
that may not maximize their revenues. This paper presents a methodology for
forecasting bidding prices in pay-as-bid ancillary service markets, focusing on
the German control reserve market. We evaluate various machine learning models,
including Support Vector Regression, Decision Trees, and k-Nearest Neighbors,
and compare their performance against benchmark models. To address the
asymmetry in the revenue function of pay-as-bid markets, we introduce an offset
adjustment technique that enhances the practical applicability of the
forecasting models. Our analysis demonstrates that the proposed approach
improves potential revenues by 27.43 % to 37.31 % compared to baseline models.
When analyzing the relationship between the model forecasting errors and the
revenue, a negative correlation is measured for three markets; according to the
results, a reduction of 1 EUR/MW model price forecasting error (MAE)
statistically leads to a yearly revenue increase between 483 EUR/MW and 3,631
EUR/MW. The proposed methodology enables industrial participants to optimize
their bidding strategies, leading to increased earnings and contributing to the
efficiency and stability of the electrical grid.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.17214v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Addressing complex structures of measurement error arising in the
  exposure assessment in occupational epidemiology using a Bayesian
  hierarchical approach</h3>
                    <p><strong>Authors:</strong> Raphael Rehms, Nicole Ellenbach, Veronika Deffner, Sabine Hoffmann</p>
                    <p>  Exposure assessment in occupational epidemiology may involve multiple unknown
quantities that are measured or reconstructed simultaneously for groups of
workers and over several years. Additionally, exposures may be collected using
different assessment strategies, depending on the period of exposure. As a
consequence, researchers who are analyzing occupational cohort studies are
commonly faced with challenging structures of exposure measurement error,
involving complex dependence structures and multiple measurement error models,
depending on the period of exposure. However, previous work has often made many
simplifying assumptions concerning these errors. In this work, we propose a
Bayesian hierarchical approach to account for a broad range of error structures
arising in occupational epidemiology. The considered error structures may
involve several unknown quantities that can be subject to mixtures of Berkson
and classical measurement error. It is possible to account for different error
structures, depending on the exposure period and the location of a worker.
Moreover, errors can present complex dependence structures over time and
between workers. We illustrate the proposed hierarchical approach on a subgroup
of the German cohort of uranium miners to account for potential exposure
uncertainties in the association between radon exposure and lung cancer
mortality. The performance of the proposed approach and its sensitivity to
model misspecification are evaluated in a simulation study. The results show
that biases in estimates arising from very complex measurement errors can be
corrected through the proposed Bayesian hierarchical approach.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.17161v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>A New Statistical Model of Star Speckles for Learning to Detect and
  Characterize Exoplanets in Direct Imaging Observations</h3>
                    <p><strong>Authors:</strong> Théo Bodrito, Olivier Flasseur, Julien Mairal, Jean Ponce, Maud Langlois, Anne-Marie Lagrange</p>
                    <p>  The search for exoplanets is an active field in astronomy, with direct
imaging as one of the most challenging methods due to faint exoplanet signals
buried within stronger residual starlight. Successful detection requires
advanced image processing to separate the exoplanet signal from this nuisance
component. This paper presents a novel statistical model that captures nuisance
fluctuations using a multi-scale approach, leveraging problem symmetries and a
joint spectral channel representation grounded in physical principles. Our
model integrates into an interpretable, end-to-end learnable framework for
simultaneous exoplanet detection and flux estimation. The proposed algorithm is
evaluated against the state of the art using datasets from the SPHERE
instrument operating at the Very Large Telescope (VLT). It significantly
improves the precision-recall trade-off, notably on challenging datasets that
are otherwise unusable by astronomers. The proposed approach is computationally
efficient, robust to varying data quality, and well suited for large-scale
observational surveys.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.17117v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Modeling and forecasting subnational age distribution of death counts</h3>
                    <p><strong>Authors:</strong> Han Lin Shang, Cristian F. Jiménez-Varón</p>
                    <p>  This paper presents several forecasting methods to model and forecast
subnational age distribution of death counts. The age distribution of death
counts has many similarities to probability density functions, which are
nonnegative and have a constrained integral, and thus live in a constrained
nonlinear space. To address the nonlinear nature of objects, we implement a
cumulative distribution function transformation that has an additional
monotonicity. Using the Japanese subnational life-table death counts obtained
from the Japanese Mortality Database (2025), we evaluate the forecast accuracy
of the transformation and forecasting methods. The improved forecast accuracy
of life-table death counts implemented here will be of great interest to
demographers in estimating regional age-specific survival probabilities and
life expectancy.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.16744v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Spatial-temporal models for forest inventory data</h3>
                    <p><strong>Authors:</strong> Paul B. May, Andrew O. Finley</p>
                    <p>  The USDA Forest Inventory and Analysis (FIA) program conducts a national
forest inventory for the United States through a network of permanent field
plots. FIA produces estimates of area averages/totals for plot-measured forest
variables through design-based inference, assuming a fixed population and a
probability sample of field plot locations. The fixed-population assumption and
characteristics of the FIA sampling scheme make it difficult to estimate change
in forest variables over time using design-based inference. We propose
spatial-temporal models based on Gaussian processes as a flexible tool for
forest inventory data, capable of inferring forest variables and change thereof
over arbitrary spatial and temporal domains. It is shown to be beneficial for
the covariance function governing the latent Gaussian process to account for
variation at multiple scales, separating spatially local variation from
ecosystem-scale variation. We demonstrate a model for forest biomass density,
inferring 20 years of biomass change within two US National Forests.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.16691v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Ensemble Survival Analysis for Preclinical Cognitive Decline Prediction
  in Alzheimer's Disease Using Longitudinal Biomarkers</h3>
                    <p><strong>Authors:</strong> Dhrubajyoti Ghosh, Samhita Pal, Michael Lutz, Sheng Luo</p>
                    <p>  Predicting the risk of clinical progression from cognitively normal (CN)
status to mild cognitive impairment (MCI) or Alzheimer's disease (AD) is
critical for early intervention in Alzheimer's disease (AD). Traditional
survival models often fail to capture complex longitudinal biomarker patterns
associated with disease progression. We propose an ensemble survival analysis
framework integrating multiple survival models to improve early prediction of
clinical progression in initially cognitively normal individuals. We analyzed
longitudinal biomarker data from the Alzheimer's Disease Neuroimaging
Initiative (ADNI) cohort, including 721 participants, limiting analysis to up
to three visits (baseline, 6-month follow-up, 12-month follow-up). Of these,
142 (19.7%) experienced clinical progression to MCI or AD. Our approach
combined penalized Cox regression (LASSO, Elastic Net) with advanced survival
models (Random Survival Forest, DeepSurv, XGBoost). Model predictions were
aggregated using ensemble averaging and Bayesian Model Averaging (BMA).
Predictive performance was assessed using Harrell's concordance index (C-index)
and time-dependent area under the curve (AUC). The ensemble model achieved a
peak C-index of 0.907 and an integrated time-dependent AUC of 0.904,
outperforming baseline-only models (C-index 0.608). One follow-up visit after
baseline significantly improved prediction accuracy (48.1% C-index, 48.2% AUC
gains), while adding a second follow-up provided only marginal gains (2.1%
C-index, 2.7% AUC). Our ensemble survival framework effectively integrates
diverse survival models and aggregation techniques to enhance early prediction
of preclinical AD progression. These findings highlight the importance of
leveraging longitudinal biomarker data, particularly one follow-up visit, for
accurate risk stratification and personalized intervention strategies.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.16645v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Active Learning For Repairable Hardware Systems With Partial Coverage</h3>
                    <p><strong>Authors:</strong> Michael Potter, Beyza Kalkanlı, Deniz Erdoğmuş, Michael Everett</p>
                    <p>  Identifying the optimal diagnostic test and hardware system instance to infer
reliability characteristics using field data is challenging, especially when
constrained by fixed budgets and minimal maintenance cycles. Active Learning
(AL) has shown promise for parameter inference with limited data and budget
constraints in machine learning/deep learning tasks. However, AL for
reliability model parameter inference remains underexplored for repairable
hardware systems. It requires specialized AL Acquisition Functions (AFs) that
consider hardware aging and the fact that a hardware system consists of
multiple sub-systems, which may undergo only partial testing during a given
diagnostic test. To address these challenges, we propose a relaxed Mixed
Integer Semidefinite Program (MISDP) AL AF that incorporates Diagnostic
Coverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing
budgets. Furthermore, we design empirical-based simulation experiments focusing
on two diagnostic testing scenarios: (1) partial tests of a hardware system
with overlapping subsystem coverage, and (2) partial tests where one diagnostic
test fully subsumes the subsystem coverage of another. We evaluate our proposed
approach against the most widely used AL AF in the literature (entropy), as
well as several intuitive AL AFs tailored for reliability model parameter
inference. Our proposed AF ranked best on average among the alternative AFs
across 6,000 experimental configurations, with respect to Area Under the Curve
(AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error
(MSE) curves, with statistical significance calculated at a 0.05 alpha level
using a Friedman hypothesis test.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.16315v1" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Affective Polarization Amongst Swedish Politicians</h3>
                    <p><strong>Authors:</strong> François t'Serstevens, Roberto Cerina, Gustav Peper</p>
                    <p>  This study investigates affective polarization among Swedish politicians on
Twitter from 2021 to 2023, including the September 2022 parliamentary election.
Analyzing over 25,000 tweets and employing large language models (LLMs) for
sentiment and political classification, we distinguish between positive
partisanship (support of allies) and negative partisanship (criticism of
opponents).
  Our findings are contingent on the definition of the in-group. When political
in-groups are defined at the ideological bloc level, negative and positive
partisanship occur at similar rates. However, when the in-group is defined at
the party level, negative partisanship becomes significantly more dominant and
is 1.51 times more likely (1.45, 1.58). This effect is even stronger among
extreme politicians, who engage in negativity more than their moderate
counterparts. Negative partisanship also proves to be a strategic choice for
online visibility, attracting 3.18 more likes and 1.69 more retweets on
average.
  By adapting methods developed for two-party systems and leveraging LLMs for
Swedish-language analysis, we provide novel insights into how multiparty
politics shapes polarizing discourse. Our results underscore both the strategic
appeal of negativity in digital spaces and the growing potential of LLMs for
large-scale, non-English political research.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.16193v2" target="_blank">Read PDF</a></p>
                </div>
            
                <div class="paper">
                    <h3>Forecasting Extreme Temperatures in Siberia Using Supervised Learning
  and Conformal Prediction Regions</h3>
                    <p><strong>Authors:</strong> Richard A. Berk, Amy Braverman</p>
                    <p>  In this paper, we step back from a variety of competing heat wave definitions
and forecast directly unusually high temperatures. Our testbed is the Russian
Far East in the summers of 2022 and 2023. Remotely sensed data from NASA's Aqua
spacecraft are organized into a within-subject design that can reduce nuisance
variation in forecasted temperatures. Spatial grid cells are the study units.
Each is exposed to precursors of a faux heat wave in 2022 and to precursors of
a reported heat wave in 2023. The precursors are used to forecast temperatures
two weeks in the future for each of 31 consecutive days. Algorithmic fitting
procedures produce forecasts with promise and relatively small conformal
prediction regions having a coverage probability of at least .75. Spatial and
temporal dependence are manageable. At worst, there is weak dependence such
that conformal prediction inference is only asymptotically valid.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.16118v1" target="_blank">Read PDF</a></p>
                </div>
            </div>
    <script src="scripts/update-papers.js"></script>
</body>
<p></p>
<p></p>
<footer>
    <p>&copy; 2025 Pascale's Coding Blog. All rights reserved.</p>
    <p><a href="https://github.com/panevins" style="color:gold">@panevins</a> on GitHub</p>
</footer>

</html>